{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b7528b4f-6ffa-4eee-bd29-6ffa632a9e7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Loaded 73279 game date-team-game_id mappings\n",
      "ðŸ“… Pulling Offense matchup for 20221018 (Regular Season)\n",
      "   Initial mapping - OFF_TEAM_ID: 86.5%, DEF_TEAM_ID: 86.5%\n",
      "   Created 15 team pair mappings for 20221018\n",
      "   Filled 15825 missing OFF_TEAM_ID using opponent mapping\n",
      "   Filled 15805 missing DEF_TEAM_ID using opponent mapping\n",
      "   Created 15 team pair mappings for 20221018\n",
      "   Final mapping - OFF_TEAM_ID: 100.0% (+13.5%)\n",
      "   Final mapping - DEF_TEAM_ID: 100.0% (+13.5%)\n",
      "   Created 15 team pair mappings for 20221018\n",
      "âœ… Saved/updated file for 2023: matchup_outputs/matchups_offense_2023.csv\n",
      "   Final offensive team mapping coverage: 100.0%\n",
      "   Final defensive team mapping coverage: 100.0%\n",
      "\n",
      "âœ… Done. Enhanced team mapping completed with 15 total team pair patterns learned.\n",
      "ðŸ“Š Team mapping improvements:\n",
      "   - Game-level opponent mapping: Map missing team IDs using known opponent teams\n",
      "   - Player consistency: Ensure same player has same team for all matchups on same date\n",
      "   - Cross-validation: Use both offensive and defensive team data to fill gaps\n",
      "   - Pattern learning: Build global team pairing database across all processed dates\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "index_frame = pd.read_csv('index_master.csv')\n",
    "filtered_df = index_frame[index_frame['year'] >= 2017]\n",
    "\n",
    "# Find player-year combinations with exactly one team\n",
    "unique_teams = (\n",
    "    filtered_df.groupby(['nba_id', 'year'])['team_id']\n",
    "    .nunique()\n",
    "    .reset_index()\n",
    "    .rename(columns={'team_id': 'team_count'})\n",
    ")\n",
    "single_team_players = unique_teams[unique_teams['team_count'] == 1]\n",
    "result_df = pd.merge(filtered_df, single_team_players[['nba_id', 'year']], on=['nba_id', 'year'])\n",
    "player_team_map = result_df[['nba_id', 'team_id', 'year']].drop_duplicates()\n",
    "player_team_map = player_team_map.rename(columns={'nba_id': 'player_id', 'year': 'season_end_year'})\n",
    "def format_date_to_url(date):\n",
    "    # Convert date from YYYYMMDD to datetime object\n",
    "    date_obj = datetime.strptime(str(date), '%Y%m%d')\n",
    "    \n",
    "    # Format the date as MM%2FDD%2FYYYY\n",
    "    formatted_date = date_obj.strftime('%m%%2F%d%%2F%Y')\n",
    "    \n",
    "    return formatted_date\n",
    "def pull_data(url):\n",
    "    headers = {\n",
    "        \"Host\": \"stats.nba.com\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\",\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://stats.nba.com/\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    json = response.json()\n",
    "\n",
    "    if len(json[\"resultSets\"]) == 1:\n",
    "        data = json[\"resultSets\"][0][\"rowSet\"]\n",
    "        columns = json[\"resultSets\"][0][\"headers\"]\n",
    "    else:\n",
    "        data = json[\"resultSets\"][\"rowSet\"]\n",
    "        columns = json[\"resultSets\"][\"headers\"][1]['columnNames']\n",
    "    \n",
    "    return pd.DataFrame.from_records(data, columns=columns)\n",
    "\n",
    "def get_matchups_for_date(season, date, mode='Offense', is_playoffs=False):\n",
    "    fdate = format_date_to_url(date)\n",
    "    stype = 'Playoffs' if is_playoffs else 'Regular%20Season'\n",
    "    url = (\n",
    "        'https://stats.nba.com/stats/leagueseasonmatchups?'\n",
    "        f'DateFrom={fdate}&DateTo={fdate}&DefPlayerID=&DefTeamID=&LeagueID=00'\n",
    "        f'&Matchup={mode}&OffPlayerID=&OffTeamID=&Outcome=&PORound=0&PerMode=Totals'\n",
    "        f'&Season={season}&SeasonType={stype}'\n",
    "    )\n",
    "    return pull_data(url)\n",
    "\n",
    "def create_game_team_mappings(matchups_df, date):\n",
    "    \"\"\"\n",
    "    Create team-to-opponent mappings for a specific game date.\n",
    "    Since teams play against each other on the same date, we can map team pairs.\n",
    "    \"\"\"\n",
    "    # Get all known team assignments for this date\n",
    "    known_teams = set()\n",
    "    \n",
    "    if 'OFF_TEAM_ID' in matchups_df.columns:\n",
    "        known_teams.update(matchups_df['OFF_TEAM_ID'].dropna().unique())\n",
    "    if 'DEF_TEAM_ID' in matchups_df.columns:\n",
    "        known_teams.update(matchups_df['DEF_TEAM_ID'].dropna().unique())\n",
    "    \n",
    "    # Create opponent mapping by finding team pairs that appear together\n",
    "    team_opponent_map = {}\n",
    "    \n",
    "    # Method 1: Use rows where both offensive and defensive teams are known\n",
    "    if 'OFF_TEAM_ID' in matchups_df.columns and 'DEF_TEAM_ID' in matchups_df.columns:\n",
    "        both_known = matchups_df[(matchups_df['OFF_TEAM_ID'].notna()) & \n",
    "                                (matchups_df['DEF_TEAM_ID'].notna())].copy()\n",
    "        \n",
    "        if not both_known.empty:\n",
    "            for _, row in both_known.iterrows():\n",
    "                off_team = row['OFF_TEAM_ID']\n",
    "                def_team = row['DEF_TEAM_ID']\n",
    "                \n",
    "                # Map each team to its opponent\n",
    "                team_opponent_map[off_team] = def_team\n",
    "                team_opponent_map[def_team] = off_team\n",
    "    \n",
    "    # Method 2: If we don't have direct mappings, infer from team co-occurrence patterns\n",
    "    if len(team_opponent_map) == 0:\n",
    "        # Group by potential game identifiers and find team pairs\n",
    "        if 'SEASON_ID' in matchups_df.columns or other_grouping_available:\n",
    "            # This would require more sophisticated logic based on available columns\n",
    "            # For now, we'll rely on the direct mapping method above\n",
    "            pass\n",
    "    \n",
    "    print(f\"   Created {len(team_opponent_map)//2} team pair mappings for {date}\")\n",
    "    return team_opponent_map\n",
    "\n",
    "def map_teams_to_matchups(matchups_df, player_team_map, season_end_year, date, mode='Defense'):\n",
    "    \"\"\"\n",
    "    Enhanced team mapping using game-level team pairing logic\n",
    "    \"\"\"\n",
    "    matchups_df = matchups_df.copy()\n",
    "    matchups_df['season_end_year'] = season_end_year\n",
    "    \n",
    "    # Step 1: Initial mapping from player_team_map\n",
    "    off_map = player_team_map.rename(columns={'player_id': 'OFF_PLAYER_ID', 'team_id': 'OFF_TEAM_ID'})\n",
    "    def_map = player_team_map.rename(columns={'player_id': 'DEF_PLAYER_ID', 'team_id': 'DEF_TEAM_ID'})\n",
    "    \n",
    "    matchups_df = pd.merge(matchups_df, off_map, on=['OFF_PLAYER_ID', 'season_end_year'], how='left')\n",
    "    matchups_df = pd.merge(matchups_df, def_map, on=['DEF_PLAYER_ID', 'season_end_year'], how='left')\n",
    "    \n",
    "    initial_off_coverage = matchups_df['OFF_TEAM_ID'].notna().mean()\n",
    "    initial_def_coverage = matchups_df['DEF_TEAM_ID'].notna().mean()\n",
    "    \n",
    "    print(f\"   Initial mapping - OFF_TEAM_ID: {initial_off_coverage:.1%}, DEF_TEAM_ID: {initial_def_coverage:.1%}\")\n",
    "    \n",
    "    # Step 2: Create game-level team opponent mappings\n",
    "    team_opponent_map = create_game_team_mappings(matchups_df, date)\n",
    "    \n",
    "    # Step 3: Fill missing team IDs using opponent mappings\n",
    "    if team_opponent_map:\n",
    "        # Fill missing offensive team IDs using known defensive team IDs\n",
    "        mask_missing_off = matchups_df['OFF_TEAM_ID'].isna() & matchups_df['DEF_TEAM_ID'].notna()\n",
    "        if mask_missing_off.any():\n",
    "            matchups_df.loc[mask_missing_off, 'OFF_TEAM_ID'] = matchups_df.loc[mask_missing_off, 'DEF_TEAM_ID'].map(team_opponent_map)\n",
    "            filled_off = matchups_df.loc[mask_missing_off, 'OFF_TEAM_ID'].notna().sum()\n",
    "            print(f\"   Filled {filled_off} missing OFF_TEAM_ID using opponent mapping\")\n",
    "        \n",
    "        # Fill missing defensive team IDs using known offensive team IDs\n",
    "        mask_missing_def = matchups_df['DEF_TEAM_ID'].isna() & matchups_df['OFF_TEAM_ID'].notna()\n",
    "        if mask_missing_def.any():\n",
    "            matchups_df.loc[mask_missing_def, 'DEF_TEAM_ID'] = matchups_df.loc[mask_missing_def, 'OFF_TEAM_ID'].map(team_opponent_map)\n",
    "            filled_def = matchups_df.loc[mask_missing_def, 'DEF_TEAM_ID'].notna().sum()\n",
    "            print(f\"   Filled {filled_def} missing DEF_TEAM_ID using opponent mapping\")\n",
    "    \n",
    "    # Step 4: Propagate consistent team assignments within the same date\n",
    "    # All matchups for the same player on the same date should have the same team\n",
    "    \n",
    "    # Propagate offensive team consistency\n",
    "    off_player_teams = (matchups_df[matchups_df['OFF_TEAM_ID'].notna()]\n",
    "                       .groupby('OFF_PLAYER_ID')['OFF_TEAM_ID']\n",
    "                       .first()\n",
    "                       .to_dict())\n",
    "    \n",
    "    mask_propagate_off = matchups_df['OFF_TEAM_ID'].isna()\n",
    "    matchups_df.loc[mask_propagate_off, 'OFF_TEAM_ID'] = (\n",
    "        matchups_df.loc[mask_propagate_off, 'OFF_PLAYER_ID'].map(off_player_teams)\n",
    "    )\n",
    "    \n",
    "    # Propagate defensive team consistency\n",
    "    def_player_teams = (matchups_df[matchups_df['DEF_TEAM_ID'].notna()]\n",
    "                       .groupby('DEF_PLAYER_ID')['DEF_TEAM_ID']\n",
    "                       .first()\n",
    "                       .to_dict())\n",
    "    \n",
    "    mask_propagate_def = matchups_df['DEF_TEAM_ID'].isna()\n",
    "    matchups_df.loc[mask_propagate_def, 'DEF_TEAM_ID'] = (\n",
    "        matchups_df.loc[mask_propagate_def, 'DEF_PLAYER_ID'].map(def_player_teams)\n",
    "    )\n",
    "    \n",
    "    # Step 5: Second round of opponent mapping after propagation\n",
    "    if team_opponent_map:\n",
    "        # Update team_opponent_map with any new teams we've mapped\n",
    "        updated_team_opponent_map = create_game_team_mappings(matchups_df, date)\n",
    "        if len(updated_team_opponent_map) > len(team_opponent_map):\n",
    "            team_opponent_map.update(updated_team_opponent_map)\n",
    "            print(f\"   Updated opponent mapping with {len(updated_team_opponent_map)//2} team pairs\")\n",
    "        \n",
    "        # Second round of filling\n",
    "        mask_missing_off = matchups_df['OFF_TEAM_ID'].isna() & matchups_df['DEF_TEAM_ID'].notna()\n",
    "        if mask_missing_off.any():\n",
    "            matchups_df.loc[mask_missing_off, 'OFF_TEAM_ID'] = matchups_df.loc[mask_missing_off, 'DEF_TEAM_ID'].map(team_opponent_map)\n",
    "        \n",
    "        mask_missing_def = matchups_df['DEF_TEAM_ID'].isna() & matchups_df['OFF_TEAM_ID'].notna()\n",
    "        if mask_missing_def.any():\n",
    "            matchups_df.loc[mask_missing_def, 'DEF_TEAM_ID'] = matchups_df.loc[mask_missing_def, 'OFF_TEAM_ID'].map(team_opponent_map)\n",
    "    \n",
    "    # Final coverage report\n",
    "    final_off_coverage = matchups_df['OFF_TEAM_ID'].notna().mean()\n",
    "    final_def_coverage = matchups_df['DEF_TEAM_ID'].notna().mean()\n",
    "    \n",
    "    print(f\"   Final mapping - OFF_TEAM_ID: {final_off_coverage:.1%} (+{final_off_coverage-initial_off_coverage:.1%})\")\n",
    "    print(f\"   Final mapping - DEF_TEAM_ID: {final_def_coverage:.1%} (+{final_def_coverage-initial_def_coverage:.1%})\")\n",
    "    \n",
    "    # Report unmapped players\n",
    "    unmapped_off = matchups_df[matchups_df['OFF_TEAM_ID'].isna()]['OFF_PLAYER_ID'].unique()\n",
    "    unmapped_def = matchups_df[matchups_df['DEF_TEAM_ID'].isna()]['DEF_PLAYER_ID'].unique()\n",
    "    \n",
    "    if len(unmapped_off) > 0:\n",
    "        print(f\"   Still unmapped offensive players: {len(unmapped_off)}\")\n",
    "    if len(unmapped_def) > 0:\n",
    "        print(f\"   Still unmapped defensive players: {len(unmapped_def)}\")\n",
    "    \n",
    "    return matchups_df\n",
    "\n",
    "def enhance_existing_file_mappings(filename, player_team_map):\n",
    "    \"\"\"\n",
    "    Apply enhanced team mapping to existing files to improve coverage\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        return\n",
    "        \n",
    "    print(f\"ðŸ”§ Enhancing existing file: {filename}\")\n",
    "    df = pd.read_csv(filename)\n",
    "    \n",
    "    # Group by date and apply enhanced mapping\n",
    "    enhanced_dfs = []\n",
    "    for date in df['game_date'].unique():\n",
    "        date_df = df[df['game_date'] == date].copy()\n",
    "        season_end_year = date_df['season_end_year'].iloc[0] if 'season_end_year' in date_df.columns else None\n",
    "        \n",
    "        if season_end_year:\n",
    "            enhanced_df = map_teams_to_matchups(date_df, player_team_map, season_end_year, date)\n",
    "            enhanced_dfs.append(enhanced_df)\n",
    "        else:\n",
    "            enhanced_dfs.append(date_df)\n",
    "    \n",
    "    if enhanced_dfs:\n",
    "        enhanced_final = pd.concat(enhanced_dfs, ignore_index=True)\n",
    "        enhanced_final.to_csv(filename, index=False)\n",
    "        print(f\"   âœ… Enhanced file saved\")\n",
    "\n",
    "# === Pipeline Start ===\n",
    "df = pd.read_csv(\"game_dates.csv\")\n",
    "df['season_end_year'] = df['season'].str.split('-').str[0].astype(int) + 1\n",
    "filtered_df = df[df['season_end_year'] >= 2023]\n",
    "unique_dates = filtered_df[['date', 'season', 'season_end_year', 'playoffs']].drop_duplicates()\n",
    "unique_dates = unique_dates.head(1)\n",
    "\n",
    "# Create game_id mapping lookup from game_dates file\n",
    "game_dates_lookup = df[['date', 'TEAM_ID', 'GAME_ID']].drop_duplicates()\n",
    "print(f\"ðŸ“‹ Loaded {len(game_dates_lookup)} game date-team-game_id mappings\")\n",
    "\n",
    "mode = 'Offense'  # Change to 'Defense' if needed\n",
    "\n",
    "# Optional: create output folder\n",
    "output_dir = 'matchup_outputs'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Track global team mappings across all dates for pattern learning\n",
    "global_team_mappings = {}\n",
    "\n",
    "grouped = unique_dates.groupby('season_end_year')\n",
    "for year, group in grouped:\n",
    "    filename = os.path.join(output_dir, f\"matchups_{mode.lower()}_{year}.csv\")\n",
    "\n",
    "    # Step 1: Check for existing file and enhance it first\n",
    "    if os.path.exists(filename):\n",
    "        existing_df = pd.read_csv(filename)\n",
    "        scraped_dates = set(existing_df['game_date'].unique())\n",
    "        print(f\"ðŸ“ Found existing file for {year} with {len(scraped_dates)} dates already scraped.\")\n",
    "        \n",
    "        # Enhance existing file with improved team mapping\n",
    "        enhance_existing_file_mappings(filename, player_team_map)\n",
    "        \n",
    "        # Reload the enhanced file\n",
    "        existing_df = pd.read_csv(filename)\n",
    "        \n",
    "        # Learn team patterns from existing data\n",
    "        if 'OFF_TEAM_ID' in existing_df.columns and 'DEF_TEAM_ID' in existing_df.columns:\n",
    "            for date in existing_df['game_date'].unique():\n",
    "                date_df = existing_df[existing_df['game_date'] == date]\n",
    "                date_mappings = create_game_team_mappings(date_df, date)\n",
    "                global_team_mappings.update(date_mappings)\n",
    "        \n",
    "        print(f\"   Learned {len(global_team_mappings)//2} team pair patterns from existing data\")\n",
    "    else:\n",
    "        scraped_dates = set()\n",
    "\n",
    "    # Step 2: Filter group to exclude scraped dates\n",
    "    group_to_scrape = group[~group['date'].isin(scraped_dates)]\n",
    "\n",
    "    if group_to_scrape.empty:\n",
    "        print(f\"â­ï¸  All dates already scraped for {year}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    all_matchups = []\n",
    "\n",
    "    for _, row in group_to_scrape.iterrows():\n",
    "        date = row['date']\n",
    "        season = row['season']\n",
    "        is_playoffs = row['playoffs']\n",
    "\n",
    "        print(f\"ðŸ“… Pulling {mode} matchup for {date} ({'Playoffs' if is_playoffs else 'Regular Season'})\")\n",
    "\n",
    "        try:\n",
    "            matchups_df = get_matchups_for_date(season, date, mode=mode, is_playoffs=is_playoffs)\n",
    "            \n",
    "            # Enhanced team mapping with game-level logic\n",
    "            matchups_df = map_teams_to_matchups(matchups_df, player_team_map, year, date, mode)\n",
    "            \n",
    "            # Add metadata\n",
    "            matchups_df['date'] = date\n",
    "            matchups_df['season'] = season\n",
    "            matchups_df['mode'] = mode\n",
    "            matchups_df['playoffs'] = is_playoffs\n",
    "            \n",
    "            # Learn from this date's team mappings for future use\n",
    "            date_mappings = create_game_team_mappings(matchups_df, date)\n",
    "            global_team_mappings.update(date_mappings)\n",
    "            matchups_df=matchups_df.merge(game_dates_lookup)\n",
    "            all_matchups.append(matchups_df)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error on {date} ({season}) - {e}\")\n",
    "\n",
    "        time.sleep(0.3)  # Rate limiting\n",
    "\n",
    "    if all_matchups:\n",
    "        new_df = pd.concat(all_matchups, ignore_index=True)\n",
    "        new_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        # Step 3: Append to existing file if it exists\n",
    "        if os.path.exists(filename):\n",
    "            existing_df = pd.read_csv(filename)  # Reload enhanced file\n",
    "            final_df = pd.concat([existing_df, new_df], ignore_index=True).drop_duplicates()\n",
    "        else:\n",
    "            final_df = new_df\n",
    "\n",
    "        final_df.to_csv(filename, index=False)\n",
    "        print(f\"âœ… Saved/updated file for {year}: {filename}\")\n",
    "        \n",
    "        # Report final team mapping coverage\n",
    "        if 'OFF_TEAM_ID' in final_df.columns:\n",
    "            off_coverage = final_df['OFF_TEAM_ID'].notna().mean()\n",
    "            print(f\"   Final offensive team mapping coverage: {off_coverage:.1%}\")\n",
    "        if 'DEF_TEAM_ID' in final_df.columns:\n",
    "            def_coverage = final_df['DEF_TEAM_ID'].notna().mean()\n",
    "            print(f\"   Final defensive team mapping coverage: {def_coverage:.1%}\")\n",
    "            \n",
    "    else:\n",
    "        print(f\"âš ï¸ No new data collected for {year}.\")\n",
    "\n",
    "print(f\"\\nâœ… Done. Enhanced team mapping completed with {len(global_team_mappings)//2} total team pair patterns learned.\")\n",
    "print(\"ðŸ“Š Team mapping improvements:\")\n",
    "print(\"   - Game-level opponent mapping: Map missing team IDs using known opponent teams\")\n",
    "print(\"   - Player consistency: Ensure same player has same team for all matchups on same date\")\n",
    "print(\"   - Cross-validation: Use both offensive and defensive team data to fill gaps\")\n",
    "print(\"   - Pattern learning: Build global team pairing database across all processed dates\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9d7b9e6d-a5ef-417d-9e8b-b1887f3c00fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['GAME_ID', 'TEAM_ID', 'HTM', 'VTM', 'date', 'season', 'playoffs',\n",
       "       'team', 'opp_team', 'season_end_year'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d87960b9-1014-4221-bf90-24f9eed9c6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pulling Offense matchups for 20231024 in season 2023-24\n",
      "Done. Saved to matchups_by_date_offense.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def pull_data(url):\n",
    "    headers = {\n",
    "        \"Host\": \"stats.nba.com\",\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:72.0) Gecko/20100101 Firefox/72.0\",\n",
    "        \"Accept\": \"application/json, text/plain, */*\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "        \"Connection\": \"keep-alive\",\n",
    "        \"Referer\": \"https://stats.nba.com/\"\n",
    "    }\n",
    "    response = requests.get(url, headers=headers)\n",
    "    response.raise_for_status()\n",
    "    json = response.json()\n",
    "\n",
    "    if len(json[\"resultSets\"]) == 1:\n",
    "        data = json[\"resultSets\"][0][\"rowSet\"]\n",
    "        columns = json[\"resultSets\"][0][\"headers\"]\n",
    "    else:\n",
    "        data = json[\"resultSets\"][\"rowSet\"]\n",
    "        columns = json[\"resultSets\"][\"headers\"][1]['columnNames']\n",
    "    \n",
    "    df = pd.DataFrame.from_records(data, columns=columns)\n",
    "    return df\n",
    "\n",
    "def get_omatchups_by_date(season, date, mode='Offense', ps=False):\n",
    "    \"\"\"\n",
    "    Pulls offensive or defensive matchup data for a specific date in a given season.\n",
    "    mode: 'Offense' or 'Defense'\n",
    "    \"\"\"\n",
    "    stype = 'Playoffs' if ps else 'Regular%20Season'\n",
    "    url = (\n",
    "        'https://stats.nba.com/stats/leagueseasonmatchups?'\n",
    "        f'DateFrom={date}&DateTo={date}&DefPlayerID=&DefTeamID=&LeagueID=00'\n",
    "        f'&Matchup={mode}&OffPlayerID=&OffTeamID=&Outcome=&PORound=0&PerMode=Totals'\n",
    "        f'&Season={season}&SeasonType={stype}'\n",
    "    )\n",
    "    return pull_data(url)\n",
    "\n",
    "# === Pipeline ===\n",
    "\n",
    "# Load game_dates.csv\n",
    "df = pd.read_csv(\"game_dates.csv\")\n",
    "\n",
    "# Convert season and filter from 2017 onward\n",
    "df['season_end_year'] = df['season'].str.split('-').str[0].astype(int) + 1\n",
    "filtered_df = df[df['season_end_year'] >= 2024]\n",
    "\n",
    "# Unique date/season pairs\n",
    "unique_dates = filtered_df[['date', 'season']].drop_duplicates()\n",
    "unique_dates=unique_dates.head(1)\n",
    "# Change mode here: 'Offense' or 'Defense'\n",
    "mode = 'Offense'  # Change to 'Defense' if needed\n",
    "\n",
    "all_data = []\n",
    "\n",
    "for _, row in unique_dates.iterrows():\n",
    "    date = row['date']\n",
    "    season = row['season']\n",
    "    print(f\"Pulling {mode} matchups for {date} in season {season}\")\n",
    "    \n",
    "    try:\n",
    "        matchup_df = get_omatchups_by_date(season, date, mode=mode)\n",
    "        matchup_df['game_date'] = date\n",
    "        matchup_df['season'] = season\n",
    "        matchup_df['mode'] = mode\n",
    "        all_data.append(matchup_df)\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to pull data for {date} in {season}: {e}\")\n",
    "    \n",
    "    time.sleep(0.7)  # Be kind to the NBA API\n",
    "\n",
    "# Combine\n",
    "final_df = pd.concat(all_data, ignore_index=True)\n",
    "\n",
    "# Save\n",
    "filename = f\"matchups_by_date_{mode.lower()}.csv\"\n",
    "final_df.to_csv(filename, index=False)\n",
    "print(f\"Done. Saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8788f0ce-379e-475c-8549-c5e313ffd9a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
