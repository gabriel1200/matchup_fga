{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7dc145ac-f8b4-4023-9fda-82df3c918fa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded master record with 28019 rows\n",
      "Columns: ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ABBREVIATION', 'TEAM_ID', 'GAME_ID', 'year']\n",
      "Found 28019 unique player-game combinations to process\n",
      "Data distribution by year:\n",
      "  2025: 28019 combinations\n",
      "1631260\n",
      "22400066\n",
      "1610612749.0\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[31mSystemExit\u001b[39m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaber/basketball/web_app/venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3680: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "def fetch_details(game_id, player_id, team_id, context_measure=\"DEF_FGA\"):\n",
    "    \"\"\"\n",
    "    Fetch video details for a specific player in a specific game\n",
    "    \"\"\"\n",
    "    base_url = \"https://stats.nba.com/stats/videodetailsasset\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Referer\": \"https://www.nba.com\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Origin\": \"https://www.nba.com\"\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"GameID\": '00'+str(game_id),\n",
    "        \"GameEventID\": \"\",\n",
    "        \"PlayerID\": str(player_id),\n",
    "        \"TeamID\": str(team_id),\n",
    "        \"Season\": \"\",\n",
    "        \"SeasonType\": \"\",\n",
    "        \"AheadBehind\": \"\",\n",
    "        \"CFID\": \"\",\n",
    "        \"CFPARAMS\": \"\",\n",
    "        \"ClutchTime\": \"\",\n",
    "        \"Conference\": \"\",\n",
    "        \"ContextFilter\": \"\",\n",
    "        \"ContextMeasure\": context_measure,\n",
    "        \"DateFrom\": \"\",\n",
    "        \"DateTo\": \"\",\n",
    "        \"Division\": \"\",\n",
    "        \"EndPeriod\": 0,\n",
    "        \"EndRange\": 40800,\n",
    "        \"GROUP_ID\": \"\",\n",
    "        \"GameSegment\": \"\",\n",
    "        \"GroupID\": \"\",\n",
    "        \"GroupMode\": \"\",\n",
    "        \"GroupQuantity\": 5,\n",
    "        \"LastNGames\": 0,\n",
    "        \"Location\": \"\",\n",
    "        \"Month\": 0,\n",
    "        \"OnOff\": \"\",\n",
    "        \"OppPlayerID\": \"\",\n",
    "        \"OpponentTeamID\": 0,\n",
    "        \"Outcome\": \"\",\n",
    "        \"PORound\": 0,\n",
    "        \"Period\": 0,\n",
    "        \"PlayerID1\": \"\",\n",
    "        \"PlayerID2\": \"\",\n",
    "        \"PlayerID3\": \"\",\n",
    "        \"PlayerID4\": \"\",\n",
    "        \"PlayerID5\": \"\",\n",
    "        \"PlayerPosition\": \"\",\n",
    "        \"PointDiff\": \"\",\n",
    "        \"Position\": \"\",\n",
    "        \"RangeType\": 0,\n",
    "        \"RookieYear\": \"\",\n",
    "        \"SeasonSegment\": \"\",\n",
    "        \"ShotClockRange\": \"\",\n",
    "        \"StartPeriod\": 0,\n",
    "        \"StartRange\": 0,\n",
    "        \"StarterBench\": \"\",\n",
    "        \"VsConference\": \"\",\n",
    "        \"VsDivision\": \"\",\n",
    "        \"VsPlayerID1\": \"\",\n",
    "        \"VsPlayerID2\": \"\",\n",
    "        \"VsPlayerID3\": \"\",\n",
    "        \"VsPlayerID4\": \"\",\n",
    "        \"VsPlayerID5\": \"\",\n",
    "        \"VsTeamID\": \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Request failed with status code {response.status_code} for Player {player_id}, Game {game_id}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error for Player {player_id}, Game {game_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_video_data(video_json, player_id, team_id, player_name, year):\n",
    "    \"\"\"\n",
    "    Process the video JSON response into a DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if video_json and 'resultSets' in video_json and 'playlist' in video_json['resultSets']:\n",
    "            playlist = video_json['resultSets']['playlist']\n",
    "            if playlist:  # Check if playlist is not empty\n",
    "                df = pd.DataFrame(playlist)\n",
    "                if not df.empty and all(col in df.columns for col in ['gi', 'ei', 'dsc']):\n",
    "                    df = df[['gi', 'ei', 'dsc']]\n",
    "                    df['def_id'] = player_id\n",
    "                    df['team_id'] = team_id\n",
    "                    df['player_name'] = player_name\n",
    "                    df['year'] = year\n",
    "                    return df\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data for Player {player_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_batch_data_by_year(year_data_dict, batch_num, output_dir=\"scraped_data\"):\n",
    "    \"\"\"\n",
    "    Save batch data organized by year\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    total_records_saved = 0\n",
    "    \n",
    "    for year, data_list in year_data_dict.items():\n",
    "        if data_list:\n",
    "            # Create year-specific directory\n",
    "            year_dir = os.path.join(output_dir, f\"year_{year}\")\n",
    "            if not os.path.exists(year_dir):\n",
    "                os.makedirs(year_dir)\n",
    "            \n",
    "            # Combine all data for this year\n",
    "            combined_df = pd.concat(data_list, ignore_index=True)\n",
    "            \n",
    "            # Save with timestamp and batch number\n",
    "            filename = f\"{year_dir}/batch_{batch_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            combined_df.to_csv(filename, index=False)\n",
    "            \n",
    "            records_count = len(combined_df)\n",
    "            total_records_saved += records_count\n",
    "            print(f\"Saved {year} batch {batch_num} with {records_count} records to {filename}\")\n",
    "    \n",
    "    return total_records_saved\n",
    "\n",
    "def scrape_nba_video_details(master_record_path, context_measure=\"DEF_FGA\", \n",
    "                           delay_between_requests=2, batch_size=50):\n",
    "    \"\"\"\n",
    "    Main scraper function that processes all unique player_id and game_id combinations,\n",
    "    organizing data by year\n",
    "    \"\"\"\n",
    "    # Load master record\n",
    "    try:\n",
    "        master_record = pd.read_csv(master_record_path)\n",
    "        master_record['TEAM_ID']=master_record['TEAM_ID'].astype(int)\n",
    "        master_record['PLAYER_ID']=master_record['PLAYER_ID'].astype(int)\n",
    "\n",
    "        master_record=master_record[master_record.year>2024]\n",
    "        print(f\"Loaded master record with {len(master_record)} rows\")\n",
    "        print(f\"Columns: {list(master_record.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading master record: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get unique combinations of player_id and game_id\n",
    "    unique_combinations = master_record[['PLAYER_ID', 'GAME_ID', 'TEAM_ID', 'PLAYER_NAME', 'year']].drop_duplicates()\n",
    "    total_combinations = len(unique_combinations)\n",
    "    print(f\"Found {total_combinations} unique player-game combinations to process\")\n",
    "    \n",
    "    # Show year distribution\n",
    "    year_counts = unique_combinations['year'].value_counts().sort_index()\n",
    "    print(f\"Data distribution by year:\")\n",
    "    for year, count in year_counts.items():\n",
    "        print(f\"  {year}: {count} combinations\")\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    year_data = {}  # Dictionary to hold data by year\n",
    "    successful_requests = 0\n",
    "    failed_requests = 0\n",
    "    batch_num = 1\n",
    "    total_records_saved = 0\n",
    "    \n",
    "    # Process each combination\n",
    "    for idx, row in unique_combinations.iterrows():\n",
    "        player_id = str(row['PLAYER_ID'])\n",
    "        game_id = str(row['GAME_ID'])\n",
    "        team_id = str(row['TEAM_ID'])\n",
    "        player_name = row['PLAYER_NAME']\n",
    "        year = row['year']\n",
    "        print(player_id)\n",
    "        print(game_id)\n",
    "        print(team_id)\n",
    "        sys.exit()\n",
    "        \n",
    "        print(f\"Processing {idx + 1}/{total_combinations}: Player {player_name} ({player_id}) in Game {game_id} - {year}\")\n",
    "        \n",
    "        # Fetch video details\n",
    "        video_json = fetch_details(game_id, player_id, team_id, context_measure)\n",
    "        \n",
    "        if video_json:\n",
    "            # Process the data\n",
    "            processed_df = process_video_data(video_json, player_id, team_id, player_name, year)\n",
    "            \n",
    "            if processed_df is not None and not processed_df.empty:\n",
    "                # Initialize year key if it doesn't exist\n",
    "                if year not in year_data:\n",
    "                    year_data[year] = []\n",
    "                \n",
    "                # Add data to the appropriate year\n",
    "                year_data[year].append(processed_df)\n",
    "                successful_requests += 1\n",
    "                print(f\"  ✓ Found {len(processed_df)} video records for {year}\")\n",
    "            else:\n",
    "                print(f\"  - No video data available for {year}\")\n",
    "        else:\n",
    "            failed_requests += 1\n",
    "        \n",
    "        # Check if we should save a batch (based on total successful requests)\n",
    "        if successful_requests > 0 and successful_requests % batch_size == 0:\n",
    "            records_saved = save_batch_data_by_year(year_data, batch_num)\n",
    "            total_records_saved += records_saved\n",
    "            year_data = {}  # Clear all year data after saving\n",
    "            batch_num += 1\n",
    "        \n",
    "        # Add delay between requests to be respectful to the API\n",
    "        if idx < len(unique_combinations) - 1:  # Don't delay after the last request\n",
    "            time.sleep(delay_between_requests)\n",
    "    \n",
    "    # Save any remaining data\n",
    "    if any(year_data.values()):  # Check if there's any data left to save\n",
    "        records_saved = save_batch_data_by_year(year_data, batch_num)\n",
    "        total_records_saved += records_saved\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== SCRAPING COMPLETE ===\")\n",
    "    print(f\"Total combinations processed: {total_combinations}\")\n",
    "    print(f\"Successful requests: {successful_requests}\")\n",
    "    print(f\"Failed requests: {failed_requests}\")\n",
    "    print(f\"Total video records saved: {total_records_saved}\")\n",
    "    print(f\"Success rate: {(successful_requests/total_combinations)*100:.1f}%\")\n",
    "\n",
    "def combine_batches_by_year(output_dir=\"scraped_data\"):\n",
    "    \"\"\"\n",
    "    Combine all batch files within each year into a single CSV per year\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Output directory {output_dir} does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Find all year directories\n",
    "    year_dirs = [d for d in os.listdir(output_dir) if d.startswith('year_') and os.path.isdir(os.path.join(output_dir, d))]\n",
    "    \n",
    "    if not year_dirs:\n",
    "        print(\"No year directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(year_dirs)} year directories: {sorted(year_dirs)}\")\n",
    "    \n",
    "    for year_dir in sorted(year_dirs):\n",
    "        year_path = os.path.join(output_dir, year_dir)\n",
    "        year = year_dir.replace('year_', '')\n",
    "        \n",
    "        # Find all batch files for this year\n",
    "        batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "        \n",
    "        if not batch_files:\n",
    "            print(f\"No batch files found for {year}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing {year}: Found {len(batch_files)} batch files\")\n",
    "        \n",
    "        # Load and combine all batches for this year\n",
    "        year_batches = []\n",
    "        for file in batch_files:\n",
    "            file_path = os.path.join(year_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            year_batches.append(df)\n",
    "            print(f\"  Loaded {file}: {len(df)} records\")\n",
    "        \n",
    "        # Combine all batches for this year\n",
    "        if year_batches:\n",
    "            final_df = pd.concat(year_batches, ignore_index=True)\n",
    "            \n",
    "            # Remove duplicates if any\n",
    "            initial_count = len(final_df)\n",
    "            final_df = final_df.drop_duplicates()\n",
    "            final_count = len(final_df)\n",
    "            \n",
    "            if initial_count != final_count:\n",
    "                print(f\"  Removed {initial_count - final_count} duplicate records for {year}\")\n",
    "            \n",
    "            # Save final combined file for this year\n",
    "            final_filename = f\"combined_video_details_{year}.csv\"\n",
    "            final_path = os.path.join(year_path, final_filename)\n",
    "            final_df.to_csv(final_path, index=False)\n",
    "            print(f\"  ✓ Combined file saved: {final_path} with {final_count} total records\")\n",
    "\n",
    "def get_year_summary(output_dir=\"scraped_data\"):\n",
    "    \"\"\"\n",
    "    Print a summary of data collected by year\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Output directory {output_dir} does not exist\")\n",
    "        return\n",
    "    \n",
    "    year_dirs = [d for d in os.listdir(output_dir) if d.startswith('year_') and os.path.isdir(os.path.join(output_dir, d))]\n",
    "    \n",
    "    if not year_dirs:\n",
    "        print(\"No year directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== DATA SUMMARY BY YEAR ===\")\n",
    "    total_records = 0\n",
    "    \n",
    "    for year_dir in sorted(year_dirs):\n",
    "        year = year_dir.replace('year_', '')\n",
    "        year_path = os.path.join(output_dir, year_dir)\n",
    "        \n",
    "        # Look for combined file first, otherwise count batch files\n",
    "        combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "        \n",
    "        if os.path.exists(combined_file):\n",
    "            df = pd.read_csv(combined_file)\n",
    "            record_count = len(df)\n",
    "            print(f\"{year}: {record_count:,} records (combined)\")\n",
    "        else:\n",
    "            # Count records in batch files\n",
    "            batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "            record_count = 0\n",
    "            for file in batch_files:\n",
    "                file_path = os.path.join(year_path, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                record_count += len(df)\n",
    "            print(f\"{year}: {record_count:,} records ({len(batch_files)} batch files)\")\n",
    "        \n",
    "        total_records += record_count\n",
    "    \n",
    "    print(f\"\\nTotal records across all years: {total_records:,}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the scraper\n",
    "    scrape_nba_video_details(\n",
    "        master_record_path='master_record.csv',\n",
    "        context_measure=\"DEF_FGA\",\n",
    "        delay_between_requests=2,  # 2 seconds between requests\n",
    "        batch_size=50  # Save every 50 successful requests\n",
    "    )\n",
    "    \n",
    "    # Combine all batch files by year\n",
    "    combine_batches_by_year()\n",
    "    \n",
    "    # Print summary\n",
    "    get_year_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe9717b-6fab-459d-be6f-fe6dd1013d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
