{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02084670-14ac-48e6-ba26-d6543e3c441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import threading\n",
    "from queue import Queue\n",
    "import logging\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class NBAScraper:\n",
    "    def __init__(self, max_concurrent_requests=10, request_delay=0.1, \n",
    "                 batch_size=50, output_dir=\"scraped_data\"):\n",
    "        self.max_concurrent_requests = max_concurrent_requests\n",
    "        self.request_delay = request_delay\n",
    "        self.batch_size = batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.session = None\n",
    "        self.log_data = {}\n",
    "        self.lock = threading.Lock()\n",
    "        \n",
    "        # Headers for requests\n",
    "        self.headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Referer\": \"https://www.nba.com\",\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Origin\": \"https://www.nba.com\"\n",
    "        }\n",
    "\n",
    "    def load_scrape_log(self, log_file=\"scrape_log.json\"):\n",
    "        \"\"\"Load the scrape log that tracks all attempted scrapes\"\"\"\n",
    "        if os.path.exists(log_file):\n",
    "            try:\n",
    "                with open(log_file, 'r') as f:\n",
    "                    self.log_data = json.load(f)\n",
    "                logger.info(f\"Loaded scrape log with {len(self.log_data)} previous attempts\")\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error loading scrape log: {e}\")\n",
    "                self.log_data = {}\n",
    "        else:\n",
    "            logger.info(\"No existing scrape log found. Starting fresh.\")\n",
    "            self.log_data = {}\n",
    "\n",
    "    def save_scrape_log(self, log_file=\"scrape_log.json\"):\n",
    "        \"\"\"Save the scrape log to disk\"\"\"\n",
    "        try:\n",
    "            with open(log_file, 'w') as f:\n",
    "                json.dump(self.log_data, f, indent=2)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error saving scrape log: {e}\")\n",
    "\n",
    "    def update_scrape_log(self, player_id, game_id, team_id, player_name, year, \n",
    "                         success, record_count=0, error_msg=None):\n",
    "        \"\"\"Thread-safe update of scrape log\"\"\"\n",
    "        key = f\"{player_id}_{game_id}\"\n",
    "        with self.lock:\n",
    "            self.log_data[key] = {\n",
    "                \"player_id\": str(player_id),\n",
    "                \"game_id\": str(game_id),\n",
    "                \"team_id\": str(team_id),\n",
    "                \"player_name\": player_name,\n",
    "                \"year\": year,\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"success\": success,\n",
    "                \"record_count\": record_count,\n",
    "                \"error_msg\": error_msg\n",
    "            }\n",
    "\n",
    "    async def fetch_details_async(self, session, game_id, player_id, team_id, context_measure=\"DEF_FGA\"):\n",
    "        \"\"\"Async version of fetch_details\"\"\"\n",
    "        base_url = \"https://stats.nba.com/stats/videodetailsasset\"\n",
    "        \n",
    "        params = {\n",
    "            \"GameID\": '00'+str(game_id),\n",
    "            \"GameEventID\": \"\",\n",
    "            \"PlayerID\": str(player_id),\n",
    "            \"TeamID\": str(team_id),\n",
    "            \"Season\": \"\",\n",
    "            \"SeasonType\": \"\",\n",
    "            \"AheadBehind\": \"\",\n",
    "            \"CFID\": \"\",\n",
    "            \"CFPARAMS\": \"\",\n",
    "            \"ClutchTime\": \"\",\n",
    "            \"Conference\": \"\",\n",
    "            \"ContextFilter\": \"\",\n",
    "            \"ContextMeasure\": context_measure,\n",
    "            \"DateFrom\": \"\",\n",
    "            \"DateTo\": \"\",\n",
    "            \"Division\": \"\",\n",
    "            \"EndPeriod\": 0,\n",
    "            \"EndRange\": 40800,\n",
    "            \"GROUP_ID\": \"\",\n",
    "            \"GameSegment\": \"\",\n",
    "            \"GroupID\": \"\",\n",
    "            \"GroupMode\": \"\",\n",
    "            \"GroupQuantity\": 5,\n",
    "            \"LastNGames\": 0,\n",
    "            \"Location\": \"\",\n",
    "            \"Month\": 0,\n",
    "            \"OnOff\": \"\",\n",
    "            \"OppPlayerID\": \"\",\n",
    "            \"OpponentTeamID\": 0,\n",
    "            \"Outcome\": \"\",\n",
    "            \"PORound\": 0,\n",
    "            \"Period\": 0,\n",
    "            \"PlayerID1\": \"\",\n",
    "            \"PlayerID2\": \"\",\n",
    "            \"PlayerID3\": \"\",\n",
    "            \"PlayerID4\": \"\",\n",
    "            \"PlayerID5\": \"\",\n",
    "            \"PlayerPosition\": \"\",\n",
    "            \"PointDiff\": \"\",\n",
    "            \"Position\": \"\",\n",
    "            \"RangeType\": 0,\n",
    "            \"RookieYear\": \"\",\n",
    "            \"SeasonSegment\": \"\",\n",
    "            \"ShotClockRange\": \"\",\n",
    "            \"StartPeriod\": 0,\n",
    "            \"StartRange\": 0,\n",
    "            \"StarterBench\": \"\",\n",
    "            \"VsConference\": \"\",\n",
    "            \"VsDivision\": \"\",\n",
    "            \"VsPlayerID1\": \"\",\n",
    "            \"VsPlayerID2\": \"\",\n",
    "            \"VsPlayerID3\": \"\",\n",
    "            \"VsPlayerID4\": \"\",\n",
    "            \"VsPlayerID5\": \"\",\n",
    "            \"VsTeamID\": \"\"\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            async with session.get(base_url, headers=self.headers, params=params, timeout=30) as response:\n",
    "                if response.status == 200:\n",
    "                    return await response.json()\n",
    "                else:\n",
    "                    logger.warning(f\"Request failed with status {response.status} for Player {player_id}, Game {game_id}\")\n",
    "                    return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Request error for Player {player_id}, Game {game_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_video_data(self, video_json, player_id, team_id, player_name, year):\n",
    "        \"\"\"Process the video JSON response into a DataFrame\"\"\"\n",
    "        try:\n",
    "            if video_json and 'resultSets' in video_json and 'playlist' in video_json['resultSets']:\n",
    "                playlist = video_json['resultSets']['playlist']\n",
    "                if playlist:\n",
    "                    df = pd.DataFrame(playlist)\n",
    "                    if not df.empty and all(col in df.columns for col in ['gi', 'ei', 'dsc']):\n",
    "                        df = df[['gi', 'ei', 'dsc']]\n",
    "                        df['def_id'] = player_id\n",
    "                        df['team_id'] = team_id\n",
    "                        df['player_name'] = player_name\n",
    "                        df['year'] = year\n",
    "                        return df\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing data for Player {player_id}: {e}\")\n",
    "            return None\n",
    "\n",
    "    async def process_single_request(self, session, player_id, game_id, team_id, player_name, year, context_measure=\"DEF_FGA\"):\n",
    "        \"\"\"Process a single request asynchronously\"\"\"\n",
    "        # Add small delay to avoid overwhelming the server\n",
    "        await asyncio.sleep(self.request_delay)\n",
    "        \n",
    "        video_json = await self.fetch_details_async(session, game_id, player_id, team_id, context_measure)\n",
    "        \n",
    "        if video_json:\n",
    "            processed_df = self.process_video_data(video_json, player_id, team_id, player_name, year)\n",
    "            \n",
    "            if processed_df is not None and not processed_df.empty:\n",
    "                record_count = len(processed_df)\n",
    "                self.update_scrape_log(player_id, game_id, team_id, player_name, year, True, record_count)\n",
    "                return processed_df, True, record_count\n",
    "            else:\n",
    "                self.update_scrape_log(player_id, game_id, team_id, player_name, year, True, 0)\n",
    "                return None, True, 0\n",
    "        else:\n",
    "            self.update_scrape_log(player_id, game_id, team_id, player_name, year, False, 0, \"API request failed\")\n",
    "            return None, False, 0\n",
    "\n",
    "    async def scrape_batch_async(self, batch_data, context_measure=\"DEF_FGA\"):\n",
    "        \"\"\"Scrape a batch of requests asynchronously\"\"\"\n",
    "        connector = aiohttp.TCPConnector(limit=self.max_concurrent_requests, limit_per_host=self.max_concurrent_requests)\n",
    "        timeout = aiohttp.ClientTimeout(total=60)\n",
    "        \n",
    "        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:\n",
    "            # Create semaphore to limit concurrent requests\n",
    "            semaphore = asyncio.Semaphore(self.max_concurrent_requests)\n",
    "            \n",
    "            async def limited_request(row):\n",
    "                async with semaphore:\n",
    "                    return await self.process_single_request(\n",
    "                        session, row['PLAYER_ID'], row['GAME_ID'], row['TEAM_ID'], \n",
    "                        row['PLAYER_NAME'], row['year'], context_measure\n",
    "                    )\n",
    "            \n",
    "            # Execute all requests concurrently\n",
    "            tasks = [limited_request(row) for _, row in batch_data.iterrows()]\n",
    "            results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            \n",
    "            return results\n",
    "\n",
    "    def save_batch_data_by_year(self, year_data_dict, batch_num):\n",
    "        \"\"\"Save batch data organized by year\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        \n",
    "        total_records_saved = 0\n",
    "        \n",
    "        for year, data_list in year_data_dict.items():\n",
    "            if data_list:\n",
    "                # Create year-specific directory\n",
    "                year_dir = os.path.join(self.output_dir, f\"year_{year}\")\n",
    "                if not os.path.exists(year_dir):\n",
    "                    os.makedirs(year_dir)\n",
    "                \n",
    "                # Combine all data for this year\n",
    "                combined_df = pd.concat(data_list, ignore_index=True)\n",
    "                \n",
    "                # Save with timestamp and batch number\n",
    "                filename = f\"{year_dir}/batch_{batch_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                combined_df.to_csv(filename, index=False)\n",
    "                \n",
    "                records_count = len(combined_df)\n",
    "                total_records_saved += records_count\n",
    "                logger.info(f\"Saved {year} batch {batch_num} with {records_count} records to {filename}\")\n",
    "        \n",
    "        return total_records_saved\n",
    "\n",
    "    def load_already_scraped_combinations_from_files(self):\n",
    "        \"\"\"Load all previously scraped combinations from existing batch files\"\"\"\n",
    "        already_scraped = set()\n",
    "        \n",
    "        if not os.path.exists(self.output_dir):\n",
    "            logger.info(\"No existing scraped data directory found.\")\n",
    "            return already_scraped\n",
    "        \n",
    "        # Find all year directories\n",
    "        year_dirs = [d for d in os.listdir(self.output_dir) if d.startswith('year_') and os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "        \n",
    "        if not year_dirs:\n",
    "            logger.info(\"No existing year directories found.\")\n",
    "            return already_scraped\n",
    "        \n",
    "        logger.info(f\"Checking existing scraped data in {len(year_dirs)} year directories...\")\n",
    "        \n",
    "        for year_dir in year_dirs:\n",
    "            year_path = os.path.join(self.output_dir, year_dir)\n",
    "            year = year_dir.replace('year_', '')\n",
    "            \n",
    "            # Check for combined file first\n",
    "            combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "            \n",
    "            if os.path.exists(combined_file):\n",
    "                logger.info(f\"  Loading from combined file: {combined_file}\")\n",
    "                try:\n",
    "                    df = pd.read_csv(combined_file)\n",
    "                    if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                        combinations = set(zip(df['def_id'].astype(str), df['gi'].astype(str)))\n",
    "                        already_scraped.update(combinations)\n",
    "                        logger.info(f\"    Found {len(combinations)} combinations for {year}\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"    Error reading {combined_file}: {e}\")\n",
    "            else:\n",
    "                # Check batch files\n",
    "                batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "                year_combinations = 0\n",
    "                \n",
    "                for batch_file in batch_files:\n",
    "                    batch_path = os.path.join(year_path, batch_file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(batch_path)\n",
    "                        if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                            combinations = set(zip(df['def_id'].astype(str), df['gi'].astype(str)))\n",
    "                            already_scraped.update(combinations)\n",
    "                            year_combinations += len(combinations)\n",
    "                    except Exception as e:\n",
    "                        logger.error(f\"    Error reading {batch_path}: {e}\")\n",
    "                \n",
    "                if year_combinations > 0:\n",
    "                    logger.info(f\"  Found {year_combinations} combinations from {len(batch_files)} batch files for {year}\")\n",
    "        \n",
    "        logger.info(f\"Total unique combinations with data in CSV files: {len(already_scraped)}\")\n",
    "        return already_scraped\n",
    "\n",
    "    def scrape_nba_video_details(self, master_record_path, context_measure=\"DEF_FGA\", \n",
    "                               force_retry_failed=False):\n",
    "        \"\"\"Main scraper function with async processing\"\"\"\n",
    "        \n",
    "        # Load scrape log\n",
    "        logger.info(\"=== LOADING SCRAPE LOG ===\")\n",
    "        self.load_scrape_log()\n",
    "        \n",
    "        # Load already scraped combinations from CSV files\n",
    "        logger.info(\"=== LOADING EXISTING CSV DATA ===\")\n",
    "        csv_combinations = self.load_already_scraped_combinations_from_files()\n",
    "        \n",
    "        # Migrate CSV data to log if needed\n",
    "        self.migrate_csv_data_to_log(csv_combinations)\n",
    "        \n",
    "        # Load master record\n",
    "        try:\n",
    "            master_record = pd.read_csv(master_record_path)\n",
    "            master_record['TEAM_ID'] = master_record['TEAM_ID'].astype(int)\n",
    "            master_record['PLAYER_ID'] = master_record['PLAYER_ID'].astype(int)\n",
    "            master_record = master_record[master_record.year > 2024]\n",
    "            logger.info(f\"Loaded master record with {len(master_record)} rows\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading master record: {e}\")\n",
    "            return\n",
    "        \n",
    "        # Get unique combinations and filter\n",
    "        unique_combinations = master_record[['PLAYER_ID', 'GAME_ID', 'TEAM_ID', 'PLAYER_NAME', 'year']].drop_duplicates()\n",
    "        unique_combinations['scrape_key'] = unique_combinations['PLAYER_ID'].astype(str) + \"_\" + unique_combinations['GAME_ID'].astype(str)\n",
    "        \n",
    "        # Filter based on scrape log\n",
    "        already_attempted = set(self.log_data.keys())\n",
    "        unique_combinations['log_status'] = unique_combinations['scrape_key'].apply(\n",
    "            lambda x: 'never_attempted' if x not in already_attempted else \n",
    "                     ('successful' if self.log_data[x]['success'] else 'failed')\n",
    "        )\n",
    "        \n",
    "        never_attempted = unique_combinations[unique_combinations['log_status'] == 'never_attempted']\n",
    "        previously_failed = unique_combinations[unique_combinations['log_status'] == 'failed']\n",
    "        \n",
    "        # Determine what to scrape\n",
    "        if force_retry_failed:\n",
    "            to_scrape_df = pd.concat([never_attempted, previously_failed])\n",
    "            logger.info(f\"Force retry enabled: Will attempt {len(to_scrape_df)} combinations\")\n",
    "        else:\n",
    "            to_scrape_df = never_attempted\n",
    "            logger.info(f\"Will attempt {len(to_scrape_df)} never-attempted combinations\")\n",
    "        \n",
    "        if len(to_scrape_df) == 0:\n",
    "            logger.info(\"ðŸŽ‰ No combinations to scrape!\")\n",
    "            return\n",
    "        \n",
    "        # Process in batches\n",
    "        logger.info(\"=== STARTING ASYNC SCRAPING PROCESS ===\")\n",
    "        \n",
    "        total_batches = (len(to_scrape_df) + self.batch_size - 1) // self.batch_size\n",
    "        successful_requests = 0\n",
    "        failed_requests = 0\n",
    "        total_records_saved = 0\n",
    "        \n",
    "        for batch_idx in range(total_batches):\n",
    "            start_idx = batch_idx * self.batch_size\n",
    "            end_idx = min((batch_idx + 1) * self.batch_size, len(to_scrape_df))\n",
    "            batch_data = to_scrape_df.iloc[start_idx:end_idx]\n",
    "            \n",
    "            logger.info(f\"Processing batch {batch_idx + 1}/{total_batches} ({len(batch_data)} items)\")\n",
    "            \n",
    "            # Process batch asynchronously\n",
    "            try:\n",
    "                results = asyncio.run(self.scrape_batch_async(batch_data, context_measure))\n",
    "                \n",
    "                # Process results\n",
    "                year_data = {}\n",
    "                batch_successful = 0\n",
    "                batch_failed = 0\n",
    "                \n",
    "                for i, result in enumerate(results):\n",
    "                    if isinstance(result, Exception):\n",
    "                        logger.error(f\"Exception in batch processing: {result}\")\n",
    "                        batch_failed += 1\n",
    "                        continue\n",
    "                    \n",
    "                    processed_df, success, record_count = result\n",
    "                    row = batch_data.iloc[i]\n",
    "                    \n",
    "                    if success:\n",
    "                        batch_successful += 1\n",
    "                        if processed_df is not None and not processed_df.empty:\n",
    "                            year = row['year']\n",
    "                            if year not in year_data:\n",
    "                                year_data[year] = []\n",
    "                            year_data[year].append(processed_df)\n",
    "                    else:\n",
    "                        batch_failed += 1\n",
    "                \n",
    "                # Save batch data\n",
    "                if any(year_data.values()):\n",
    "                    records_saved = self.save_batch_data_by_year(year_data, batch_idx + 1)\n",
    "                    total_records_saved += records_saved\n",
    "                \n",
    "                successful_requests += batch_successful\n",
    "                failed_requests += batch_failed\n",
    "                \n",
    "                logger.info(f\"Batch {batch_idx + 1} complete: {batch_successful} successful, {batch_failed} failed\")\n",
    "                \n",
    "                # Save log periodically\n",
    "                if (batch_idx + 1) % 5 == 0:\n",
    "                    self.save_scrape_log()\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing batch {batch_idx + 1}: {e}\")\n",
    "                failed_requests += len(batch_data)\n",
    "        \n",
    "        # Save final log\n",
    "        self.save_scrape_log()\n",
    "        \n",
    "        # Print summary\n",
    "        logger.info(\"=== SCRAPING COMPLETE ===\")\n",
    "        logger.info(f\"Attempted to scrape: {len(to_scrape_df)}\")\n",
    "        logger.info(f\"Successful requests: {successful_requests}\")\n",
    "        logger.info(f\"Failed requests: {failed_requests}\")\n",
    "        logger.info(f\"New video records saved: {total_records_saved}\")\n",
    "        if len(to_scrape_df) > 0:\n",
    "            logger.info(f\"Success rate: {(successful_requests/len(to_scrape_df))*100:.1f}%\")\n",
    "\n",
    "    def migrate_csv_data_to_log(self, csv_combinations):\n",
    "        \"\"\"Migrate existing CSV data to the scrape log\"\"\"\n",
    "        migrated_count = 0\n",
    "        for player_id, game_id in csv_combinations:\n",
    "            key = f\"{player_id}_{game_id}\"\n",
    "            if key not in self.log_data:\n",
    "                self.log_data[key] = {\n",
    "                    \"player_id\": str(player_id),\n",
    "                    \"game_id\": str(game_id),\n",
    "                    \"team_id\": \"unknown\",\n",
    "                    \"player_name\": \"unknown\",\n",
    "                    \"year\": \"unknown\",\n",
    "                    \"timestamp\": \"migrated_from_csv\",\n",
    "                    \"success\": True,\n",
    "                    \"record_count\": 1,\n",
    "                    \"error_msg\": None\n",
    "                }\n",
    "                migrated_count += 1\n",
    "        \n",
    "        if migrated_count > 0:\n",
    "            logger.info(f\"Migrated {migrated_count} successful scrapes from CSV files to log\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to run the optimized scraper\"\"\"\n",
    "    scraper = NBAScraper(\n",
    "        max_concurrent_requests=15,  # Adjust based on your system and API limits\n",
    "        request_delay=0.05,  # Much smaller delay between requests\n",
    "        batch_size=100,  # Larger batch size for efficiency\n",
    "        output_dir=\"scraped_data\"\n",
    "    )\n",
    "    \n",
    "    scraper.scrape_nba_video_details(\n",
    "        master_record_path='master_record.csv',\n",
    "        context_measure=\"DEF_FGA\",\n",
    "        force_retry_failed=False\n",
    "    )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
