{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aaa460-cc8e-4918-b94d-24127a0e3c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-20 15:49:43,101 - INFO - Loaded master record with 188540 rows\n",
      "2025-06-20 15:49:43,685 - INFO - Successfully loaded JSON from scrape_log.json with 187842 entries\n",
      "2025-06-20 15:49:47,477 - INFO - Found 173579 existing combinations in CSV files\n",
      "2025-06-20 15:49:48,208 - INFO - Successfully loaded JSON from scrape_log.json with 187842 entries\n",
      "2025-06-20 15:49:48,228 - INFO - \n",
      "=== SCRAPE LOG ANALYSIS ===\n",
      "2025-06-20 15:49:48,229 - INFO - Total attempts logged: 187842\n",
      "2025-06-20 15:49:48,230 - INFO - Successful attempts: 187797\n",
      "2025-06-20 15:49:48,231 - INFO - Failed attempts: 45\n",
      "2025-06-20 15:49:48,232 - INFO - Success rate: 100.0%\n",
      "2025-06-20 15:49:48,233 - INFO - Total video records recorded in log: 445273\n",
      "2025-06-20 15:49:48,299 - INFO - \n",
      "Breakdown by year:\n",
      "2025-06-20 15:49:48,301 - INFO -   2019: 2272 attempts, 2267 successful (99.8%), 220 records\n",
      "2025-06-20 15:49:48,302 - INFO -   2020: 10048 attempts, 10048 successful (100.0%), 78541 records\n",
      "2025-06-20 15:49:48,303 - INFO -   2021: 24915 attempts, 24913 successful (100.0%), 219375 records\n",
      "2025-06-20 15:49:48,305 - INFO -   2022: 2568 attempts, 2568 successful (100.0%), 1134 records\n",
      "2025-06-20 15:49:48,305 - INFO -   2023: 2384 attempts, 2384 successful (100.0%), 1204 records\n",
      "2025-06-20 15:49:48,306 - INFO -   2024: 2493 attempts, 2493 successful (100.0%), 1497 records\n",
      "2025-06-20 15:49:48,307 - INFO -   2025: 143162 attempts, 143124 successful (100.0%), 143302 records\n",
      "2025-06-20 15:49:48,329 - INFO - === STARTING IMPROVED NBA VIDEO SCRAPER ===\n",
      "2025-06-20 15:49:48,611 - INFO - === SCRAPE STATUS ANALYSIS ===\n",
      "2025-06-20 15:49:48,612 - INFO - Total combinations: 188540\n",
      "2025-06-20 15:49:48,614 - INFO - successful_with_data: 173889 combinations\n",
      "2025-06-20 15:49:48,614 - INFO - successful_no_data: 13908 combinations\n",
      "2025-06-20 15:49:48,615 - INFO - never_attempted: 698 combinations\n",
      "2025-06-20 15:49:48,616 - INFO - failed: 45 combinations\n",
      "2025-06-20 15:49:48,634 - INFO - \n",
      "successful_with_data by year:\n",
      "2025-06-20 15:49:48,637 - INFO -   2019: 25612 combinations\n",
      "2025-06-20 15:49:48,637 - INFO -   2020: 22287 combinations\n",
      "2025-06-20 15:49:48,638 - INFO -   2021: 22921 combinations\n",
      "2025-06-20 15:49:48,639 - INFO -   2022: 25490 combinations\n",
      "2025-06-20 15:49:48,640 - INFO -   2023: 25358 combinations\n",
      "2025-06-20 15:49:48,641 - INFO -   2024: 25758 combinations\n",
      "2025-06-20 15:49:48,642 - INFO -   2025: 26463 combinations\n",
      "2025-06-20 15:49:48,657 - INFO - \n",
      "successful_no_data by year:\n",
      "2025-06-20 15:49:48,659 - INFO -   2019: 2245 combinations\n",
      "2025-06-20 15:49:48,660 - INFO -   2020: 1800 combinations\n",
      "2025-06-20 15:49:48,662 - INFO -   2021: 1992 combinations\n",
      "2025-06-20 15:49:48,663 - INFO -   2022: 2440 combinations\n",
      "2025-06-20 15:49:48,664 - INFO -   2023: 2264 combinations\n",
      "2025-06-20 15:49:48,665 - INFO -   2024: 2328 combinations\n",
      "2025-06-20 15:49:48,666 - INFO -   2025: 839 combinations\n",
      "2025-06-20 15:49:48,679 - INFO - \n",
      "never_attempted by year:\n",
      "2025-06-20 15:49:48,680 - INFO -   2025: 698 combinations\n",
      "2025-06-20 15:49:48,693 - INFO - \n",
      "failed by year:\n",
      "2025-06-20 15:49:48,695 - INFO -   2019: 5 combinations\n",
      "2025-06-20 15:49:48,695 - INFO -   2021: 2 combinations\n",
      "2025-06-20 15:49:48,696 - INFO -   2025: 38 combinations\n",
      "2025-06-20 15:49:48,716 - INFO - Force retry failed enabled: Including 45 failed attempts\n",
      "2025-06-20 15:49:48,718 - INFO - Will attempt 743 combinations\n",
      "2025-06-20 15:49:48,721 - INFO - Starting with batch number: 3785\n",
      "2025-06-20 15:49:49,770 - WARNING - Request failed with status 500 for Player 1629630, Game 22400962\n",
      "2025-06-20 15:49:49,920 - WARNING - Request failed with status 500 for Player 1629630, Game 22400962\n",
      "2025-06-20 15:49:50,069 - WARNING - Request failed with status 500 for Player 1629630, Game 22400962\n",
      "2025-06-20 15:49:51,618 - WARNING - Request failed with status 500 for Player 203081, Game 22400972\n",
      "2025-06-20 15:49:51,845 - WARNING - Request failed with status 500 for Player 203081, Game 22400972\n",
      "2025-06-20 15:49:52,201 - WARNING - Request failed with status 500 for Player 203081, Game 22400972\n",
      "2025-06-20 15:49:54,810 - WARNING - Request failed with status 500 for Player 203507, Game 22400972\n",
      "2025-06-20 15:49:54,979 - WARNING - Request failed with status 500 for Player 203507, Game 22400972\n",
      "2025-06-20 15:49:55,139 - WARNING - Request failed with status 500 for Player 203507, Game 22400972\n",
      "2025-06-20 15:49:56,136 - WARNING - Request failed with status 500 for Player 1629008, Game 22400975\n",
      "2025-06-20 15:49:56,308 - WARNING - Request failed with status 500 for Player 1629008, Game 22400975\n",
      "2025-06-20 15:49:56,524 - WARNING - Request failed with status 500 for Player 1629008, Game 22400975\n",
      "2025-06-20 15:49:57,363 - WARNING - Request failed with status 500 for Player 1630578, Game 22400970\n",
      "2025-06-20 15:49:57,526 - WARNING - Request failed with status 500 for Player 1630578, Game 22400970\n",
      "2025-06-20 15:49:57,733 - WARNING - Request failed with status 500 for Player 1630578, Game 22400970\n",
      "2025-06-20 15:50:00,351 - WARNING - Request failed with status 500 for Player 202699, Game 22400969\n",
      "2025-06-20 15:50:00,522 - WARNING - Request failed with status 500 for Player 202699, Game 22400969\n",
      "2025-06-20 15:50:00,695 - WARNING - Request failed with status 500 for Player 202699, Game 22400969\n",
      "2025-06-20 15:50:01,553 - WARNING - Request failed with status 500 for Player 203952, Game 22400971\n",
      "2025-06-20 15:50:01,839 - WARNING - Request failed with status 500 for Player 203952, Game 22400971\n",
      "2025-06-20 15:50:02,041 - WARNING - Request failed with status 500 for Player 203952, Game 22400971\n",
      "2025-06-20 15:50:04,580 - WARNING - Request failed with status 500 for Player 1642377, Game 22400971\n",
      "2025-06-20 15:50:04,725 - WARNING - Request failed with status 500 for Player 1642377, Game 22400971\n",
      "2025-06-20 15:50:04,975 - WARNING - Request failed with status 500 for Player 1642377, Game 22400971\n",
      "2025-06-20 15:50:05,712 - WARNING - Request failed with status 500 for Player 203468, Game 22400973\n",
      "2025-06-20 15:50:05,959 - WARNING - Request failed with status 500 for Player 203468, Game 22400973\n",
      "2025-06-20 15:50:06,172 - WARNING - Request failed with status 500 for Player 203468, Game 22400973\n",
      "2025-06-20 15:50:06,906 - WARNING - Request failed with status 500 for Player 1629661, Game 22400968\n",
      "2025-06-20 15:50:07,087 - WARNING - Request failed with status 500 for Player 1629661, Game 22400968\n",
      "2025-06-20 15:50:07,235 - WARNING - Request failed with status 500 for Player 1629661, Game 22400968\n",
      "2025-06-20 15:50:08,044 - WARNING - Request failed with status 500 for Player 203110, Game 22400974\n",
      "2025-06-20 15:50:08,220 - WARNING - Request failed with status 500 for Player 203110, Game 22400974\n",
      "2025-06-20 15:50:08,366 - WARNING - Request failed with status 500 for Player 203110, Game 22400974\n",
      "2025-06-20 15:50:08,994 - WARNING - Request failed with status 500 for Player 1642270, Game 22400980\n",
      "2025-06-20 15:50:09,166 - WARNING - Request failed with status 500 for Player 1642270, Game 22400980\n",
      "2025-06-20 15:50:09,339 - WARNING - Request failed with status 500 for Player 1642270, Game 22400980\n",
      "2025-06-20 15:50:10,166 - WARNING - Request failed with status 500 for Player 1627751, Game 22400980\n",
      "2025-06-20 15:50:10,313 - WARNING - Request failed with status 500 for Player 1627751, Game 22400980\n",
      "2025-06-20 15:50:10,456 - WARNING - Request failed with status 500 for Player 1627751, Game 22400980\n",
      "2025-06-20 15:50:11,097 - WARNING - Request failed with status 500 for Player 1630534, Game 22400980\n",
      "2025-06-20 15:50:11,246 - WARNING - Request failed with status 500 for Player 1630534, Game 22400980\n",
      "2025-06-20 15:50:11,393 - WARNING - Request failed with status 500 for Player 1630534, Game 22400980\n",
      "2025-06-20 15:50:12,795 - WARNING - Request failed with status 500 for Player 1631096, Game 22400982\n",
      "2025-06-20 15:50:12,941 - WARNING - Request failed with status 500 for Player 1631096, Game 22400982\n",
      "2025-06-20 15:50:13,094 - WARNING - Request failed with status 500 for Player 1631096, Game 22400982\n",
      "2025-06-20 15:50:16,065 - WARNING - Request failed with status 500 for Player 1629610, Game 22400538\n",
      "2025-06-20 15:50:16,214 - WARNING - Request failed with status 500 for Player 1629610, Game 22400538\n",
      "2025-06-20 15:50:16,364 - WARNING - Request failed with status 500 for Player 1629610, Game 22400538\n",
      "2025-06-20 15:50:17,098 - WARNING - Request failed with status 500 for Player 1630163, Game 22400538\n",
      "2025-06-20 15:50:17,247 - WARNING - Request failed with status 500 for Player 1630163, Game 22400538\n",
      "2025-06-20 15:50:17,398 - WARNING - Request failed with status 500 for Player 1630163, Game 22400538\n",
      "2025-06-20 15:50:18,111 - WARNING - Request failed with status 500 for Player 1631109, Game 22400538\n",
      "2025-06-20 15:50:18,279 - WARNING - Request failed with status 500 for Player 1631109, Game 22400538\n",
      "2025-06-20 15:50:18,427 - WARNING - Request failed with status 500 for Player 1631109, Game 22400538\n",
      "2025-06-20 15:50:19,972 - WARNING - Request failed with status 500 for Player 1631094, Game 22400979\n",
      "2025-06-20 15:50:20,130 - WARNING - Request failed with status 500 for Player 1631094, Game 22400979\n",
      "2025-06-20 15:50:20,289 - WARNING - Request failed with status 500 for Player 1631094, Game 22400979\n",
      "2025-06-20 15:50:20,982 - WARNING - Request failed with status 500 for Player 1629012, Game 22400981\n",
      "2025-06-20 15:50:21,200 - WARNING - Request failed with status 500 for Player 1629012, Game 22400981\n",
      "2025-06-20 15:50:21,362 - WARNING - Request failed with status 500 for Player 1629012, Game 22400981\n",
      "2025-06-20 15:50:22,000 - WARNING - Request failed with status 500 for Player 201142, Game 22400977\n",
      "2025-06-20 15:50:22,144 - WARNING - Request failed with status 500 for Player 201142, Game 22400977\n",
      "2025-06-20 15:50:22,292 - WARNING - Request failed with status 500 for Player 201142, Game 22400977\n",
      "2025-06-20 15:50:22,948 - WARNING - Request failed with status 500 for Player 1630208, Game 22400977\n",
      "2025-06-20 15:50:23,099 - WARNING - Request failed with status 500 for Player 1630208, Game 22400977\n",
      "2025-06-20 15:50:23,290 - WARNING - Request failed with status 500 for Player 1630208, Game 22400977\n",
      "2025-06-20 15:50:24,084 - WARNING - Request failed with status 500 for Player 1642346, Game 22400977\n",
      "2025-06-20 15:50:24,250 - WARNING - Request failed with status 500 for Player 1642346, Game 22400977\n",
      "2025-06-20 15:50:24,395 - WARNING - Request failed with status 500 for Player 1642346, Game 22400977\n",
      "2025-06-20 15:50:25,015 - WARNING - Request failed with status 500 for Player 1630700, Game 22400978\n",
      "2025-06-20 15:50:25,170 - WARNING - Request failed with status 500 for Player 1630700, Game 22400978\n",
      "2025-06-20 15:50:25,319 - WARNING - Request failed with status 500 for Player 1630700, Game 22400978\n",
      "2025-06-20 15:50:29,628 - WARNING - Request failed with status 500 for Player 1629651, Game 22400978\n",
      "2025-06-20 15:50:29,791 - WARNING - Request failed with status 500 for Player 1629651, Game 22400978\n",
      "2025-06-20 15:50:30,040 - WARNING - Request failed with status 500 for Player 1629651, Game 22400978\n",
      "2025-06-20 15:50:30,754 - WARNING - Request failed with status 500 for Player 1630658, Game 22400991\n",
      "2025-06-20 15:50:30,953 - WARNING - Request failed with status 500 for Player 1630658, Game 22400991\n",
      "2025-06-20 15:50:31,137 - WARNING - Request failed with status 500 for Player 1630658, Game 22400991\n",
      "2025-06-20 15:50:31,881 - WARNING - Request failed with status 500 for Player 1631221, Game 22400991\n",
      "2025-06-20 15:50:32,085 - WARNING - Request failed with status 500 for Player 1631221, Game 22400991\n",
      "2025-06-20 15:50:32,335 - WARNING - Request failed with status 500 for Player 1631221, Game 22400991\n",
      "2025-06-20 15:50:33,027 - WARNING - Request failed with status 500 for Player 1642367, Game 22400991\n",
      "2025-06-20 15:50:33,221 - WARNING - Request failed with status 500 for Player 1642367, Game 22400991\n",
      "2025-06-20 15:50:33,373 - WARNING - Request failed with status 500 for Player 1642367, Game 22400991\n",
      "2025-06-20 15:50:34,135 - WARNING - Request failed with status 500 for Player 1630567, Game 22400991\n",
      "2025-06-20 15:50:34,310 - WARNING - Request failed with status 500 for Player 1630567, Game 22400991\n",
      "2025-06-20 15:50:34,748 - WARNING - Request failed with status 500 for Player 1630567, Game 22400991\n",
      "2025-06-20 15:50:35,550 - WARNING - Request failed with status 500 for Player 1642259, Game 22400995\n",
      "2025-06-20 15:50:35,705 - WARNING - Request failed with status 500 for Player 1642259, Game 22400995\n",
      "2025-06-20 15:50:35,861 - WARNING - Request failed with status 500 for Player 1642259, Game 22400995\n",
      "2025-06-20 15:50:38,271 - WARNING - Request failed with status 500 for Player 1631212, Game 22400990\n",
      "2025-06-20 15:50:38,411 - WARNING - Request failed with status 500 for Player 1631212, Game 22400990\n",
      "2025-06-20 15:50:39,573 - WARNING - Request failed with status 500 for Player 1631212, Game 22400990\n",
      "2025-06-20 15:50:40,297 - WARNING - Request failed with status 500 for Player 1630215, Game 22400985\n",
      "2025-06-20 15:50:40,436 - WARNING - Request failed with status 500 for Player 1630215, Game 22400985\n",
      "2025-06-20 15:50:40,617 - WARNING - Request failed with status 500 for Player 1630215, Game 22400985\n",
      "2025-06-20 15:50:44,259 - WARNING - Request failed with status 500 for Player 201144, Game 22400986\n",
      "2025-06-20 15:50:44,408 - WARNING - Request failed with status 500 for Player 201144, Game 22400986\n",
      "2025-06-20 15:50:44,580 - WARNING - Request failed with status 500 for Player 201144, Game 22400986\n",
      "2025-06-20 15:50:45,316 - WARNING - Request failed with status 500 for Player 1628384, Game 22400984\n",
      "2025-06-20 15:50:45,468 - WARNING - Request failed with status 500 for Player 1628384, Game 22400984\n",
      "2025-06-20 15:50:45,619 - WARNING - Request failed with status 500 for Player 1628384, Game 22400984\n",
      "2025-06-20 15:50:46,979 - WARNING - Request failed with status 500 for Player 203482, Game 22400987\n",
      "2025-06-20 15:50:47,135 - WARNING - Request failed with status 500 for Player 203482, Game 22400987\n",
      "2025-06-20 15:50:47,315 - WARNING - Request failed with status 500 for Player 203482, Game 22400987\n",
      "2025-06-20 15:50:49,614 - INFO - Processing 50/743: Player Dorian Finney-Smith (1627827) in Game 22400537 - 2025\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "from typing import Dict, Set, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SafeJSONManager:\n",
    "    \"\"\"\n",
    "    Manages JSON files with atomic writes and corruption recovery\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_write_json(data: dict, filepath: str, backup_count: int = 3) -> bool:\n",
    "        \"\"\"\n",
    "        Safely write JSON data with atomic operations and backups\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create backup of existing file if it exists\n",
    "            if os.path.exists(filepath) and backup_count > 0:\n",
    "                SafeJSONManager._rotate_backups(filepath, backup_count)\n",
    "            \n",
    "            # Write to temporary file first\n",
    "            temp_filepath = f\"{filepath}.tmp\"\n",
    "            with open(temp_filepath, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            \n",
    "            # Atomic move to final location\n",
    "            shutil.move(temp_filepath, filepath)\n",
    "            \n",
    "            # Verify the file can be read back\n",
    "            with open(filepath, 'r') as f:\n",
    "                json.load(f)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error safely writing JSON to {filepath}: {e}\")\n",
    "            # Clean up temp file if it exists\n",
    "            if os.path.exists(f\"{filepath}.tmp\"):\n",
    "                try:\n",
    "                    os.remove(f\"{filepath}.tmp\")\n",
    "                except:\n",
    "                    pass\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _rotate_backups(filepath: str, backup_count: int):\n",
    "        \"\"\"\n",
    "        Rotate backup files, keeping the most recent N backups\n",
    "        \"\"\"\n",
    "        # Rotate existing backups\n",
    "        for i in range(backup_count - 1, 0, -1):\n",
    "            old_backup = f\"{filepath}.backup{i}\"\n",
    "            new_backup = f\"{filepath}.backup{i + 1}\"\n",
    "            if os.path.exists(old_backup):\n",
    "                if os.path.exists(new_backup):\n",
    "                    os.remove(new_backup)\n",
    "                shutil.move(old_backup, new_backup)\n",
    "        \n",
    "        # Create new backup from current file\n",
    "        if os.path.exists(filepath):\n",
    "            backup_path = f\"{filepath}.backup1\"\n",
    "            if os.path.exists(backup_path):\n",
    "                os.remove(backup_path)\n",
    "            shutil.copy2(filepath, backup_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_load_json(filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Safely load JSON with automatic corruption recovery\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            logger.info(f\"No existing JSON file found at {filepath}\")\n",
    "            return {}\n",
    "        \n",
    "        # Try to load the main file\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            logger.info(f\"Successfully loaded JSON from {filepath} with {len(data)} entries\")\n",
    "            return data\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.warning(f\"JSON corruption detected in {filepath}: {e}\")\n",
    "            return SafeJSONManager._recover_from_corruption(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error loading {filepath}: {e}\")\n",
    "            return SafeJSONManager._recover_from_corruption(filepath)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _recover_from_corruption(filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Attempt to recover from JSON corruption using backups\n",
    "        \"\"\"\n",
    "        logger.info(\"Attempting JSON corruption recovery...\")\n",
    "        \n",
    "        # Try backup files\n",
    "        for i in range(1, 6):  # Try up to 5 backups\n",
    "            backup_path = f\"{filepath}.backup{i}\"\n",
    "            if os.path.exists(backup_path):\n",
    "                try:\n",
    "                    with open(backup_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    logger.info(f\"Successfully recovered from backup {backup_path} with {len(data)} entries\")\n",
    "                    \n",
    "                    # Restore the good backup as the main file\n",
    "                    shutil.copy2(backup_path, filepath)\n",
    "                    return data\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(f\"Backup {backup_path} is also corrupted, trying next...\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error reading backup {backup_path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # If all backups failed, try manual recovery\n",
    "        logger.warning(\"All backups failed, attempting manual recovery...\")\n",
    "        return SafeJSONManager._manual_recovery(filepath)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _manual_recovery(filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Last resort: try to manually recover what we can from corrupted JSON\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Try to find complete entries using regex\n",
    "            import re\n",
    "            pattern = r'\"(\\d+_\\d+)\"\\s*:\\s*(\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\})'\n",
    "            matches = re.finditer(pattern, content)\n",
    "            \n",
    "            recovered_data = {}\n",
    "            for match in matches:\n",
    "                key = match.group(1)\n",
    "                entry_json = match.group(2)\n",
    "                try:\n",
    "                    entry_data = json.loads(entry_json)\n",
    "                    recovered_data[key] = entry_data\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if recovered_data:\n",
    "                logger.info(f\"Manual recovery found {len(recovered_data)} entries\")\n",
    "                return recovered_data\n",
    "            else:\n",
    "                logger.warning(\"Manual recovery found no valid entries\")\n",
    "                return {}\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Manual recovery failed: {e}\")\n",
    "            return {}\n",
    "\n",
    "class ImprovedNBAScraper:\n",
    "    \"\"\"\n",
    "    Improved NBA video details scraper with enhanced error handling and corruption resistance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, master_record_path: str, output_dir: str = \"scraped_data\", \n",
    "                 log_file: str = \"scrape_log.json\"):\n",
    "        self.master_record_path = master_record_path\n",
    "        self.output_dir = output_dir\n",
    "        self.log_file = log_file\n",
    "        self.json_manager = SafeJSONManager()\n",
    "        \n",
    "        # Load master record\n",
    "        self.master_record = self._load_master_record()\n",
    "        \n",
    "        # Load scrape log\n",
    "        self.log_data = self.json_manager.safe_load_json(self.log_file)\n",
    "        \n",
    "        # Migrate CSV data to log if needed\n",
    "        self._migrate_csv_data_to_log()\n",
    "    \n",
    "    def _load_master_record(self) -> pd.DataFrame:\n",
    "        \"\"\"Load and prepare master record\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.master_record_path)\n",
    "            df['TEAM_ID'] = df['TEAM_ID'].astype(int)\n",
    "            df['PLAYER_ID'] = df['PLAYER_ID'].astype(int)\n",
    "            df = df[df.year >= 2019]\n",
    "            logger.info(f\"Loaded master record with {len(df)} rows\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading master record: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _migrate_csv_data_to_log(self):\n",
    "        \"\"\"Migrate existing CSV data to log\"\"\"\n",
    "        csv_combinations = self._load_existing_csv_combinations()\n",
    "        if not csv_combinations:\n",
    "            return\n",
    "        \n",
    "        migrated_count = 0\n",
    "        for player_id, game_id in csv_combinations:\n",
    "            key = f\"{player_id}_{game_id}\"\n",
    "            if key not in self.log_data:\n",
    "                self.log_data[key] = {\n",
    "                    \"player_id\": str(player_id),\n",
    "                    \"game_id\": str(game_id),\n",
    "                    \"team_id\": \"unknown\",\n",
    "                    \"player_name\": \"unknown\",\n",
    "                    \"year\": 2025,\n",
    "                    \"timestamp\": \"migrated_from_csv\",\n",
    "                    \"success\": True,\n",
    "                    \"record_count\": 1,\n",
    "                    \"has_data\": True,\n",
    "                    \"error_msg\": None\n",
    "                }\n",
    "                migrated_count += 1\n",
    "        \n",
    "        if migrated_count > 0:\n",
    "            logger.info(f\"Migrated {migrated_count} successful scrapes from CSV files to log\")\n",
    "            self._save_log()\n",
    "    \n",
    "    def _load_existing_csv_combinations(self) -> Set[Tuple[str, str]]:\n",
    "        \"\"\"Load all existing player-game combinations from CSV files\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            return set()\n",
    "        \n",
    "        combinations = set()\n",
    "        year_dirs = [d for d in os.listdir(self.output_dir) \n",
    "                    if d.startswith('year_') and os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "        \n",
    "        for year_dir in year_dirs:\n",
    "            year_path = os.path.join(self.output_dir, year_dir)\n",
    "            year = year_dir.replace('year_', '')\n",
    "            \n",
    "            # Check combined file first\n",
    "            combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "            if os.path.exists(combined_file):\n",
    "                try:\n",
    "                    df = pd.read_csv(combined_file)\n",
    "                    if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                        year_combinations = set(zip(df['def_id'].astype(str), df['gi'].astype(str)))\n",
    "                        combinations.update(year_combinations)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error reading {combined_file}: {e}\")\n",
    "            else:\n",
    "                # Check batch files\n",
    "                batch_files = [f for f in os.listdir(year_path) \n",
    "                             if f.startswith('batch_') and f.endswith('.csv')]\n",
    "                for batch_file in batch_files:\n",
    "                    try:\n",
    "                        batch_path = os.path.join(year_path, batch_file)\n",
    "                        df = pd.read_csv(batch_path)\n",
    "                        if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                            batch_combinations = set(zip(df['def_id'].astype(str), df['gi'].astype(str)))\n",
    "                            combinations.update(batch_combinations)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error reading {batch_path}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Found {len(combinations)} existing combinations in CSV files\")\n",
    "        return combinations\n",
    "    \n",
    "    def _save_log(self) -> bool:\n",
    "        \"\"\"Safely save the scrape log\"\"\"\n",
    "        return self.json_manager.safe_write_json(self.log_data, self.log_file)\n",
    "    \n",
    "    def _update_log(self, player_id: str, game_id: str, team_id: str, player_name: str, \n",
    "                   year: int, success: bool, record_count: int = 0, has_data: bool = None, \n",
    "                   error_msg: str = None):\n",
    "        \"\"\"Update the scrape log with a new attempt\"\"\"\n",
    "        key = f\"{player_id}_{game_id}\"\n",
    "        self.log_data[key] = {\n",
    "            \"player_id\": str(player_id),\n",
    "            \"game_id\": str(game_id),\n",
    "            \"team_id\": str(team_id),\n",
    "            \"player_name\": player_name,\n",
    "            \"year\": year,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"success\": success,\n",
    "            \"record_count\": record_count,\n",
    "            \"has_data\": has_data if has_data is not None else (record_count > 0),\n",
    "            \"error_msg\": error_msg\n",
    "        }\n",
    "    \n",
    "    def _fetch_video_details(self, game_id: str, player_id: str, team_id: str, \n",
    "                           context_measure: str = \"DEF_FGA\") -> Optional[dict]:\n",
    "        \"\"\"Fetch video details from NBA API with improved error handling\"\"\"\n",
    "        base_url = \"https://stats.nba.com/stats/videodetailsasset\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Referer\": \"https://www.nba.com\",\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Origin\": \"https://www.nba.com\"\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            \"GameID\": f\"00{game_id}\",\n",
    "            \"GameEventID\": \"\",\n",
    "            \"PlayerID\": str(player_id),\n",
    "            \"TeamID\": str(team_id),\n",
    "            \"Season\": \"\",\n",
    "            \"SeasonType\": \"\",\n",
    "            \"AheadBehind\": \"\",\n",
    "            \"CFID\": \"\",\n",
    "            \"CFPARAMS\": \"\",\n",
    "            \"ClutchTime\": \"\",\n",
    "            \"Conference\": \"\",\n",
    "            \"ContextFilter\": \"\",\n",
    "            \"ContextMeasure\": context_measure,\n",
    "            \"DateFrom\": \"\",\n",
    "            \"DateTo\": \"\",\n",
    "            \"Division\": \"\",\n",
    "            \"EndPeriod\": 0,\n",
    "            \"EndRange\": 40800,\n",
    "            \"GROUP_ID\": \"\",\n",
    "            \"GameSegment\": \"\",\n",
    "            \"GroupID\": \"\",\n",
    "            \"GroupMode\": \"\",\n",
    "            \"GroupQuantity\": 5,\n",
    "            \"LastNGames\": 0,\n",
    "            \"Location\": \"\",\n",
    "            \"Month\": 0,\n",
    "            \"OnOff\": \"\",\n",
    "            \"OppPlayerID\": \"\",\n",
    "            \"OpponentTeamID\": 0,\n",
    "            \"Outcome\": \"\",\n",
    "            \"PORound\": 0,\n",
    "            \"Period\": 0,\n",
    "            \"PlayerID1\": \"\",\n",
    "            \"PlayerID2\": \"\",\n",
    "            \"PlayerID3\": \"\",\n",
    "            \"PlayerID4\": \"\",\n",
    "            \"PlayerID5\": \"\",\n",
    "            \"PlayerPosition\": \"\",\n",
    "            \"PointDiff\": \"\",\n",
    "            \"Position\": \"\",\n",
    "            \"RangeType\": 0,\n",
    "            \"RookieYear\": \"\",\n",
    "            \"SeasonSegment\": \"\",\n",
    "            \"ShotClockRange\": \"\",\n",
    "            \"StartPeriod\": 0,\n",
    "            \"StartRange\": 0,\n",
    "            \"StarterBench\": \"\",\n",
    "            \"VsConference\": \"\",\n",
    "            \"VsDivision\": \"\",\n",
    "            \"VsPlayerID1\": \"\",\n",
    "            \"VsPlayerID2\": \"\",\n",
    "            \"VsPlayerID3\": \"\",\n",
    "            \"VsPlayerID4\": \"\",\n",
    "            \"VsPlayerID5\": \"\",\n",
    "            \"VsTeamID\": \"\"\n",
    "        }\n",
    "        \n",
    "        max_retries = 3\n",
    "        base_delay = 1\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(base_url, headers=headers, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 429:  # Rate limited\n",
    "                    delay = base_delay * (2 ** attempt)\n",
    "                    logger.warning(f\"Rate limited for Player {player_id}, Game {game_id}. Waiting {delay}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.warning(f\"Request failed with status {response.status_code} for Player {player_id}, Game {game_id}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        return None\n",
    "                    \n",
    "            except requests.RequestException as e:\n",
    "                logger.warning(f\"Request error for Player {player_id}, Game {game_id} (attempt {attempt + 1}): {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return None\n",
    "                time.sleep(base_delay * (attempt + 1))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _process_video_data(self, video_json: dict, player_id: str, team_id: str, \n",
    "                          player_name: str, year: int) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Process video JSON response into DataFrame\"\"\"\n",
    "        try:\n",
    "            if not (video_json and 'resultSets' in video_json and 'playlist' in video_json['resultSets']):\n",
    "                return None\n",
    "            \n",
    "            playlist = video_json['resultSets']['playlist']\n",
    "            if not playlist:\n",
    "                return None\n",
    "            \n",
    "            df = pd.DataFrame(playlist)\n",
    "            if df.empty or not all(col in df.columns for col in ['gi', 'ei', 'dsc']):\n",
    "                return None\n",
    "            \n",
    "            df = df[['gi', 'ei', 'dsc']]\n",
    "            df['def_id'] = player_id\n",
    "            df['team_id'] = team_id\n",
    "            df['player_name'] = player_name\n",
    "            df['year'] = year\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing video data for Player {player_id}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _save_batch_data(self, year_data_dict: Dict[int, list], batch_num: int) -> int:\n",
    "        \"\"\"Save batch data organized by year with improved error handling\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        \n",
    "        total_records_saved = 0\n",
    "        \n",
    "        for year, data_list in year_data_dict.items():\n",
    "            if not data_list:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Create year directory\n",
    "                year_dir = os.path.join(self.output_dir, f\"year_{year}\")\n",
    "                os.makedirs(year_dir, exist_ok=True)\n",
    "                \n",
    "                # Combine data\n",
    "                combined_df = pd.concat(data_list, ignore_index=True)\n",
    "                \n",
    "                # Save with atomic write\n",
    "                filename = f\"batch_{batch_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                filepath = os.path.join(year_dir, filename)\n",
    "                temp_filepath = f\"{filepath}.tmp\"\n",
    "                \n",
    "                # Write to temp file first\n",
    "                combined_df.to_csv(temp_filepath, index=False)\n",
    "                \n",
    "                # Atomic move\n",
    "                shutil.move(temp_filepath, filepath)\n",
    "                \n",
    "                records_count = len(combined_df)\n",
    "                total_records_saved += records_count\n",
    "                logger.info(f\"Saved {year} batch {batch_num} with {records_count} records to {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving batch data for year {year}: {e}\")\n",
    "                # Clean up temp file if it exists\n",
    "                temp_path = os.path.join(year_dir, f\"batch_{batch_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv.tmp\")\n",
    "                if os.path.exists(temp_path):\n",
    "                    try:\n",
    "                        os.remove(temp_path)\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        return total_records_saved\n",
    "    \n",
    "    def analyze_scrape_status(self):\n",
    "        \"\"\"Analyze current scrape status\"\"\"\n",
    "        unique_combinations = self.master_record[['PLAYER_ID', 'GAME_ID', 'TEAM_ID', 'PLAYER_NAME', 'year']].drop_duplicates()\n",
    "        total_combinations = len(unique_combinations)\n",
    "        \n",
    "        unique_combinations['scrape_key'] = unique_combinations['PLAYER_ID'].astype(str) + \"_\" + unique_combinations['GAME_ID'].astype(str)\n",
    "        \n",
    "        def categorize_attempt(scrape_key):\n",
    "            if scrape_key not in self.log_data:\n",
    "                return 'never_attempted'\n",
    "            entry = self.log_data[scrape_key]\n",
    "            if not entry['success']:\n",
    "                return 'failed'\n",
    "            elif entry.get('has_data', entry['record_count'] > 0):\n",
    "                return 'successful_with_data'\n",
    "            else:\n",
    "                return 'successful_no_data'\n",
    "        \n",
    "        unique_combinations['log_status'] = unique_combinations['scrape_key'].apply(categorize_attempt)\n",
    "        \n",
    "        status_counts = unique_combinations['log_status'].value_counts()\n",
    "        \n",
    "        logger.info(\"=== SCRAPE STATUS ANALYSIS ===\")\n",
    "        logger.info(f\"Total combinations: {total_combinations}\")\n",
    "        for status, count in status_counts.items():\n",
    "            logger.info(f\"{status}: {count} combinations\")\n",
    "        \n",
    "        # Show breakdown by year\n",
    "        for status in status_counts.index:\n",
    "            status_data = unique_combinations[unique_combinations['log_status'] == status]\n",
    "            if len(status_data) > 0:\n",
    "                logger.info(f\"\\n{status} by year:\")\n",
    "                year_counts = status_data['year'].value_counts().sort_index()\n",
    "                for year, count in year_counts.items():\n",
    "                    logger.info(f\"  {year}: {count} combinations\")\n",
    "        \n",
    "        return unique_combinations\n",
    "    \n",
    "    def scrape(self, context_measure: str = \"DEF_FGA\", delay_between_requests: float = 2.0,\n",
    "              batch_size: int = 50, save_log_frequency: int = 10, \n",
    "              force_retry_failed: bool = False, retry_no_data: bool = False):\n",
    "        \"\"\"\n",
    "        Main scraping function with improved error handling and progress tracking\n",
    "        \"\"\"\n",
    "        logger.info(\"=== STARTING IMPROVED NBA VIDEO SCRAPER ===\")\n",
    "        \n",
    "        # Analyze current status\n",
    "        unique_combinations = self.analyze_scrape_status()\n",
    "        \n",
    "        # Determine what to scrape\n",
    "        to_scrape_parts = [unique_combinations[unique_combinations['log_status'] == 'never_attempted']]\n",
    "        \n",
    "        if force_retry_failed:\n",
    "            failed_combinations = unique_combinations[unique_combinations['log_status'] == 'failed']\n",
    "            to_scrape_parts.append(failed_combinations)\n",
    "            logger.info(f\"Force retry failed enabled: Including {len(failed_combinations)} failed attempts\")\n",
    "        \n",
    "        if retry_no_data:\n",
    "            no_data_combinations = unique_combinations[unique_combinations['log_status'] == 'successful_no_data']\n",
    "            to_scrape_parts.append(no_data_combinations)\n",
    "            logger.info(f\"Retry no data enabled: Including {len(no_data_combinations)} no-data attempts\")\n",
    "        \n",
    "        to_scrape_df = pd.concat(to_scrape_parts) if len(to_scrape_parts) > 1 else to_scrape_parts[0]\n",
    "        \n",
    "        if len(to_scrape_df) == 0:\n",
    "            logger.info(\"🎉 No combinations to scrape!\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Will attempt {len(to_scrape_df)} combinations\")\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        year_data = {}\n",
    "        successful_requests = 0\n",
    "        failed_requests = 0\n",
    "        no_data_requests = 0\n",
    "        batch_num = self._get_next_batch_number()\n",
    "        total_records_saved = 0\n",
    "        \n",
    "        logger.info(f\"Starting with batch number: {batch_num}\")\n",
    "        \n",
    "        # Process each combination\n",
    "        for idx, (_, row) in enumerate(to_scrape_df.iterrows()):\n",
    "            try:\n",
    "                player_id = str(row['PLAYER_ID'])\n",
    "                game_id = str(row['GAME_ID'])\n",
    "                team_id = str(row['TEAM_ID'])\n",
    "                player_name = row['PLAYER_NAME']\n",
    "                year = row['year']\n",
    "                \n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    logger.info(f\"Processing {idx + 1}/{len(to_scrape_df)}: Player {player_name} ({player_id}) in Game {game_id} - {year}\")\n",
    "                \n",
    "                # Fetch video details\n",
    "                video_json = self._fetch_video_details(game_id, player_id, team_id, context_measure)\n",
    "                \n",
    "                if video_json:\n",
    "                    processed_df = self._process_video_data(video_json, player_id, team_id, player_name, year)\n",
    "                    \n",
    "                    if processed_df is not None and not processed_df.empty:\n",
    "                        if year not in year_data:\n",
    "                            year_data[year] = []\n",
    "                        \n",
    "                        year_data[year].append(processed_df)\n",
    "                        successful_requests += 1\n",
    "                        record_count = len(processed_df)\n",
    "                        \n",
    "                        self._update_log(player_id, game_id, team_id, player_name, year, \n",
    "                                       True, record_count, has_data=True)\n",
    "                    else:\n",
    "                        no_data_requests += 1\n",
    "                        self._update_log(player_id, game_id, team_id, player_name, year, \n",
    "                                       True, 0, has_data=False)\n",
    "                else:\n",
    "                    failed_requests += 1\n",
    "                    self._update_log(player_id, game_id, team_id, player_name, year, \n",
    "                                   False, 0, error_msg=\"API request failed\")\n",
    "                \n",
    "                # Save log periodically\n",
    "                if (idx + 1) % save_log_frequency == 0:\n",
    "                    if not self._save_log():\n",
    "                        logger.error(f\"Failed to save log at iteration {idx + 1}\")\n",
    "                \n",
    "                # Save batch data when threshold is reached\n",
    "                if successful_requests > 0 and successful_requests % batch_size == 0:\n",
    "                    records_saved = self._save_batch_data(year_data, batch_num)\n",
    "                    total_records_saved += records_saved\n",
    "                    year_data = {}\n",
    "                    batch_num += 1\n",
    "                \n",
    "                # Rate limiting\n",
    "                if idx < len(to_scrape_df) - 1:\n",
    "                    time.sleep(delay_between_requests)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"Scraping interrupted by user. Saving progress...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error processing row {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save any remaining data\n",
    "        if any(year_data.values()):\n",
    "            records_saved = self._save_batch_data(year_data, batch_num)\n",
    "            total_records_saved += records_saved\n",
    "        \n",
    "        # Final log save\n",
    "        if not self._save_log():\n",
    "            logger.error(\"Failed to save final log\")\n",
    "        \n",
    "        # Print summary\n",
    "        logger.info(\"=== SCRAPING COMPLETE ===\")\n",
    "        logger.info(f\"Attempted to scrape: {len(to_scrape_df)}\")\n",
    "        logger.info(f\"Successful requests with data: {successful_requests}\")\n",
    "        logger.info(f\"Successful requests with no data: {no_data_requests}\")\n",
    "        logger.info(f\"Failed requests: {failed_requests}\")\n",
    "        logger.info(f\"New video records saved: {total_records_saved}\")\n",
    "        \n",
    "        if len(to_scrape_df) > 0:\n",
    "            success_rate = ((successful_requests + no_data_requests) / len(to_scrape_df)) * 100\n",
    "            logger.info(f\"Overall success rate: {success_rate:.1f}%\")\n",
    "            if successful_requests > 0:\n",
    "                data_rate = (successful_requests / len(to_scrape_df)) * 100\n",
    "                logger.info(f\"Data found rate: {data_rate:.1f}%\")\n",
    "    \n",
    "    def _get_next_batch_number(self) -> int:\n",
    "        \"\"\"Get the next batch number by examining existing files\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            return 1\n",
    "        \n",
    "        existing_batches = []\n",
    "        for year_dir in os.listdir(self.output_dir):\n",
    "            if year_dir.startswith('year_'):\n",
    "                year_path = os.path.join(self.output_dir, year_dir)\n",
    "                if os.path.isdir(year_path):\n",
    "                    batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_')]\n",
    "                    for batch_file in batch_files:\n",
    "                        try:\n",
    "                            batch_num_str = batch_file.split('_')[1]\n",
    "                            existing_batches.append(int(batch_num_str))\n",
    "                        except (ValueError, IndexError):\n",
    "                            pass\n",
    "        \n",
    "        return max(existing_batches) + 1 if existing_batches else 1\n",
    "    \n",
    "    def combine_batches_by_year(self):\n",
    "        \"\"\"Combine all batch files within each year into a single CSV per year\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            logger.info(f\"Output directory {self.output_dir} does not exist\")\n",
    "            return\n",
    "        \n",
    "        year_dirs = [d for d in os.listdir(self.output_dir) \n",
    "                    if d.startswith('year_') and os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "        \n",
    "        if not year_dirs:\n",
    "            logger.info(\"No year directories found\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Found {len(year_dirs)} year directories\")\n",
    "        \n",
    "        for year_dir in sorted(year_dirs):\n",
    "            year_path = os.path.join(self.output_dir, year_dir)\n",
    "            year = year_dir.replace('year_', '')\n",
    "            \n",
    "            # Check if combined file already exists\n",
    "            combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "            if os.path.exists(combined_file):\n",
    "                logger.info(f\"Combined file already exists for {year}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Find batch files\n",
    "            batch_files = [f for f in os.listdir(year_path) \n",
    "                         if f.startswith('batch_') and f.endswith('.csv')]\n",
    "            \n",
    "            if not batch_files:\n",
    "                logger.info(f\"No batch files found for {year}\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Processing {year}: Found {len(batch_files)} batch files\")\n",
    "            \n",
    "            try:\n",
    "                # Load and combine all batches\n",
    "                year_batches = []\n",
    "                for file in batch_files:\n",
    "                    file_path = os.path.join(year_path, file)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    year_batches.append(df)\n",
    "                \n",
    "                if year_batches:\n",
    "                    final_df = pd.concat(year_batches, ignore_index=True)\n",
    "                    \n",
    "                    # Remove duplicates\n",
    "                    initial_count = len(final_df)\n",
    "                    final_df = final_df.drop_duplicates()\n",
    "                    final_count = len(final_df)\n",
    "                    \n",
    "                    if initial_count != final_count:\n",
    "                        logger.info(f\"Removed {initial_count - final_count} duplicate records for {year}\")\n",
    "                    \n",
    "                    # Save with atomic write\n",
    "                    temp_path = f\"{combined_file}.tmp\"\n",
    "                    final_df.to_csv(temp_path, index=False)\n",
    "                    shutil.move(temp_path, combined_file)\n",
    "                    \n",
    "                    logger.info(f\"✓ Combined file saved: {combined_file} with {final_count} total records\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error combining batches for {year}: {e}\")\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Print a summary of collected data by year.\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            logger.info(f\"Output directory {self.output_dir} does not exist.\")\n",
    "            return\n",
    "\n",
    "        year_dirs = [d for d in os.listdir(self.output_dir)\n",
    "                     if d.startswith('year_') and os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "\n",
    "        if not year_dirs:\n",
    "            logger.info(\"No year directories found.\")\n",
    "            return\n",
    "\n",
    "        logger.info(\"\\n=== DATA SUMMARY BY YEAR ===\")\n",
    "        total_records = 0\n",
    "\n",
    "        for year_dir in sorted(year_dirs):\n",
    "            year = year_dir.replace('year_', '')\n",
    "            year_path = os.path.join(self.output_dir, year_dir)\n",
    "\n",
    "            # Look for combined file first, otherwise count batch files\n",
    "            combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "\n",
    "            if os.path.exists(combined_file):\n",
    "                try:\n",
    "                    df = pd.read_csv(combined_file)\n",
    "                    record_count = len(df)\n",
    "                    logger.info(f\"{year}: {record_count:,} records (combined)\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error reading combined file {combined_file}: {e}\")\n",
    "                    record_count = 0 # reset record count if there was an error\n",
    "            else:\n",
    "                # Count records in batch files\n",
    "                batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "                record_count = 0\n",
    "                num_batch_files = 0\n",
    "                for file in batch_files:\n",
    "                    file_path = os.path.join(year_path, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        record_count += len(df)\n",
    "                        num_batch_files += 1\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error reading batch file {file_path}: {e}\")\n",
    "                logger.info(f\"{year}: {record_count:,} records ({num_batch_files} batch files)\")\n",
    "            \n",
    "            total_records += record_count\n",
    "\n",
    "        logger.info(f\"\\nTotal records across all years: {total_records:,}\")\n",
    "\n",
    "    def analyze_log_summary(self):\n",
    "        \"\"\"\n",
    "        Analyze the scrape log and provide statistics, replacing the old analyze_scrape_log function.\n",
    "        \"\"\"\n",
    "        log_data = self.json_manager.safe_load_json(self.log_file) # Use the class's safe loader\n",
    "        \n",
    "        if not log_data:\n",
    "            logger.info(\"No scrape log data found.\")\n",
    "            return\n",
    "        \n",
    "        successful = sum(1 for entry in log_data.values() if entry['success'])\n",
    "        failed = len(log_data) - successful\n",
    "        total_records_found = sum(entry.get('record_count', 0) for entry in log_data.values() if entry['success']) # Renamed to avoid confusion with `total_records` in `get_summary`\n",
    "        \n",
    "        logger.info(\"\\n=== SCRAPE LOG ANALYSIS ===\")\n",
    "        logger.info(f\"Total attempts logged: {len(log_data)}\")\n",
    "        logger.info(f\"Successful attempts: {successful}\")\n",
    "        logger.info(f\"Failed attempts: {failed}\")\n",
    "        \n",
    "        if len(log_data) > 0: # Avoid division by zero\n",
    "            success_rate = (successful / len(log_data)) * 100\n",
    "            logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "        else:\n",
    "            logger.info(\"Success rate: N/A (no attempts logged)\")\n",
    "            \n",
    "        logger.info(f\"Total video records recorded in log: {total_records_found}\") # Clarified output\n",
    "        \n",
    "        # Analyze by year if available\n",
    "        year_stats = {}\n",
    "        for entry in log_data.values():\n",
    "            year = entry.get('year', 'unknown')\n",
    "            if year not in year_stats:\n",
    "                year_stats[year] = {'attempts': 0, 'success': 0, 'records': 0}\n",
    "            year_stats[year]['attempts'] += 1\n",
    "            if entry['success']:\n",
    "                year_stats[year]['success'] += 1\n",
    "                year_stats[year]['records'] += entry.get('record_count', 0)\n",
    "        \n",
    "        logger.info(\"\\nBreakdown by year:\")\n",
    "        \n",
    "        # Separate numeric years from non-numeric ones for proper sorting\n",
    "        numeric_years = []\n",
    "        non_numeric_years = []\n",
    "        \n",
    "        for year in year_stats.keys():\n",
    "            try:\n",
    "                numeric_years.append(int(year))\n",
    "            except (ValueError, TypeError):\n",
    "                non_numeric_years.append(str(year))\n",
    "        \n",
    "        sorted_years = sorted(numeric_years) + sorted(non_numeric_years)\n",
    "        \n",
    "        for year in sorted_years:\n",
    "            stats = year_stats[year]\n",
    "            success_rate = (stats['success'] / stats['attempts']) * 100 if stats['attempts'] > 0 else 0\n",
    "            logger.info(f\"  {year}: {stats['attempts']} attempts, {stats['success']} successful ({success_rate:.1f}%), {stats['records']} records\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ImprovedNBAScraper(\n",
    "        master_record_path='master_record.csv',\n",
    "        output_dir='scraped_data',\n",
    "        log_file='scrape_log.json'\n",
    "    )\n",
    "\n",
    "    # Analyze existing log\n",
    "    scraper.analyze_log_summary()\n",
    "\n",
    "    # Run the scraper (set force_retry_failed=True to retry previously failed attempts)\n",
    "    scraper.scrape(\n",
    "        context_measure=\"DEF_FGA\",\n",
    "        delay_between_requests=0.001,  # 2 seconds between requests\n",
    "        batch_size=50,  # Save every 50 successful requests\n",
    "        force_retry_failed=False,  # Set to True to retry failed attempts\n",
    "        retry_no_data=False # Set to True to retry successful attempts that found no data\n",
    "    )\n",
    "\n",
    "    # Combine all batch files by year\n",
    "    scraper.combine_batches_by_year()\n",
    "\n",
    "    # Print summary of collected data\n",
    "    scraper.get_summary()\n",
    "    \n",
    "    # Final log analysis\n",
    "    scraper.analyze_log_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac4e4e-8a31-4512-9beb-07da679b7b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe35d83-e185-40d4-9ae0-2865a92d58fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154409f-6ced-4e92-8e88-5ce0f5a2e986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f4a31-31cf-4579-a044-97a45a4c67e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd54a2b-dbef-4d1d-9980-c77fdd7296d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
