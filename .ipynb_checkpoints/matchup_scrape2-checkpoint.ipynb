{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9aaa460-cc8e-4918-b94d-24127a0e3c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-22 11:11:38,450 - INFO - Loaded master record with 216357 rows\n",
      "2025-06-22 11:11:39,452 - INFO - Successfully loaded JSON from scrape_log.json with 220646 entries\n",
      "2025-06-22 11:11:43,841 - INFO - Found 175502 existing combinations in CSV files\n",
      "2025-06-22 11:11:44,916 - INFO - Successfully loaded JSON from scrape_log.json with 220646 entries\n",
      "2025-06-22 11:11:44,953 - INFO - \n",
      "=== SCRAPE LOG ANALYSIS ===\n",
      "2025-06-22 11:11:44,954 - INFO - Total attempts logged: 220646\n",
      "2025-06-22 11:11:44,955 - INFO - Successful attempts: 220214\n",
      "2025-06-22 11:11:44,956 - INFO - Failed attempts: 432\n",
      "2025-06-22 11:11:44,957 - INFO - Success rate: 99.8%\n",
      "2025-06-22 11:11:44,957 - INFO - Total video records recorded in log: 664728\n",
      "2025-06-22 11:11:45,043 - INFO - \n",
      "Breakdown by year:\n",
      "2025-06-22 11:11:45,046 - INFO -   2015: 3802 attempts, 3795 successful (99.8%), 0 records\n",
      "2025-06-22 11:11:45,047 - INFO -   2016: 306 attempts, 306 successful (100.0%), 0 records\n",
      "2025-06-22 11:11:45,049 - INFO -   2017: 2348 attempts, 2348 successful (100.0%), 576 records\n",
      "2025-06-22 11:11:45,050 - INFO -   2018: 25650 attempts, 25650 successful (100.0%), 218663 records\n",
      "2025-06-22 11:11:45,052 - INFO -   2019: 2272 attempts, 2272 successful (100.0%), 220 records\n",
      "2025-06-22 11:11:45,053 - INFO -   2020: 10048 attempts, 10048 successful (100.0%), 78541 records\n",
      "2025-06-22 11:11:45,055 - INFO -   2021: 24915 attempts, 24915 successful (100.0%), 219403 records\n",
      "2025-06-22 11:11:45,057 - INFO -   2022: 2568 attempts, 2568 successful (100.0%), 1134 records\n",
      "2025-06-22 11:11:45,058 - INFO -   2023: 2384 attempts, 2384 successful (100.0%), 1204 records\n",
      "2025-06-22 11:11:45,059 - INFO -   2024: 2493 attempts, 2493 successful (100.0%), 1497 records\n",
      "2025-06-22 11:11:45,061 - INFO -   2025: 143860 attempts, 143435 successful (99.7%), 143490 records\n",
      "2025-06-22 11:11:45,096 - INFO - === STARTING IMPROVED NBA VIDEO SCRAPER ===\n",
      "2025-06-22 11:11:46,577 - INFO - === SCRAPE STATUS ANALYSIS ===\n",
      "2025-06-22 11:11:46,578 - INFO - Total combinations: 216357\n",
      "2025-06-22 11:11:46,579 - INFO - successful_with_data: 197802 combinations\n",
      "2025-06-22 11:11:46,580 - INFO - successful_no_data: 15963 combinations\n",
      "2025-06-22 11:11:46,581 - INFO - never_attempted: 2167 combinations\n",
      "2025-06-22 11:11:46,582 - INFO - failed: 425 combinations\n",
      "2025-06-22 11:11:46,607 - INFO - \n",
      "successful_with_data by year:\n",
      "2025-06-22 11:11:46,610 - INFO -   2018: 23896 combinations\n",
      "2025-06-22 11:11:46,611 - INFO -   2019: 25612 combinations\n",
      "2025-06-22 11:11:46,612 - INFO -   2020: 22287 combinations\n",
      "2025-06-22 11:11:46,613 - INFO -   2021: 22923 combinations\n",
      "2025-06-22 11:11:46,614 - INFO -   2022: 25490 combinations\n",
      "2025-06-22 11:11:46,615 - INFO -   2023: 25358 combinations\n",
      "2025-06-22 11:11:46,616 - INFO -   2024: 25758 combinations\n",
      "2025-06-22 11:11:46,617 - INFO -   2025: 26478 combinations\n",
      "2025-06-22 11:11:46,640 - INFO - \n",
      "successful_no_data by year:\n",
      "2025-06-22 11:11:46,642 - INFO -   2018: 1754 combinations\n",
      "2025-06-22 11:11:46,643 - INFO -   2019: 2250 combinations\n",
      "2025-06-22 11:11:46,644 - INFO -   2020: 1800 combinations\n",
      "2025-06-22 11:11:46,644 - INFO -   2021: 1992 combinations\n",
      "2025-06-22 11:11:46,645 - INFO -   2022: 2440 combinations\n",
      "2025-06-22 11:11:46,646 - INFO -   2023: 2264 combinations\n",
      "2025-06-22 11:11:46,646 - INFO -   2024: 2328 combinations\n",
      "2025-06-22 11:11:46,647 - INFO -   2025: 1135 combinations\n",
      "2025-06-22 11:11:46,665 - INFO - \n",
      "never_attempted by year:\n",
      "2025-06-22 11:11:46,667 - INFO -   2018: 2167 combinations\n",
      "2025-06-22 11:11:46,681 - INFO - \n",
      "failed by year:\n",
      "2025-06-22 11:11:46,683 - INFO -   2025: 425 combinations\n",
      "2025-06-22 11:11:46,699 - INFO - Will attempt 2167 combinations\n",
      "2025-06-22 11:11:46,706 - INFO - Starting with batch number: 4320\n",
      "2025-06-22 11:12:33,852 - INFO - Processing 50/2167: Player Khris Middleton (203114) in Game 21700316 - 2018\n",
      "2025-06-22 11:12:40,298 - INFO - Saved 2018 batch 4320 with 455 records to batch_4320_20250622_111240.csv\n",
      "2025-06-22 11:13:22,405 - INFO - Processing 100/2167: Player Nikola JokiÄ‡ (203999) in Game 21700072 - 2018\n",
      "2025-06-22 11:13:37,343 - INFO - Saved 2018 batch 4321 with 426 records to batch_4321_20250622_111337.csv\n",
      "2025-06-22 11:14:11,610 - INFO - Processing 150/2167: Player Rashad Vaughn (1626173) in Game 21700120 - 2018\n",
      "2025-06-22 11:14:26,439 - INFO - Saved 2018 batch 4322 with 405 records to batch_4322_20250622_111426.csv\n",
      "2025-06-22 11:14:59,690 - INFO - Processing 200/2167: Player Dejounte Murray (1627749) in Game 21700126 - 2018\n",
      "2025-06-22 11:15:16,771 - INFO - Saved 2018 batch 4323 with 465 records to batch_4323_20250622_111516.csv\n",
      "2025-06-22 11:15:49,781 - INFO - Processing 250/2167: Player Thabo Sefolosha (200757) in Game 21700366 - 2018\n",
      "2025-06-22 11:16:25,128 - INFO - Saved 2018 batch 4324 with 490 records to batch_4324_20250622_111625.csv\n",
      "2025-06-22 11:16:39,328 - INFO - Processing 300/2167: Player Jakob PÃ¶ltl (1627751) in Game 21700139 - 2018\n",
      "2025-06-22 11:17:15,817 - INFO - Saved 2018 batch 4325 with 432 records to batch_4325_20250622_111715.csv\n",
      "2025-06-22 11:17:30,269 - INFO - Processing 350/2167: Player Luc Mbah a Moute (201601) in Game 21701194 - 2018\n",
      "2025-06-22 11:18:05,346 - INFO - Saved 2018 batch 4326 with 466 records to batch_4326_20250622_111805.csv\n",
      "2025-06-22 11:18:17,782 - INFO - Processing 400/2167: Player Glenn Robinson III (203922) in Game 21701165 - 2018\n",
      "2025-06-22 11:18:55,874 - INFO - Saved 2018 batch 4327 with 511 records to batch_4327_20250622_111855.csv\n",
      "2025-06-22 11:19:09,101 - INFO - Processing 450/2167: Player Zach Randolph (2216) in Game 21700710 - 2018\n",
      "2025-06-22 11:19:47,395 - INFO - Saved 2018 batch 4328 with 458 records to batch_4328_20250622_111947.csv\n",
      "2025-06-22 11:19:56,382 - INFO - Processing 500/2167: Player Darrell Arthur (201589) in Game 21700782 - 2018\n",
      "2025-06-22 11:20:42,021 - INFO - Saved 2018 batch 4329 with 450 records to batch_4329_20250622_112042.csv\n",
      "2025-06-22 11:20:46,275 - INFO - Processing 550/2167: Player Isaiah Taylor (1627819) in Game 21700425 - 2018\n",
      "2025-06-22 11:21:33,895 - INFO - Saved 2018 batch 4330 with 481 records to batch_4330_20250622_112133.csv\n",
      "2025-06-22 11:21:33,899 - INFO - Processing 600/2167: Player Nik Stauskas (203917) in Game 21700314 - 2018\n",
      "2025-06-22 11:22:20,809 - INFO - Processing 650/2167: Player Dejounte Murray (1627749) in Game 21700348 - 2018\n",
      "2025-06-22 11:22:28,061 - INFO - Saved 2018 batch 4331 with 487 records to batch_4331_20250622_112228.csv\n",
      "2025-06-22 11:23:12,356 - INFO - Processing 700/2167: Player Lonzo Ball (1628366) in Game 21700332 - 2018\n",
      "2025-06-22 11:23:20,408 - INFO - Saved 2018 batch 4335 with 489 records to batch_4335_20250622_112320.csv\n",
      "2025-06-22 11:23:58,171 - INFO - Processing 750/2167: Player John Wall (202322) in Game 41700103 - 2018\n",
      "2025-06-22 11:24:05,760 - INFO - Saved 2018 batch 4336 with 425 records to batch_4336_20250622_112405.csv\n",
      "2025-06-22 11:24:36,747 - INFO - Processing 800/2167: Player Jahlil Okafor (1626143) in Game 21700027 - 2018\n",
      "2025-06-22 11:24:49,395 - INFO - Saved 2018 batch 4337 with 411 records to batch_4337_20250622_112449.csv\n",
      "2025-06-22 11:25:17,052 - INFO - Processing 850/2167: Player Jamal Crawford (2037) in Game 21701081 - 2018\n",
      "2025-06-22 11:25:28,676 - INFO - Saved 2018 batch 4338 with 497 records to batch_4338_20250622_112528.csv\n",
      "2025-06-22 11:25:55,491 - INFO - Processing 900/2167: Player Aaron Gordon (203932) in Game 21700174 - 2018\n",
      "2025-06-22 11:26:08,306 - INFO - Saved 2018 batch 4339 with 521 records to batch_4339_20250622_112608.csv\n",
      "2025-06-22 11:26:33,403 - INFO - Processing 950/2167: Player Kyle Collinsworth (1627858) in Game 21700651 - 2018\n",
      "2025-06-22 11:26:49,811 - INFO - Saved 2018 batch 4340 with 501 records to batch_4340_20250622_112649.csv\n",
      "2025-06-22 11:27:12,122 - INFO - Processing 1000/2167: Player Bogdan BogdanoviÄ‡ (203992) in Game 21700179 - 2018\n",
      "2025-06-22 11:27:33,455 - INFO - Saved 2018 batch 4341 with 446 records to batch_4341_20250622_112733.csv\n",
      "2025-06-22 11:27:51,070 - INFO - Processing 1050/2167: Player Kyrie Irving (202681) in Game 21700813 - 2018\n",
      "2025-06-22 11:28:17,582 - INFO - Saved 2018 batch 4342 with 489 records to batch_4342_20250622_112817.csv\n",
      "2025-06-22 11:28:33,165 - INFO - Processing 1100/2167: Player Emmanuel Mudiay (1626144) in Game 21700047 - 2018\n",
      "2025-06-22 11:29:02,498 - INFO - Saved 2018 batch 4343 with 450 records to batch_4343_20250622_112902.csv\n",
      "2025-06-22 11:29:13,028 - INFO - Processing 1150/2167: Player Demetrius Jackson (1627743) in Game 21700006 - 2018\n",
      "2025-06-22 11:29:49,469 - INFO - Saved 2018 batch 4344 with 517 records to batch_4344_20250622_112949.csv\n",
      "2025-06-22 11:29:52,348 - INFO - Processing 1200/2167: Player Josh Richardson (1626196) in Game 21700885 - 2018\n",
      "2025-06-22 11:30:30,493 - INFO - Saved 2018 batch 4345 with 428 records to batch_4345_20250622_113030.csv\n",
      "2025-06-22 11:30:31,692 - INFO - Processing 1250/2167: Player Paul Zipser (1627835) in Game 21700153 - 2018\n",
      "2025-06-22 11:31:10,857 - INFO - Processing 1300/2167: Player Danny Green (201980) in Game 21700633 - 2018\n",
      "2025-06-22 11:31:13,940 - INFO - Saved 2018 batch 4346 with 406 records to batch_4346_20250622_113113.csv\n",
      "2025-06-22 11:31:49,155 - INFO - Processing 1350/2167: Player Gorgui Dieng (203476) in Game 21700557 - 2018\n",
      "2025-06-22 11:31:53,453 - INFO - Saved 2018 batch 4347 with 405 records to batch_4347_20250622_113153.csv\n",
      "2025-06-22 11:32:26,391 - INFO - Processing 1400/2167: Player Nick Young (201156) in Game 21701175 - 2018\n",
      "2025-06-22 11:32:31,151 - INFO - Saved 2018 batch 4348 with 467 records to batch_4348_20250622_113231.csv\n",
      "2025-06-22 11:33:04,201 - INFO - Processing 1450/2167: Player Semi Ojeleye (1628400) in Game 21700953 - 2018\n",
      "2025-06-22 11:33:12,157 - INFO - Saved 2018 batch 4349 with 442 records to batch_4349_20250622_113312.csv\n",
      "2025-06-22 11:33:49,527 - INFO - Processing 1500/2167: Player Ben Simmons (1627732) in Game 21701198 - 2018\n",
      "2025-06-22 11:34:05,612 - INFO - Saved 2018 batch 4350 with 487 records to batch_4350_20250622_113405.csv\n",
      "2025-06-22 11:34:37,749 - INFO - Processing 1550/2167: Player Jusuf NurkiÄ‡ (203994) in Game 21701177 - 2018\n",
      "2025-06-22 11:34:56,697 - INFO - Saved 2018 batch 4351 with 467 records to batch_4351_20250622_113456.csv\n",
      "2025-06-22 11:35:28,080 - INFO - Processing 1600/2167: Player Shaquille Harrison (1627885) in Game 21701107 - 2018\n",
      "2025-06-22 11:35:46,610 - INFO - Saved 2018 batch 4352 with 483 records to batch_4352_20250622_113546.csv\n",
      "2025-06-22 11:36:15,730 - INFO - Processing 1650/2167: Player Damian Lillard (203081) in Game 21700769 - 2018\n",
      "2025-06-22 11:36:39,832 - INFO - Saved 2018 batch 4353 with 395 records to batch_4353_20250622_113639.csv\n",
      "2025-06-22 11:37:05,594 - INFO - Processing 1700/2167: Player Chinanu Onuaku (1627778) in Game 21701230 - 2018\n",
      "2025-06-22 11:37:30,894 - INFO - Saved 2018 batch 4354 with 472 records to batch_4354_20250622_113730.csv\n",
      "2025-06-22 11:37:51,760 - INFO - Processing 1750/2167: Player Klay Thompson (202691) in Game 21700963 - 2018\n",
      "2025-06-22 11:38:18,617 - INFO - Saved 2018 batch 4355 with 384 records to batch_4355_20250622_113818.csv\n",
      "2025-06-22 11:38:39,174 - INFO - Processing 1800/2167: Player Alex Caruso (1627936) in Game 21700515 - 2018\n",
      "2025-06-22 11:39:06,489 - INFO - Saved 2018 batch 4356 with 494 records to batch_4356_20250622_113906.csv\n",
      "2025-06-22 11:39:27,832 - INFO - Processing 1850/2167: Player Taurean Prince (1627752) in Game 21700268 - 2018\n",
      "2025-06-22 11:39:59,526 - INFO - Saved 2018 batch 4357 with 493 records to batch_4357_20250622_113959.csv\n",
      "2025-06-22 11:40:12,903 - INFO - Processing 1900/2167: Player De'Aaron Fox (1628368) in Game 21701142 - 2018\n",
      "2025-06-22 11:40:46,243 - INFO - Saved 2018 batch 4358 with 410 records to batch_4358_20250622_114046.csv\n",
      "2025-06-22 11:40:59,184 - INFO - Processing 1950/2167: Player Fred VanVleet (1627832) in Game 21700639 - 2018\n",
      "2025-06-22 11:41:39,513 - INFO - Saved 2018 batch 4359 with 448 records to batch_4359_20250622_114139.csv\n",
      "2025-06-22 11:41:48,699 - INFO - Processing 2000/2167: Player Malik Beasley (1627736) in Game 21700478 - 2018\n",
      "2025-06-22 11:42:32,190 - INFO - Saved 2018 batch 4360 with 467 records to batch_4360_20250622_114232.csv\n",
      "2025-06-22 11:42:35,342 - INFO - Processing 2050/2167: Player Terrance Ferguson (1628390) in Game 21700788 - 2018\n",
      "2025-06-22 11:43:20,797 - INFO - Saved 2018 batch 4361 with 499 records to batch_4361_20250622_114320.csv\n",
      "2025-06-22 11:43:23,506 - INFO - Processing 2100/2167: Player Myles Turner (1626167) in Game 41700134 - 2018\n",
      "2025-06-22 11:44:07,639 - INFO - Saved 2018 batch 4362 with 509 records to batch_4362_20250622_114407.csv\n",
      "2025-06-22 11:44:10,073 - INFO - Processing 2150/2167: Player Alex Caruso (1627936) in Game 21700459 - 2018\n",
      "2025-06-22 11:44:28,327 - INFO - Saved 2018 batch 4363 with 214 records to batch_4363_20250622_114428.csv\n",
      "2025-06-22 11:44:30,891 - INFO - === SCRAPING COMPLETE ===\n",
      "2025-06-22 11:44:30,892 - INFO - Attempted to scrape: 2167\n",
      "2025-06-22 11:44:30,893 - INFO - Successful requests with data: 2021\n",
      "2025-06-22 11:44:30,893 - INFO - Successful requests with no data: 146\n",
      "2025-06-22 11:44:30,895 - INFO - Failed requests: 0\n",
      "2025-06-22 11:44:30,897 - INFO - New video records saved: 18642\n",
      "2025-06-22 11:44:30,898 - INFO - Overall success rate: 100.0%\n",
      "2025-06-22 11:44:30,899 - INFO - Data found rate: 93.3%\n",
      "2025-06-22 11:44:30,994 - INFO - Found 9 year directories\n",
      "2025-06-22 11:44:30,995 - INFO - Combined file already exists for 2017, skipping...\n",
      "2025-06-22 11:44:30,997 - INFO - Combined file already exists for 2018, skipping...\n",
      "2025-06-22 11:44:30,998 - INFO - Combined file already exists for 2019, skipping...\n",
      "2025-06-22 11:44:30,999 - INFO - Combined file already exists for 2020, skipping...\n",
      "2025-06-22 11:44:31,001 - INFO - Combined file already exists for 2021, skipping...\n",
      "2025-06-22 11:44:31,002 - INFO - Combined file already exists for 2022, skipping...\n",
      "2025-06-22 11:44:31,003 - INFO - Combined file already exists for 2023, skipping...\n",
      "2025-06-22 11:44:31,004 - INFO - Combined file already exists for 2024, skipping...\n",
      "2025-06-22 11:44:31,006 - INFO - Combined file already exists for 2025, skipping...\n",
      "2025-06-22 11:44:31,009 - INFO - \n",
      "=== DATA SUMMARY BY YEAR ===\n",
      "2025-06-22 11:44:31,014 - INFO - 2017: 177 records (combined)\n",
      "2025-06-22 11:44:31,066 - INFO - 2018: 17,159 records (combined)\n",
      "2025-06-22 11:44:31,478 - INFO - 2019: 243,791 records (combined)\n",
      "2025-06-22 11:44:31,834 - INFO - 2020: 212,869 records (combined)\n",
      "2025-06-22 11:44:32,184 - INFO - 2021: 219,375 records (combined)\n",
      "2025-06-22 11:44:32,602 - INFO - 2022: 244,270 records (combined)\n",
      "2025-06-22 11:44:32,961 - INFO - 2023: 247,934 records (combined)\n",
      "2025-06-22 11:44:33,375 - INFO - 2024: 247,452 records (combined)\n",
      "2025-06-22 11:44:33,786 - INFO - 2025: 248,577 records (combined)\n",
      "2025-06-22 11:44:33,787 - INFO - \n",
      "Total records across all years: 1,681,604\n",
      "2025-06-22 11:44:34,411 - INFO - Successfully loaded JSON from scrape_log.json with 222813 entries\n",
      "2025-06-22 11:44:34,446 - INFO - \n",
      "=== SCRAPE LOG ANALYSIS ===\n",
      "2025-06-22 11:44:34,447 - INFO - Total attempts logged: 222813\n",
      "2025-06-22 11:44:34,448 - INFO - Successful attempts: 222381\n",
      "2025-06-22 11:44:34,449 - INFO - Failed attempts: 432\n",
      "2025-06-22 11:44:34,450 - INFO - Success rate: 99.8%\n",
      "2025-06-22 11:44:34,451 - INFO - Total video records recorded in log: 683370\n",
      "2025-06-22 11:44:34,549 - INFO - \n",
      "Breakdown by year:\n",
      "2025-06-22 11:44:34,550 - INFO -   2015: 3802 attempts, 3795 successful (99.8%), 0 records\n",
      "2025-06-22 11:44:34,551 - INFO -   2016: 306 attempts, 306 successful (100.0%), 0 records\n",
      "2025-06-22 11:44:34,552 - INFO -   2017: 2348 attempts, 2348 successful (100.0%), 576 records\n",
      "2025-06-22 11:44:34,552 - INFO -   2018: 27817 attempts, 27817 successful (100.0%), 237305 records\n",
      "2025-06-22 11:44:34,554 - INFO -   2019: 2272 attempts, 2272 successful (100.0%), 220 records\n",
      "2025-06-22 11:44:34,555 - INFO -   2020: 10048 attempts, 10048 successful (100.0%), 78541 records\n",
      "2025-06-22 11:44:34,556 - INFO -   2021: 24915 attempts, 24915 successful (100.0%), 219403 records\n",
      "2025-06-22 11:44:34,557 - INFO -   2022: 2568 attempts, 2568 successful (100.0%), 1134 records\n",
      "2025-06-22 11:44:34,558 - INFO -   2023: 2384 attempts, 2384 successful (100.0%), 1204 records\n",
      "2025-06-22 11:44:34,559 - INFO -   2024: 2493 attempts, 2493 successful (100.0%), 1497 records\n",
      "2025-06-22 11:44:34,560 - INFO -   2025: 143860 attempts, 143435 successful (99.7%), 143490 records\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "import tempfile\n",
    "import shutil\n",
    "from typing import Dict, Set, Tuple, Optional\n",
    "import logging\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('scraper.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class SafeJSONManager:\n",
    "    \"\"\"\n",
    "    Manages JSON files with atomic writes and corruption recovery\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_write_json(data: dict, filepath: str, backup_count: int = 3) -> bool:\n",
    "        \"\"\"\n",
    "        Safely write JSON data with atomic operations and backups\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Create backup of existing file if it exists\n",
    "            if os.path.exists(filepath) and backup_count > 0:\n",
    "                SafeJSONManager._rotate_backups(filepath, backup_count)\n",
    "            \n",
    "            # Write to temporary file first\n",
    "            temp_filepath = f\"{filepath}.tmp\"\n",
    "            with open(temp_filepath, 'w') as f:\n",
    "                json.dump(data, f, indent=2)\n",
    "            \n",
    "            # Atomic move to final location\n",
    "            shutil.move(temp_filepath, filepath)\n",
    "            \n",
    "            # Verify the file can be read back\n",
    "            with open(filepath, 'r') as f:\n",
    "                json.load(f)\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error safely writing JSON to {filepath}: {e}\")\n",
    "            # Clean up temp file if it exists\n",
    "            if os.path.exists(f\"{filepath}.tmp\"):\n",
    "                try:\n",
    "                    os.remove(f\"{filepath}.tmp\")\n",
    "                except:\n",
    "                    pass\n",
    "            return False\n",
    "    \n",
    "    @staticmethod\n",
    "    def _rotate_backups(filepath: str, backup_count: int):\n",
    "        \"\"\"\n",
    "        Rotate backup files, keeping the most recent N backups\n",
    "        \"\"\"\n",
    "        # Rotate existing backups\n",
    "        for i in range(backup_count - 1, 0, -1):\n",
    "            old_backup = f\"{filepath}.backup{i}\"\n",
    "            new_backup = f\"{filepath}.backup{i + 1}\"\n",
    "            if os.path.exists(old_backup):\n",
    "                if os.path.exists(new_backup):\n",
    "                    os.remove(new_backup)\n",
    "                shutil.move(old_backup, new_backup)\n",
    "        \n",
    "        # Create new backup from current file\n",
    "        if os.path.exists(filepath):\n",
    "            backup_path = f\"{filepath}.backup1\"\n",
    "            if os.path.exists(backup_path):\n",
    "                os.remove(backup_path)\n",
    "            shutil.copy2(filepath, backup_path)\n",
    "    \n",
    "    @staticmethod\n",
    "    def safe_load_json(filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Safely load JSON with automatic corruption recovery\n",
    "        \"\"\"\n",
    "        if not os.path.exists(filepath):\n",
    "            logger.info(f\"No existing JSON file found at {filepath}\")\n",
    "            return {}\n",
    "        \n",
    "        # Try to load the main file\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            logger.info(f\"Successfully loaded JSON from {filepath} with {len(data)} entries\")\n",
    "            return data\n",
    "        except json.JSONDecodeError as e:\n",
    "            logger.warning(f\"JSON corruption detected in {filepath}: {e}\")\n",
    "            return SafeJSONManager._recover_from_corruption(filepath)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error loading {filepath}: {e}\")\n",
    "            return SafeJSONManager._recover_from_corruption(filepath)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _recover_from_corruption(filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Attempt to recover from JSON corruption using backups\n",
    "        \"\"\"\n",
    "        logger.info(\"Attempting JSON corruption recovery...\")\n",
    "        \n",
    "        # Try backup files\n",
    "        for i in range(1, 6):  # Try up to 5 backups\n",
    "            backup_path = f\"{filepath}.backup{i}\"\n",
    "            if os.path.exists(backup_path):\n",
    "                try:\n",
    "                    with open(backup_path, 'r') as f:\n",
    "                        data = json.load(f)\n",
    "                    logger.info(f\"Successfully recovered from backup {backup_path} with {len(data)} entries\")\n",
    "                    \n",
    "                    # Restore the good backup as the main file\n",
    "                    shutil.copy2(backup_path, filepath)\n",
    "                    return data\n",
    "                except json.JSONDecodeError:\n",
    "                    logger.warning(f\"Backup {backup_path} is also corrupted, trying next...\")\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error reading backup {backup_path}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        # If all backups failed, try manual recovery\n",
    "        logger.warning(\"All backups failed, attempting manual recovery...\")\n",
    "        return SafeJSONManager._manual_recovery(filepath)\n",
    "    \n",
    "    @staticmethod\n",
    "    def _manual_recovery(filepath: str) -> dict:\n",
    "        \"\"\"\n",
    "        Last resort: try to manually recover what we can from corrupted JSON\n",
    "        \"\"\"\n",
    "        try:\n",
    "            with open(filepath, 'r') as f:\n",
    "                content = f.read()\n",
    "            \n",
    "            # Try to find complete entries using regex\n",
    "            import re\n",
    "            pattern = r'\"(\\d+_\\d+)\"\\s*:\\s*(\\{(?:[^{}]|(?:\\{[^{}]*\\}))*\\})'\n",
    "            matches = re.finditer(pattern, content)\n",
    "            \n",
    "            recovered_data = {}\n",
    "            for match in matches:\n",
    "                key = match.group(1)\n",
    "                entry_json = match.group(2)\n",
    "                try:\n",
    "                    entry_data = json.loads(entry_json)\n",
    "                    recovered_data[key] = entry_data\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "            \n",
    "            if recovered_data:\n",
    "                logger.info(f\"Manual recovery found {len(recovered_data)} entries\")\n",
    "                return recovered_data\n",
    "            else:\n",
    "                logger.warning(\"Manual recovery found no valid entries\")\n",
    "                return {}\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Manual recovery failed: {e}\")\n",
    "            return {}\n",
    "\n",
    "class ImprovedNBAScraper:\n",
    "    \"\"\"\n",
    "    Improved NBA video details scraper with enhanced error handling and corruption resistance\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, master_record_path: str, output_dir: str = \"scraped_data\", \n",
    "                 log_file: str = \"scrape_log.json\"):\n",
    "        self.master_record_path = master_record_path\n",
    "        self.output_dir = output_dir\n",
    "        self.log_file = log_file\n",
    "        self.json_manager = SafeJSONManager()\n",
    "        \n",
    "        # Load master record\n",
    "        self.master_record = self._load_master_record()\n",
    "        \n",
    "        # Load scrape log\n",
    "        self.log_data = self.json_manager.safe_load_json(self.log_file)\n",
    "        \n",
    "        # Migrate CSV data to log if needed\n",
    "        self._migrate_csv_data_to_log()\n",
    "    \n",
    "    def _load_master_record(self) -> pd.DataFrame:\n",
    "        \"\"\"Load and prepare master record\"\"\"\n",
    "        try:\n",
    "            df = pd.read_csv(self.master_record_path)\n",
    "            df['TEAM_ID'] = df['TEAM_ID'].astype(int)\n",
    "            df['PLAYER_ID'] = df['PLAYER_ID'].astype(int)\n",
    "            df = df[df.year >= 2018]\n",
    "            logger.info(f\"Loaded master record with {len(df)} rows\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error loading master record: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def _migrate_csv_data_to_log(self):\n",
    "        \"\"\"Migrate existing CSV data to log\"\"\"\n",
    "        csv_combinations = self._load_existing_csv_combinations()\n",
    "        if not csv_combinations:\n",
    "            return\n",
    "        \n",
    "        migrated_count = 0\n",
    "        for player_id, game_id in csv_combinations:\n",
    "            key = f\"{player_id}_{game_id}\"\n",
    "            if key not in self.log_data:\n",
    "                self.log_data[key] = {\n",
    "                    \"player_id\": str(player_id),\n",
    "                    \"game_id\": str(game_id),\n",
    "                    \"team_id\": \"unknown\",\n",
    "                    \"player_name\": \"unknown\",\n",
    "                    \"year\": 2025,\n",
    "                    \"timestamp\": \"migrated_from_csv\",\n",
    "                    \"success\": True,\n",
    "                    \"record_count\": 1,\n",
    "                    \"has_data\": True,\n",
    "                    \"error_msg\": None\n",
    "                }\n",
    "                migrated_count += 1\n",
    "        \n",
    "        if migrated_count > 0:\n",
    "            logger.info(f\"Migrated {migrated_count} successful scrapes from CSV files to log\")\n",
    "            self._save_log()\n",
    "    \n",
    "    def _load_existing_csv_combinations(self) -> Set[Tuple[str, str]]:\n",
    "        \"\"\"Load all existing player-game combinations from CSV files\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            return set()\n",
    "        \n",
    "        combinations = set()\n",
    "        year_dirs = [d for d in os.listdir(self.output_dir) \n",
    "                    if d.startswith('year_') and os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "        \n",
    "        for year_dir in year_dirs:\n",
    "            year_path = os.path.join(self.output_dir, year_dir)\n",
    "            year = year_dir.replace('year_', '')\n",
    "            \n",
    "            # Check combined file first\n",
    "            combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "            if os.path.exists(combined_file):\n",
    "                try:\n",
    "                    df = pd.read_csv(combined_file)\n",
    "                    if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                        year_combinations = set(zip(df['def_id'].astype(str), df['gi'].astype(str)))\n",
    "                        combinations.update(year_combinations)\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error reading {combined_file}: {e}\")\n",
    "            else:\n",
    "                # Check batch files\n",
    "                batch_files = [f for f in os.listdir(year_path) \n",
    "                             if f.startswith('batch_') and f.endswith('.csv')]\n",
    "                for batch_file in batch_files:\n",
    "                    try:\n",
    "                        batch_path = os.path.join(year_path, batch_file)\n",
    "                        df = pd.read_csv(batch_path)\n",
    "                        if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                            batch_combinations = set(zip(df['def_id'].astype(str), df['gi'].astype(str)))\n",
    "                            combinations.update(batch_combinations)\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error reading {batch_path}: {e}\")\n",
    "        \n",
    "        logger.info(f\"Found {len(combinations)} existing combinations in CSV files\")\n",
    "        return combinations\n",
    "    \n",
    "    def _save_log(self) -> bool:\n",
    "        \"\"\"Safely save the scrape log\"\"\"\n",
    "        return self.json_manager.safe_write_json(self.log_data, self.log_file)\n",
    "    \n",
    "    def _update_log(self, player_id: str, game_id: str, team_id: str, player_name: str, \n",
    "                   year: int, success: bool, record_count: int = 0, has_data: bool = None, \n",
    "                   error_msg: str = None):\n",
    "        \"\"\"Update the scrape log with a new attempt\"\"\"\n",
    "        key = f\"{player_id}_{game_id}\"\n",
    "        self.log_data[key] = {\n",
    "            \"player_id\": str(player_id),\n",
    "            \"game_id\": str(game_id),\n",
    "            \"team_id\": str(team_id),\n",
    "            \"player_name\": player_name,\n",
    "            \"year\": year,\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"success\": success,\n",
    "            \"record_count\": record_count,\n",
    "            \"has_data\": has_data if has_data is not None else (record_count > 0),\n",
    "            \"error_msg\": error_msg\n",
    "        }\n",
    "    \n",
    "    def _fetch_video_details(self, game_id: str, player_id: str, team_id: str, \n",
    "                           context_measure: str = \"DEF_FGA\") -> Optional[dict]:\n",
    "        \"\"\"Fetch video details from NBA API with improved error handling\"\"\"\n",
    "        base_url = \"https://stats.nba.com/stats/videodetailsasset\"\n",
    "        headers = {\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "            \"Referer\": \"https://www.nba.com\",\n",
    "            \"Accept\": \"application/json\",\n",
    "            \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "            \"Origin\": \"https://www.nba.com\"\n",
    "        }\n",
    "        \n",
    "        params = {\n",
    "            \"GameID\": f\"00{game_id}\",\n",
    "            \"GameEventID\": \"\",\n",
    "            \"PlayerID\": str(player_id),\n",
    "            \"TeamID\": str(team_id),\n",
    "            \"Season\": \"\",\n",
    "            \"SeasonType\": \"\",\n",
    "            \"AheadBehind\": \"\",\n",
    "            \"CFID\": \"\",\n",
    "            \"CFPARAMS\": \"\",\n",
    "            \"ClutchTime\": \"\",\n",
    "            \"Conference\": \"\",\n",
    "            \"ContextFilter\": \"\",\n",
    "            \"ContextMeasure\": context_measure,\n",
    "            \"DateFrom\": \"\",\n",
    "            \"DateTo\": \"\",\n",
    "            \"Division\": \"\",\n",
    "            \"EndPeriod\": 0,\n",
    "            \"EndRange\": 40800,\n",
    "            \"GROUP_ID\": \"\",\n",
    "            \"GameSegment\": \"\",\n",
    "            \"GroupID\": \"\",\n",
    "            \"GroupMode\": \"\",\n",
    "            \"GroupQuantity\": 5,\n",
    "            \"LastNGames\": 0,\n",
    "            \"Location\": \"\",\n",
    "            \"Month\": 0,\n",
    "            \"OnOff\": \"\",\n",
    "            \"OppPlayerID\": \"\",\n",
    "            \"OpponentTeamID\": 0,\n",
    "            \"Outcome\": \"\",\n",
    "            \"PORound\": 0,\n",
    "            \"Period\": 0,\n",
    "            \"PlayerID1\": \"\",\n",
    "            \"PlayerID2\": \"\",\n",
    "            \"PlayerID3\": \"\",\n",
    "            \"PlayerID4\": \"\",\n",
    "            \"PlayerID5\": \"\",\n",
    "            \"PlayerPosition\": \"\",\n",
    "            \"PointDiff\": \"\",\n",
    "            \"Position\": \"\",\n",
    "            \"RangeType\": 0,\n",
    "            \"RookieYear\": \"\",\n",
    "            \"SeasonSegment\": \"\",\n",
    "            \"ShotClockRange\": \"\",\n",
    "            \"StartPeriod\": 0,\n",
    "            \"StartRange\": 0,\n",
    "            \"StarterBench\": \"\",\n",
    "            \"VsConference\": \"\",\n",
    "            \"VsDivision\": \"\",\n",
    "            \"VsPlayerID1\": \"\",\n",
    "            \"VsPlayerID2\": \"\",\n",
    "            \"VsPlayerID3\": \"\",\n",
    "            \"VsPlayerID4\": \"\",\n",
    "            \"VsPlayerID5\": \"\",\n",
    "            \"VsTeamID\": \"\"\n",
    "        }\n",
    "        \n",
    "        max_retries = 3\n",
    "        base_delay = 1\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                response = requests.get(base_url, headers=headers, params=params, timeout=30)\n",
    "                \n",
    "                if response.status_code == 200:\n",
    "                    return response.json()\n",
    "                elif response.status_code == 429:  # Rate limited\n",
    "                    delay = base_delay * (2 ** attempt)\n",
    "                    logger.warning(f\"Rate limited for Player {player_id}, Game {game_id}. Waiting {delay}s...\")\n",
    "                    time.sleep(delay)\n",
    "                    continue\n",
    "                else:\n",
    "                    logger.warning(f\"Request failed with status {response.status_code} for Player {player_id}, Game {game_id}\")\n",
    "                    if attempt == max_retries - 1:\n",
    "                        return None\n",
    "                    \n",
    "            except requests.RequestException as e:\n",
    "                logger.warning(f\"Request error for Player {player_id}, Game {game_id} (attempt {attempt + 1}): {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    return None\n",
    "                time.sleep(base_delay * (attempt + 1))\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def _process_video_data(self, video_json: dict, player_id: str, team_id: str, \n",
    "                          player_name: str, year: int) -> Optional[pd.DataFrame]:\n",
    "        \"\"\"Process video JSON response into DataFrame\"\"\"\n",
    "        try:\n",
    "            if not (video_json and 'resultSets' in video_json and 'playlist' in video_json['resultSets']):\n",
    "                return None\n",
    "            \n",
    "            playlist = video_json['resultSets']['playlist']\n",
    "            if not playlist:\n",
    "                return None\n",
    "            \n",
    "            df = pd.DataFrame(playlist)\n",
    "            if df.empty or not all(col in df.columns for col in ['gi', 'ei', 'dsc']):\n",
    "                return None\n",
    "            \n",
    "            df = df[['gi', 'ei', 'dsc']]\n",
    "            df['def_id'] = player_id\n",
    "            df['team_id'] = team_id\n",
    "            df['player_name'] = player_name\n",
    "            df['year'] = year\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing video data for Player {player_id}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _save_batch_data(self, year_data_dict: Dict[int, list], batch_num: int) -> int:\n",
    "        \"\"\"Save batch data organized by year with improved error handling\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            os.makedirs(self.output_dir)\n",
    "        \n",
    "        total_records_saved = 0\n",
    "        \n",
    "        for year, data_list in year_data_dict.items():\n",
    "            if not data_list:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                # Create year directory\n",
    "                year_dir = os.path.join(self.output_dir, f\"year_{year}\")\n",
    "                os.makedirs(year_dir, exist_ok=True)\n",
    "                \n",
    "                # Combine data\n",
    "                combined_df = pd.concat(data_list, ignore_index=True)\n",
    "                \n",
    "                # Save with atomic write\n",
    "                filename = f\"batch_{batch_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "                filepath = os.path.join(year_dir, filename)\n",
    "                temp_filepath = f\"{filepath}.tmp\"\n",
    "                \n",
    "                # Write to temp file first\n",
    "                combined_df.to_csv(temp_filepath, index=False)\n",
    "                \n",
    "                # Atomic move\n",
    "                shutil.move(temp_filepath, filepath)\n",
    "                \n",
    "                records_count = len(combined_df)\n",
    "                total_records_saved += records_count\n",
    "                logger.info(f\"Saved {year} batch {batch_num} with {records_count} records to {filename}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error saving batch data for year {year}: {e}\")\n",
    "                # Clean up temp file if it exists\n",
    "                temp_path = os.path.join(year_dir, f\"batch_{batch_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv.tmp\")\n",
    "                if os.path.exists(temp_path):\n",
    "                    try:\n",
    "                        os.remove(temp_path)\n",
    "                    except:\n",
    "                        pass\n",
    "        \n",
    "        return total_records_saved\n",
    "    \n",
    "    def analyze_scrape_status(self):\n",
    "        \"\"\"Analyze current scrape status\"\"\"\n",
    "        unique_combinations = self.master_record[['PLAYER_ID', 'GAME_ID', 'TEAM_ID', 'PLAYER_NAME', 'year']].drop_duplicates()\n",
    "        total_combinations = len(unique_combinations)\n",
    "        \n",
    "        unique_combinations['scrape_key'] = unique_combinations['PLAYER_ID'].astype(str) + \"_\" + unique_combinations['GAME_ID'].astype(str)\n",
    "        \n",
    "        def categorize_attempt(scrape_key):\n",
    "            if scrape_key not in self.log_data:\n",
    "                return 'never_attempted'\n",
    "            entry = self.log_data[scrape_key]\n",
    "            if not entry['success']:\n",
    "                return 'failed'\n",
    "            elif entry.get('has_data', entry['record_count'] > 0):\n",
    "                return 'successful_with_data'\n",
    "            else:\n",
    "                return 'successful_no_data'\n",
    "        \n",
    "        unique_combinations['log_status'] = unique_combinations['scrape_key'].apply(categorize_attempt)\n",
    "        \n",
    "        status_counts = unique_combinations['log_status'].value_counts()\n",
    "        \n",
    "        logger.info(\"=== SCRAPE STATUS ANALYSIS ===\")\n",
    "        logger.info(f\"Total combinations: {total_combinations}\")\n",
    "        for status, count in status_counts.items():\n",
    "            logger.info(f\"{status}: {count} combinations\")\n",
    "        \n",
    "        # Show breakdown by year\n",
    "        for status in status_counts.index:\n",
    "            status_data = unique_combinations[unique_combinations['log_status'] == status]\n",
    "            if len(status_data) > 0:\n",
    "                logger.info(f\"\\n{status} by year:\")\n",
    "                year_counts = status_data['year'].value_counts().sort_index()\n",
    "                for year, count in year_counts.items():\n",
    "                    logger.info(f\"  {year}: {count} combinations\")\n",
    "        \n",
    "        return unique_combinations\n",
    "    \n",
    "    def scrape(self, context_measure: str = \"DEF_FGA\", delay_between_requests: float = 2.0,\n",
    "              batch_size: int = 50, save_log_frequency: int = 10, \n",
    "              force_retry_failed: bool = False, retry_no_data: bool = False):\n",
    "        \"\"\"\n",
    "        Main scraping function with improved error handling and progress tracking\n",
    "        \"\"\"\n",
    "        logger.info(\"=== STARTING IMPROVED NBA VIDEO SCRAPER ===\")\n",
    "        \n",
    "        # Analyze current status\n",
    "        unique_combinations = self.analyze_scrape_status()\n",
    "        \n",
    "        # Determine what to scrape\n",
    "        to_scrape_parts = [unique_combinations[unique_combinations['log_status'] == 'never_attempted']]\n",
    "        \n",
    "        if force_retry_failed:\n",
    "            failed_combinations = unique_combinations[unique_combinations['log_status'] == 'failed']\n",
    "            to_scrape_parts.append(failed_combinations)\n",
    "            logger.info(f\"Force retry failed enabled: Including {len(failed_combinations)} failed attempts\")\n",
    "        \n",
    "        if retry_no_data:\n",
    "            no_data_combinations = unique_combinations[unique_combinations['log_status'] == 'successful_no_data']\n",
    "            to_scrape_parts.append(no_data_combinations)\n",
    "            logger.info(f\"Retry no data enabled: Including {len(no_data_combinations)} no-data attempts\")\n",
    "        \n",
    "        to_scrape_df = pd.concat(to_scrape_parts) if len(to_scrape_parts) > 1 else to_scrape_parts[0]\n",
    "        \n",
    "        if len(to_scrape_df) == 0:\n",
    "            logger.info(\"ðŸŽ‰ No combinations to scrape!\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Will attempt {len(to_scrape_df)} combinations\")\n",
    "        \n",
    "        # Initialize tracking variables\n",
    "        year_data = {}\n",
    "        successful_requests = 0\n",
    "        failed_requests = 0\n",
    "        no_data_requests = 0\n",
    "        batch_num = self._get_next_batch_number()\n",
    "        total_records_saved = 0\n",
    "        \n",
    "        logger.info(f\"Starting with batch number: {batch_num}\")\n",
    "        \n",
    "        # Process each combination\n",
    "        for idx, (_, row) in enumerate(to_scrape_df.iterrows()):\n",
    "            try:\n",
    "                player_id = str(row['PLAYER_ID'])\n",
    "                game_id = str(row['GAME_ID'])\n",
    "                team_id = str(row['TEAM_ID'])\n",
    "                player_name = row['PLAYER_NAME']\n",
    "                year = row['year']\n",
    "                \n",
    "                if (idx + 1) % 50 == 0:\n",
    "                    logger.info(f\"Processing {idx + 1}/{len(to_scrape_df)}: Player {player_name} ({player_id}) in Game {game_id} - {year}\")\n",
    "                \n",
    "                # Fetch video details\n",
    "                video_json = self._fetch_video_details(game_id, player_id, team_id, context_measure)\n",
    "                \n",
    "                if video_json:\n",
    "                    processed_df = self._process_video_data(video_json, player_id, team_id, player_name, year)\n",
    "                    \n",
    "                    if processed_df is not None and not processed_df.empty:\n",
    "                        if year not in year_data:\n",
    "                            year_data[year] = []\n",
    "                        \n",
    "                        year_data[year].append(processed_df)\n",
    "                        successful_requests += 1\n",
    "                        record_count = len(processed_df)\n",
    "                        \n",
    "                        self._update_log(player_id, game_id, team_id, player_name, year, \n",
    "                                       True, record_count, has_data=True)\n",
    "                    else:\n",
    "                        no_data_requests += 1\n",
    "                        self._update_log(player_id, game_id, team_id, player_name, year, \n",
    "                                       True, 0, has_data=False)\n",
    "                else:\n",
    "                    failed_requests += 1\n",
    "                    self._update_log(player_id, game_id, team_id, player_name, year, \n",
    "                                   False, 0, error_msg=\"API request failed\")\n",
    "                \n",
    "                # Save log periodically\n",
    "                if (idx + 1) % save_log_frequency == 0:\n",
    "                    if not self._save_log():\n",
    "                        logger.error(f\"Failed to save log at iteration {idx + 1}\")\n",
    "                \n",
    "                # Save batch data when threshold is reached\n",
    "                if successful_requests > 0 and successful_requests % batch_size == 0:\n",
    "                    records_saved = self._save_batch_data(year_data, batch_num)\n",
    "                    total_records_saved += records_saved\n",
    "                    year_data = {}\n",
    "                    batch_num += 1\n",
    "                \n",
    "                # Rate limiting\n",
    "                if idx < len(to_scrape_df) - 1:\n",
    "                    time.sleep(delay_between_requests)\n",
    "                    \n",
    "            except KeyboardInterrupt:\n",
    "                logger.info(\"Scraping interrupted by user. Saving progress...\")\n",
    "                break\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Unexpected error processing row {idx}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        # Save any remaining data\n",
    "        if any(year_data.values()):\n",
    "            records_saved = self._save_batch_data(year_data, batch_num)\n",
    "            total_records_saved += records_saved\n",
    "        \n",
    "        # Final log save\n",
    "        if not self._save_log():\n",
    "            logger.error(\"Failed to save final log\")\n",
    "        \n",
    "        # Print summary\n",
    "        logger.info(\"=== SCRAPING COMPLETE ===\")\n",
    "        logger.info(f\"Attempted to scrape: {len(to_scrape_df)}\")\n",
    "        logger.info(f\"Successful requests with data: {successful_requests}\")\n",
    "        logger.info(f\"Successful requests with no data: {no_data_requests}\")\n",
    "        logger.info(f\"Failed requests: {failed_requests}\")\n",
    "        logger.info(f\"New video records saved: {total_records_saved}\")\n",
    "        \n",
    "        if len(to_scrape_df) > 0:\n",
    "            success_rate = ((successful_requests + no_data_requests) / len(to_scrape_df)) * 100\n",
    "            logger.info(f\"Overall success rate: {success_rate:.1f}%\")\n",
    "            if successful_requests > 0:\n",
    "                data_rate = (successful_requests / len(to_scrape_df)) * 100\n",
    "                logger.info(f\"Data found rate: {data_rate:.1f}%\")\n",
    "    \n",
    "    def _get_next_batch_number(self) -> int:\n",
    "        \"\"\"Get the next batch number by examining existing files\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            return 1\n",
    "        \n",
    "        existing_batches = []\n",
    "        for year_dir in os.listdir(self.output_dir):\n",
    "            if year_dir.startswith('year_'):\n",
    "                year_path = os.path.join(self.output_dir, year_dir)\n",
    "                if os.path.isdir(year_path):\n",
    "                    batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_')]\n",
    "                    for batch_file in batch_files:\n",
    "                        try:\n",
    "                            batch_num_str = batch_file.split('_')[1]\n",
    "                            existing_batches.append(int(batch_num_str))\n",
    "                        except (ValueError, IndexError):\n",
    "                            pass\n",
    "        \n",
    "        return max(existing_batches) + 1 if existing_batches else 1\n",
    "    \n",
    "    def combine_batches_by_year(self):\n",
    "        \"\"\"Combine all batch files within each year into a single CSV per year\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            logger.info(f\"Output directory {self.output_dir} does not exist\")\n",
    "            return\n",
    "        \n",
    "        year_dirs = [d for d in os.listdir(self.output_dir) \n",
    "                    if d.startswith('year_') and os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "        \n",
    "        if not year_dirs:\n",
    "            logger.info(\"No year directories found\")\n",
    "            return\n",
    "        \n",
    "        logger.info(f\"Found {len(year_dirs)} year directories\")\n",
    "        \n",
    "        for year_dir in sorted(year_dirs):\n",
    "            year_path = os.path.join(self.output_dir, year_dir)\n",
    "            year = year_dir.replace('year_', '')\n",
    "            \n",
    "            # Check if combined file already exists\n",
    "            combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "            if os.path.exists(combined_file):\n",
    "                logger.info(f\"Combined file already exists for {year}, skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Find batch files\n",
    "            batch_files = [f for f in os.listdir(year_path) \n",
    "                         if f.startswith('batch_') and f.endswith('.csv')]\n",
    "            \n",
    "            if not batch_files:\n",
    "                logger.info(f\"No batch files found for {year}\")\n",
    "                continue\n",
    "            \n",
    "            logger.info(f\"Processing {year}: Found {len(batch_files)} batch files\")\n",
    "            \n",
    "            try:\n",
    "                # Load and combine all batches\n",
    "                year_batches = []\n",
    "                for file in batch_files:\n",
    "                    file_path = os.path.join(year_path, file)\n",
    "                    df = pd.read_csv(file_path)\n",
    "                    year_batches.append(df)\n",
    "                \n",
    "                if year_batches:\n",
    "                    final_df = pd.concat(year_batches, ignore_index=True)\n",
    "                    \n",
    "                    # Remove duplicates\n",
    "                    initial_count = len(final_df)\n",
    "                    final_df = final_df.drop_duplicates()\n",
    "                    final_count = len(final_df)\n",
    "                    \n",
    "                    if initial_count != final_count:\n",
    "                        logger.info(f\"Removed {initial_count - final_count} duplicate records for {year}\")\n",
    "                    \n",
    "                    # Save with atomic write\n",
    "                    temp_path = f\"{combined_file}.tmp\"\n",
    "                    final_df.to_csv(temp_path, index=False)\n",
    "                    shutil.move(temp_path, combined_file)\n",
    "                    \n",
    "                    logger.info(f\"âœ“ Combined file saved: {combined_file} with {final_count} total records\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error combining batches for {year}: {e}\")\n",
    "    \n",
    "    def get_summary(self):\n",
    "        \"\"\"Print a summary of collected data by year.\"\"\"\n",
    "        if not os.path.exists(self.output_dir):\n",
    "            logger.info(f\"Output directory {self.output_dir} does not exist.\")\n",
    "            return\n",
    "\n",
    "        year_dirs = [d for d in os.listdir(self.output_dir)\n",
    "                     if d.startswith('year_') and os.path.isdir(os.path.join(self.output_dir, d))]\n",
    "\n",
    "        if not year_dirs:\n",
    "            logger.info(\"No year directories found.\")\n",
    "            return\n",
    "\n",
    "        logger.info(\"\\n=== DATA SUMMARY BY YEAR ===\")\n",
    "        total_records = 0\n",
    "\n",
    "        for year_dir in sorted(year_dirs):\n",
    "            year = year_dir.replace('year_', '')\n",
    "            year_path = os.path.join(self.output_dir, year_dir)\n",
    "\n",
    "            # Look for combined file first, otherwise count batch files\n",
    "            combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "\n",
    "            if os.path.exists(combined_file):\n",
    "                try:\n",
    "                    df = pd.read_csv(combined_file)\n",
    "                    record_count = len(df)\n",
    "                    logger.info(f\"{year}: {record_count:,} records (combined)\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"Error reading combined file {combined_file}: {e}\")\n",
    "                    record_count = 0 # reset record count if there was an error\n",
    "            else:\n",
    "                # Count records in batch files\n",
    "                batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "                record_count = 0\n",
    "                num_batch_files = 0\n",
    "                for file in batch_files:\n",
    "                    file_path = os.path.join(year_path, file)\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                        record_count += len(df)\n",
    "                        num_batch_files += 1\n",
    "                    except Exception as e:\n",
    "                        logger.warning(f\"Error reading batch file {file_path}: {e}\")\n",
    "                logger.info(f\"{year}: {record_count:,} records ({num_batch_files} batch files)\")\n",
    "            \n",
    "            total_records += record_count\n",
    "\n",
    "        logger.info(f\"\\nTotal records across all years: {total_records:,}\")\n",
    "\n",
    "    def analyze_log_summary(self):\n",
    "        \"\"\"\n",
    "        Analyze the scrape log and provide statistics, replacing the old analyze_scrape_log function.\n",
    "        \"\"\"\n",
    "        log_data = self.json_manager.safe_load_json(self.log_file) # Use the class's safe loader\n",
    "        \n",
    "        if not log_data:\n",
    "            logger.info(\"No scrape log data found.\")\n",
    "            return\n",
    "        \n",
    "        successful = sum(1 for entry in log_data.values() if entry['success'])\n",
    "        failed = len(log_data) - successful\n",
    "        total_records_found = sum(entry.get('record_count', 0) for entry in log_data.values() if entry['success']) # Renamed to avoid confusion with `total_records` in `get_summary`\n",
    "        \n",
    "        logger.info(\"\\n=== SCRAPE LOG ANALYSIS ===\")\n",
    "        logger.info(f\"Total attempts logged: {len(log_data)}\")\n",
    "        logger.info(f\"Successful attempts: {successful}\")\n",
    "        logger.info(f\"Failed attempts: {failed}\")\n",
    "        \n",
    "        if len(log_data) > 0: # Avoid division by zero\n",
    "            success_rate = (successful / len(log_data)) * 100\n",
    "            logger.info(f\"Success rate: {success_rate:.1f}%\")\n",
    "        else:\n",
    "            logger.info(\"Success rate: N/A (no attempts logged)\")\n",
    "            \n",
    "        logger.info(f\"Total video records recorded in log: {total_records_found}\") # Clarified output\n",
    "        \n",
    "        # Analyze by year if available\n",
    "        year_stats = {}\n",
    "        for entry in log_data.values():\n",
    "            year = entry.get('year', 'unknown')\n",
    "            if year not in year_stats:\n",
    "                year_stats[year] = {'attempts': 0, 'success': 0, 'records': 0}\n",
    "            year_stats[year]['attempts'] += 1\n",
    "            if entry['success']:\n",
    "                year_stats[year]['success'] += 1\n",
    "                year_stats[year]['records'] += entry.get('record_count', 0)\n",
    "        \n",
    "        logger.info(\"\\nBreakdown by year:\")\n",
    "        \n",
    "        # Separate numeric years from non-numeric ones for proper sorting\n",
    "        numeric_years = []\n",
    "        non_numeric_years = []\n",
    "        \n",
    "        for year in year_stats.keys():\n",
    "            try:\n",
    "                numeric_years.append(int(year))\n",
    "            except (ValueError, TypeError):\n",
    "                non_numeric_years.append(str(year))\n",
    "        \n",
    "        sorted_years = sorted(numeric_years) + sorted(non_numeric_years)\n",
    "        \n",
    "        for year in sorted_years:\n",
    "            stats = year_stats[year]\n",
    "            success_rate = (stats['success'] / stats['attempts']) * 100 if stats['attempts'] > 0 else 0\n",
    "            logger.info(f\"  {year}: {stats['attempts']} attempts, {stats['success']} successful ({success_rate:.1f}%), {stats['records']} records\")\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    scraper = ImprovedNBAScraper(\n",
    "        master_record_path='master_record.csv',\n",
    "        output_dir='scraped_data',\n",
    "        log_file='scrape_log.json'\n",
    "    )\n",
    "\n",
    "    # Analyze existing log\n",
    "    scraper.analyze_log_summary()\n",
    "\n",
    "    # Run the scraper (set force_retry_failed=True to retry previously failed attempts)\n",
    "    scraper.scrape(\n",
    "        context_measure=\"DEF_FGA\",\n",
    "        delay_between_requests=0.001,  # 2 seconds between requests\n",
    "        batch_size=50,  # Save every 50 successful requests\n",
    "        force_retry_failed=True,  # Set to True to retry failed attempts\n",
    "        retry_no_data=False # Set to True to retry successful attempts that found no data\n",
    "    )\n",
    "\n",
    "    # Combine all batch files by year\n",
    "    scraper.combine_batches_by_year()\n",
    "\n",
    "    # Print summary of collected data\n",
    "    scraper.get_summary()\n",
    "    \n",
    "    # Final log analysis\n",
    "    scraper.analyze_log_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac4e4e-8a31-4512-9beb-07da679b7b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe35d83-e185-40d4-9ae0-2865a92d58fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a154409f-6ced-4e92-8e88-5ce0f5a2e986",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880f4a31-31cf-4579-a044-97a45a4c67e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd54a2b-dbef-4d1d-9980-c77fdd7296d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
