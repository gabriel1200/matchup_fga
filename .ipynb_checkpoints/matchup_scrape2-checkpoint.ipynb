{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9aaa460-cc8e-4918-b94d-24127a0e3c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded scrape log with 111676 previous attempts\n",
      "\n",
      "=== SCRAPE LOG ANALYSIS ===\n",
      "Total attempts logged: 111676\n",
      "Successful attempts: 111251\n",
      "Failed attempts: 425\n",
      "Success rate: 99.6%\n",
      "Total video records found: 979653\n",
      "\n",
      "Breakdown by year:\n",
      "  2022: 27930 attempts, 27930 successful (100.0%), 244270 records\n",
      "  2023: 27622 attempts, 27622 successful (100.0%), 247934 records\n",
      "  2024: 28077 attempts, 28077 successful (100.0%), 248860 records\n",
      "  2025: 26647 attempts, 26222 successful (98.4%), 237189 records\n",
      "  unknown: 1400 attempts, 1400 successful (100.0%), 1400 records\n",
      "=== LOADING SCRAPE LOG ===\n",
      "Loaded scrape log with 111676 previous attempts\n",
      "\n",
      "=== LOADING EXISTING CSV DATA ===\n",
      "Checking existing scraped data in 4 year directories...\n",
      "  Found 25362 combinations from 508 batch files for 2022\n",
      "  Found 25238 combinations from 506 batch files for 2023\n",
      "  Loading from combined file: scraped_data/year_2024/combined_video_details_2024.csv\n",
      "    Found 25593 combinations for 2024\n",
      "  Loading from combined file: scraped_data/year_2025/combined_video_details_2025.csv\n",
      "    Found 26340 combinations for 2025\n",
      "Total unique combinations with data in CSV files: 102533\n",
      "\n",
      "Loaded master record with 111676 rows\n",
      "Columns: ['PLAYER_ID', 'PLAYER_NAME', 'TEAM_ABBREVIATION', 'TEAM_ID', 'GAME_ID', 'year']\n",
      "\n",
      "Found 111676 unique player-game combinations in master record\n",
      "\n",
      "=== FILTERING BASED ON SCRAPE LOG ===\n",
      "Never attempted: 0 combinations\n",
      "Previously successful: 111251 combinations\n",
      "Previously failed: 425 combinations\n",
      "\n",
      "Force retry enabled: Will attempt 425 combinations (including 425 failed retries)\n",
      "\n",
      "Previously successful by year:\n",
      "  2022: 27930 combinations\n",
      "  2023: 27622 combinations\n",
      "  2024: 28086 combinations\n",
      "  2025: 27613 combinations\n",
      "\n",
      "Previously failed by year:\n",
      "  2025: 425 combinations\n",
      "\n",
      "Starting with batch number: 2209\n",
      "\n",
      "=== STARTING SCRAPING PROCESS ===\n",
      "Processing 1/425: Player Drew Peterson (1641809) in Game 22400900 - 2025\n",
      "Request failed with status code 500 for Player 1641809, Game 22400900\n",
      "Request failed with status code 500 for Player 1641709, Game 22400919\n",
      "Request failed with status code 500 for Player 1630541, Game 22400919\n",
      "Request failed with status code 500 for Player 1629614, Game 22400933\n",
      "Request failed with status code 500 for Player 1641824, Game 22400933\n",
      "Request failed with status code 500 for Player 1642270, Game 22400938\n",
      "Request failed with status code 500 for Player 1631101, Game 22400938\n",
      "Request failed with status code 500 for Player 1630658, Game 22400947\n",
      "Request failed with status code 500 for Player 1642347, Game 22400947\n",
      "Request failed with status code 500 for Player 1628415, Game 22400949\n",
      "Request failed with status code 500 for Player 1628369, Game 22400946\n",
      "Request failed with status code 500 for Player 203952, Game 22400948\n",
      "Request failed with status code 500 for Player 1627826, Game 22400948\n",
      "Request failed with status code 500 for Player 1627739, Game 22400948\n",
      "Request failed with status code 500 for Player 1629014, Game 22400953\n",
      "Request failed with status code 500 for Player 203497, Game 22400952\n",
      "Request failed with status code 500 for Player 1629630, Game 22400950\n",
      "Request failed with status code 500 for Player 1630168, Game 22400945\n",
      "Request failed with status code 500 for Player 201572, Game 22400955\n",
      "Request failed with status code 500 for Player 1630595, Game 22400954\n",
      "Request failed with status code 500 for Player 1642273, Game 22400954\n",
      "Request failed with status code 500 for Player 203468, Game 22401141\n",
      "Request failed with status code 500 for Player 202685, Game 22400957\n",
      "Request failed with status code 500 for Player 201939, Game 22400957\n",
      "Request failed with status code 500 for Player 1628989, Game 22400956\n",
      "Request failed with status code 500 for Player 202696, Game 22400956\n",
      "Request failed with status code 500 for Player 1629012, Game 22400966\n",
      "Request failed with status code 500 for Player 203999, Game 22400965\n",
      "Request failed with status code 500 for Player 203957, Game 22400961\n",
      "Request failed with status code 500 for Player 203939, Game 22400961\n",
      "Request failed with status code 500 for Player 1631106, Game 22400961\n",
      "Request failed with status code 500 for Player 1630215, Game 22400959\n",
      "Request failed with status code 500 for Player 1631170, Game 22400958\n",
      "Request failed with status code 500 for Player 1628436, Game 22400958\n",
      "Request failed with status code 500 for Player 1642258, Game 22400960\n",
      "Request failed with status code 500 for Player 203944, Game 22400963\n",
      "Request failed with status code 500 for Player 1631109, Game 22400964\n",
      "Request failed with status code 500 for Player 1631099, Game 22400967\n",
      "Request failed with status code 500 for Player 1629630, Game 22400962\n",
      "Request failed with status code 500 for Player 203081, Game 22400972\n",
      "Request failed with status code 500 for Player 203507, Game 22400972\n",
      "Request failed with status code 500 for Player 1629008, Game 22400975\n",
      "Request failed with status code 500 for Player 1630578, Game 22400970\n",
      "Request failed with status code 500 for Player 202699, Game 22400969\n",
      "Request failed with status code 500 for Player 203952, Game 22400971\n",
      "Request failed with status code 500 for Player 1642377, Game 22400971\n",
      "Request failed with status code 500 for Player 203468, Game 22400973\n",
      "Request failed with status code 500 for Player 1629661, Game 22400968\n",
      "Request failed with status code 500 for Player 203110, Game 22400974\n",
      "Request failed with status code 500 for Player 1642270, Game 22400980\n",
      "Processing 51/425: Player Jakob Poeltl (1627751) in Game 22400980 - 2025\n",
      "Request failed with status code 500 for Player 1627751, Game 22400980\n",
      "Request failed with status code 500 for Player 1630534, Game 22400980\n",
      "Request failed with status code 500 for Player 1631096, Game 22400982\n",
      "Request failed with status code 500 for Player 1629610, Game 22400538\n",
      "Request failed with status code 500 for Player 1630163, Game 22400538\n",
      "Request failed with status code 500 for Player 1631109, Game 22400538\n",
      "Request failed with status code 500 for Player 1631094, Game 22400979\n",
      "Request failed with status code 500 for Player 1629012, Game 22400981\n",
      "Request failed with status code 500 for Player 201142, Game 22400977\n",
      "Request failed with status code 500 for Player 1630208, Game 22400977\n",
      "Request failed with status code 500 for Player 1642346, Game 22400977\n",
      "Request failed with status code 500 for Player 1630700, Game 22400978\n",
      "Request failed with status code 500 for Player 1629651, Game 22400978\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime\n",
    "import sys\n",
    "import json\n",
    "\n",
    "def load_scrape_log(log_file=\"scrape_log.json\"):\n",
    "    \"\"\"\n",
    "    Load the scrape log that tracks all attempted scrapes\n",
    "    Returns a dictionary with scrape history\n",
    "    \"\"\"\n",
    "    if os.path.exists(log_file):\n",
    "        try:\n",
    "            with open(log_file, 'r') as f:\n",
    "                log_data = json.load(f)\n",
    "            print(f\"Loaded scrape log with {len(log_data)} previous attempts\")\n",
    "            return log_data\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading scrape log: {e}\")\n",
    "            return {}\n",
    "    else:\n",
    "        print(\"No existing scrape log found. Starting fresh.\")\n",
    "        return {}\n",
    "\n",
    "def save_scrape_log(log_data, log_file=\"scrape_log.json\"):\n",
    "    \"\"\"\n",
    "    Save the scrape log to disk\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(log_file, 'w') as f:\n",
    "            json.dump(log_data, f, indent=2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving scrape log: {e}\")\n",
    "\n",
    "def update_scrape_log(log_data, player_id, game_id, team_id, player_name, year, \n",
    "                     success, record_count=0, error_msg=None):\n",
    "    \"\"\"\n",
    "    Update the scrape log with a new attempt\n",
    "    \"\"\"\n",
    "    key = f\"{player_id}_{game_id}\"\n",
    "    log_data[key] = {\n",
    "        \"player_id\": str(player_id),\n",
    "        \"game_id\": str(game_id),\n",
    "        \"team_id\": str(team_id),\n",
    "        \"player_name\": player_name,\n",
    "        \"year\": year,\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"success\": success,\n",
    "        \"record_count\": record_count,\n",
    "        \"error_msg\": error_msg\n",
    "    }\n",
    "    return log_data\n",
    "\n",
    "def load_already_scraped_combinations_from_files(output_dir=\"scraped_data\"):\n",
    "    \"\"\"\n",
    "    Load all previously scraped combinations from existing batch files\n",
    "    Returns a set of tuples (player_id, game_id) that have data in CSV files\n",
    "    \"\"\"\n",
    "    already_scraped = set()\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        print(\"No existing scraped data directory found.\")\n",
    "        return already_scraped\n",
    "    \n",
    "    # Find all year directories\n",
    "    year_dirs = [d for d in os.listdir(output_dir) if d.startswith('year_') and os.path.isdir(os.path.join(output_dir, d))]\n",
    "    \n",
    "    if not year_dirs:\n",
    "        print(\"No existing year directories found.\")\n",
    "        return already_scraped\n",
    "    \n",
    "    print(f\"Checking existing scraped data in {len(year_dirs)} year directories...\")\n",
    "    \n",
    "    for year_dir in year_dirs:\n",
    "        year_path = os.path.join(output_dir, year_dir)\n",
    "        year = year_dir.replace('year_', '')\n",
    "        \n",
    "        # Check for combined file first\n",
    "        combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "        \n",
    "        if os.path.exists(combined_file):\n",
    "            print(f\"  Loading from combined file: {combined_file}\")\n",
    "            try:\n",
    "                df = pd.read_csv(combined_file)\n",
    "                if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                    combinations = set(zip(df['def_id'].astype(str), df['gi'].astype(str)))\n",
    "                    already_scraped.update(combinations)\n",
    "                    print(f\"    Found {len(combinations)} combinations for {year}\")\n",
    "                else:\n",
    "                    print(f\"    Warning: Expected columns not found in {combined_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"    Error reading {combined_file}: {e}\")\n",
    "        else:\n",
    "            # Check batch files\n",
    "            batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "            year_combinations = 0\n",
    "            \n",
    "            for batch_file in batch_files:\n",
    "                batch_path = os.path.join(year_path, batch_file)\n",
    "                try:\n",
    "                    df = pd.read_csv(batch_path)\n",
    "                    if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                        combinations = set(zip(df['def_id'].astype(str), df['gi'].astype(str)))\n",
    "                        already_scraped.update(combinations)\n",
    "                        year_combinations += len(combinations)\n",
    "                    else:\n",
    "                        print(f\"    Warning: Expected columns not found in {batch_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    Error reading {batch_path}: {e}\")\n",
    "            \n",
    "            if year_combinations > 0:\n",
    "                print(f\"  Found {year_combinations} combinations from {len(batch_files)} batch files for {year}\")\n",
    "    \n",
    "    print(f\"Total unique combinations with data in CSV files: {len(already_scraped)}\")\n",
    "    return already_scraped\n",
    "\n",
    "def migrate_csv_data_to_log(csv_combinations, log_data):\n",
    "    \"\"\"\n",
    "    Migrate existing CSV data to the scrape log for combinations not already in the log\n",
    "    \"\"\"\n",
    "    migrated_count = 0\n",
    "    for player_id, game_id in csv_combinations:\n",
    "        key = f\"{player_id}_{game_id}\"\n",
    "        if key not in log_data:\n",
    "            # We know this was successful since it's in CSV files, but we don't have other details\n",
    "            log_data[key] = {\n",
    "                \"player_id\": str(player_id),\n",
    "                \"game_id\": str(game_id),\n",
    "                \"team_id\": \"unknown\",\n",
    "                \"player_name\": \"unknown\",  # We don't have this from CSV alone\n",
    "                \"year\": 2025,  # We don't have this from CSV alone\n",
    "                \"timestamp\": \"migrated_from_csv\",\n",
    "                \"success\": True,\n",
    "                \"record_count\": 1,  # We know there was at least some data\n",
    "                \"error_msg\": None\n",
    "            }\n",
    "            migrated_count += 1\n",
    "    \n",
    "    if migrated_count > 0:\n",
    "        print(f\"Migrated {migrated_count} successful scrapes from CSV files to log\")\n",
    "    \n",
    "    return log_data\n",
    "\n",
    "def fetch_details(game_id, player_id, team_id, context_measure=\"DEF_FGA\"):\n",
    "    \"\"\"\n",
    "    Fetch video details for a specific player in a specific game\n",
    "    \"\"\"\n",
    "    base_url = \"https://stats.nba.com/stats/videodetailsasset\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n",
    "        \"Referer\": \"https://www.nba.com\",\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
    "        \"Origin\": \"https://www.nba.com\"\n",
    "    }\n",
    "    \n",
    "    params = {\n",
    "        \"GameID\": '00'+str(game_id),\n",
    "        \"GameEventID\": \"\",\n",
    "        \"PlayerID\": str(player_id),\n",
    "        \"TeamID\": str(team_id),\n",
    "        \"Season\": \"\",\n",
    "        \"SeasonType\": \"\",\n",
    "        \"AheadBehind\": \"\",\n",
    "        \"CFID\": \"\",\n",
    "        \"CFPARAMS\": \"\",\n",
    "        \"ClutchTime\": \"\",\n",
    "        \"Conference\": \"\",\n",
    "        \"ContextFilter\": \"\",\n",
    "        \"ContextMeasure\": context_measure,\n",
    "        \"DateFrom\": \"\",\n",
    "        \"DateTo\": \"\",\n",
    "        \"Division\": \"\",\n",
    "        \"EndPeriod\": 0,\n",
    "        \"EndRange\": 40800,\n",
    "        \"GROUP_ID\": \"\",\n",
    "        \"GameSegment\": \"\",\n",
    "        \"GroupID\": \"\",\n",
    "        \"GroupMode\": \"\",\n",
    "        \"GroupQuantity\": 5,\n",
    "        \"LastNGames\": 0,\n",
    "        \"Location\": \"\",\n",
    "        \"Month\": 0,\n",
    "        \"OnOff\": \"\",\n",
    "        \"OppPlayerID\": \"\",\n",
    "        \"OpponentTeamID\": 0,\n",
    "        \"Outcome\": \"\",\n",
    "        \"PORound\": 0,\n",
    "        \"Period\": 0,\n",
    "        \"PlayerID1\": \"\",\n",
    "        \"PlayerID2\": \"\",\n",
    "        \"PlayerID3\": \"\",\n",
    "        \"PlayerID4\": \"\",\n",
    "        \"PlayerID5\": \"\",\n",
    "        \"PlayerPosition\": \"\",\n",
    "        \"PointDiff\": \"\",\n",
    "        \"Position\": \"\",\n",
    "        \"RangeType\": 0,\n",
    "        \"RookieYear\": \"\",\n",
    "        \"SeasonSegment\": \"\",\n",
    "        \"ShotClockRange\": \"\",\n",
    "        \"StartPeriod\": 0,\n",
    "        \"StartRange\": 0,\n",
    "        \"StarterBench\": \"\",\n",
    "        \"VsConference\": \"\",\n",
    "        \"VsDivision\": \"\",\n",
    "        \"VsPlayerID1\": \"\",\n",
    "        \"VsPlayerID2\": \"\",\n",
    "        \"VsPlayerID3\": \"\",\n",
    "        \"VsPlayerID4\": \"\",\n",
    "        \"VsPlayerID5\": \"\",\n",
    "        \"VsTeamID\": \"\"\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(base_url, headers=headers, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            print(f\"Request failed with status code {response.status_code} for Player {player_id}, Game {game_id}\")\n",
    "            return None\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Request error for Player {player_id}, Game {game_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def process_video_data(video_json, player_id, team_id, player_name, year):\n",
    "    \"\"\"\n",
    "    Process the video JSON response into a DataFrame\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if video_json and 'resultSets' in video_json and 'playlist' in video_json['resultSets']:\n",
    "            playlist = video_json['resultSets']['playlist']\n",
    "            if playlist:  # Check if playlist is not empty\n",
    "                df = pd.DataFrame(playlist)\n",
    "                if not df.empty and all(col in df.columns for col in ['gi', 'ei', 'dsc']):\n",
    "                    df = df[['gi', 'ei', 'dsc']]\n",
    "                    df['def_id'] = player_id\n",
    "                    df['team_id'] = team_id\n",
    "                    df['player_name'] = player_name\n",
    "                    df['year'] = year\n",
    "                    return df\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing data for Player {player_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_batch_data_by_year(year_data_dict, batch_num, output_dir=\"scraped_data\"):\n",
    "    \"\"\"\n",
    "    Save batch data organized by year\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "    \n",
    "    total_records_saved = 0\n",
    "    \n",
    "    for year, data_list in year_data_dict.items():\n",
    "        if data_list:\n",
    "            # Create year-specific directory\n",
    "            year_dir = os.path.join(output_dir, f\"year_{year}\")\n",
    "            if not os.path.exists(year_dir):\n",
    "                os.makedirs(year_dir)\n",
    "            \n",
    "            # Combine all data for this year\n",
    "            combined_df = pd.concat(data_list, ignore_index=True)\n",
    "            \n",
    "            # Save with timestamp and batch number\n",
    "            filename = f\"{year_dir}/batch_{batch_num}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            combined_df.to_csv(filename, index=False)\n",
    "            \n",
    "            records_count = len(combined_df)\n",
    "            total_records_saved += records_count\n",
    "            print(f\"Saved {year} batch {batch_num} with {records_count} records to {filename}\")\n",
    "    \n",
    "    return total_records_saved\n",
    "\n",
    "def scrape_nba_video_details(master_record_path, context_measure=\"DEF_FGA\", \n",
    "                           delay_between_requests=2, batch_size=50, \n",
    "                           force_retry_failed=False):\n",
    "    \"\"\"\n",
    "    Main scraper function that processes all unique player_id and game_id combinations,\n",
    "    organizing data by year, while skipping already attempted combinations\n",
    "    \n",
    "    Args:\n",
    "        force_retry_failed: If True, will retry previously failed attempts\n",
    "    \"\"\"\n",
    "    # Load scrape log\n",
    "    print(\"=== LOADING SCRAPE LOG ===\")\n",
    "    log_data = load_scrape_log()\n",
    "    \n",
    "    # Load already scraped combinations from CSV files\n",
    "    print(\"\\n=== LOADING EXISTING CSV DATA ===\")\n",
    "    csv_combinations = load_already_scraped_combinations_from_files()\n",
    "    \n",
    "    # Migrate CSV data to log if needed\n",
    "    log_data = migrate_csv_data_to_log(csv_combinations, log_data)\n",
    "    \n",
    "    # Load master record\n",
    "    try:\n",
    "        master_record = pd.read_csv(master_record_path)\n",
    "        master_record['TEAM_ID']=master_record['TEAM_ID'].astype(int)\n",
    "        master_record['PLAYER_ID']=master_record['PLAYER_ID'].astype(int)\n",
    "\n",
    "        master_record=master_record[master_record.year>=2019]\n",
    "        print(f\"\\nLoaded master record with {len(master_record)} rows\")\n",
    "        print(f\"Columns: {list(master_record.columns)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading master record: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Get unique combinations of player_id and game_id\n",
    "    unique_combinations = master_record[['PLAYER_ID', 'GAME_ID', 'TEAM_ID', 'PLAYER_NAME', 'year']].drop_duplicates()\n",
    "    total_combinations = len(unique_combinations)\n",
    "    print(f\"\\nFound {total_combinations} unique player-game combinations in master record\")\n",
    "    \n",
    "    # Filter based on scrape log\n",
    "    print(f\"\\n=== FILTERING BASED ON SCRAPE LOG ===\")\n",
    "    unique_combinations['scrape_key'] = unique_combinations['PLAYER_ID'].astype(str) + \"_\" + unique_combinations['GAME_ID'].astype(str)\n",
    "    \n",
    "    # Check what's in the log\n",
    "    already_attempted = set(log_data.keys())\n",
    "    \n",
    "    # Categorize combinations\n",
    "    unique_combinations['log_status'] = unique_combinations['scrape_key'].apply(\n",
    "        lambda x: 'never_attempted' if x not in already_attempted else \n",
    "                 ('successful' if log_data[x]['success'] else 'failed')\n",
    "    )\n",
    "    \n",
    "    never_attempted = unique_combinations[unique_combinations['log_status'] == 'never_attempted']\n",
    "    previously_successful = unique_combinations[unique_combinations['log_status'] == 'successful']\n",
    "    previously_failed = unique_combinations[unique_combinations['log_status'] == 'failed']\n",
    "    \n",
    "    print(f\"Never attempted: {len(never_attempted)} combinations\")\n",
    "    print(f\"Previously successful: {len(previously_successful)} combinations\")\n",
    "    print(f\"Previously failed: {len(previously_failed)} combinations\")\n",
    "    \n",
    "    # Determine what to scrape\n",
    "    if force_retry_failed:\n",
    "        to_scrape_df = pd.concat([never_attempted, previously_failed])\n",
    "        print(f\"\\nForce retry enabled: Will attempt {len(to_scrape_df)} combinations (including {len(previously_failed)} failed retries)\")\n",
    "    else:\n",
    "        to_scrape_df = never_attempted\n",
    "        print(f\"\\nWill attempt {len(to_scrape_df)} never-attempted combinations\")\n",
    "        if len(previously_failed) > 0:\n",
    "            print(f\"Skipping {len(previously_failed)} previously failed combinations (use force_retry_failed=True to retry)\")\n",
    "    \n",
    "    # Show breakdown by year\n",
    "    if len(never_attempted) > 0:\n",
    "        print(\"\\nNever attempted by year:\")\n",
    "        never_by_year = never_attempted['year'].value_counts().sort_index()\n",
    "        for year, count in never_by_year.items():\n",
    "            print(f\"  {year}: {count} combinations\")\n",
    "    \n",
    "    if len(previously_successful) > 0:\n",
    "        print(\"\\nPreviously successful by year:\")\n",
    "        success_by_year = previously_successful['year'].value_counts().sort_index()\n",
    "        for year, count in success_by_year.items():\n",
    "            print(f\"  {year}: {count} combinations\")\n",
    "    \n",
    "    if len(previously_failed) > 0:\n",
    "        print(\"\\nPreviously failed by year:\")\n",
    "        failed_by_year = previously_failed['year'].value_counts().sort_index()\n",
    "        for year, count in failed_by_year.items():\n",
    "            print(f\"  {year}: {count} combinations\")\n",
    "    \n",
    "    if len(to_scrape_df) == 0:\n",
    "        print(\"\\n🎉 No combinations to scrape!\")\n",
    "        return\n",
    "    \n",
    "    # Initialize tracking variables\n",
    "    year_data = {}  # Dictionary to hold data by year\n",
    "    successful_requests = 0\n",
    "    failed_requests = 0\n",
    "    batch_num = 1\n",
    "    total_records_saved = 0\n",
    "    \n",
    "    # Get the next batch number by checking existing files\n",
    "    if os.path.exists(\"scraped_data\"):\n",
    "        existing_batches = []\n",
    "        for year_dir in os.listdir(\"scraped_data\"):\n",
    "            if year_dir.startswith('year_'):\n",
    "                year_path = os.path.join(\"scraped_data\", year_dir)\n",
    "                if os.path.isdir(year_path):\n",
    "                    batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_')]\n",
    "                    for batch_file in batch_files:\n",
    "                        try:\n",
    "                            batch_num_str = batch_file.split('_')[1]\n",
    "                            existing_batches.append(int(batch_num_str))\n",
    "                        except:\n",
    "                            pass\n",
    "        if existing_batches:\n",
    "            batch_num = max(existing_batches) + 1\n",
    "            print(f\"\\nStarting with batch number: {batch_num}\")\n",
    "    \n",
    "    print(f\"\\n=== STARTING SCRAPING PROCESS ===\")\n",
    "    \n",
    "    # Process each combination that needs to be scraped\n",
    "    for idx, row in to_scrape_df.iterrows():\n",
    "        player_id = str(row['PLAYER_ID'])\n",
    "        game_id = str(row['GAME_ID'])\n",
    "        team_id = str(row['TEAM_ID'])\n",
    "        player_name = row['PLAYER_NAME']\n",
    "        year = row['year']\n",
    "        \n",
    "        current_position = len(to_scrape_df) - len(to_scrape_df.loc[idx:])\n",
    "        if current_position %50 ==0:\n",
    "            print(f\"Processing {current_position + 1}/{len(to_scrape_df)}: Player {player_name} ({player_id}) in Game {game_id} - {year}\")\n",
    "        \n",
    "        # Fetch video details\n",
    "        video_json = fetch_details(game_id, player_id, team_id, context_measure)\n",
    "        \n",
    "        if video_json:\n",
    "            # Process the data\n",
    "            processed_df = process_video_data(video_json, player_id, team_id, player_name, year)\n",
    "            \n",
    "            if processed_df is not None and not processed_df.empty:\n",
    "                # Initialize year key if it doesn't exist\n",
    "                if year not in year_data:\n",
    "                    year_data[year] = []\n",
    "                \n",
    "                # Add data to the appropriate year\n",
    "                year_data[year].append(processed_df)\n",
    "                successful_requests += 1\n",
    "                record_count = len(processed_df)\n",
    "                print(f\"  ✓ Found {record_count} video records for {year}\")\n",
    "                \n",
    "                # Update log with success\n",
    "                log_data = update_scrape_log(log_data, player_id, game_id, team_id, \n",
    "                                           player_name, year, True, record_count)\n",
    "            else:\n",
    "                print(f\"  - No video data available for {year}\")\n",
    "                # Update log with success but no data\n",
    "                log_data = update_scrape_log(log_data, player_id, game_id, team_id, \n",
    "                                           player_name, year, True, 0)\n",
    "        else:\n",
    "            failed_requests += 1\n",
    "            # Update log with failure\n",
    "            log_data = update_scrape_log(log_data, player_id, game_id, team_id, \n",
    "                                       player_name, year, False, 0, \"API request failed\")\n",
    "        \n",
    "        # Save log periodically (every 10 requests)\n",
    "        if (current_position + 1) % 10 == 0:\n",
    "            save_scrape_log(log_data)\n",
    "        \n",
    "        # Check if we should save a batch (based on total successful requests)\n",
    "        if successful_requests > 0 and successful_requests % batch_size == 0:\n",
    "            records_saved = save_batch_data_by_year(year_data, batch_num)\n",
    "            total_records_saved += records_saved\n",
    "            year_data = {}  # Clear all year data after saving\n",
    "            batch_num += 1\n",
    "        \n",
    "        # Add delay between requests to be respectful to the API\n",
    "        if current_position < len(to_scrape_df) - 1:  # Don't delay after the last request\n",
    "            time.sleep(delay_between_requests)\n",
    "    \n",
    "    # Save any remaining data\n",
    "    if any(year_data.values()):  # Check if there's any data left to save\n",
    "        records_saved = save_batch_data_by_year(year_data, batch_num)\n",
    "        total_records_saved += records_saved\n",
    "    \n",
    "    # Save final log\n",
    "    save_scrape_log(log_data)\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"\\n=== SCRAPING COMPLETE ===\")\n",
    "    print(f\"Total combinations in master record: {total_combinations}\")\n",
    "    print(f\"Attempted to scrape: {len(to_scrape_df)}\")\n",
    "    print(f\"Successful new requests: {successful_requests}\")\n",
    "    print(f\"Failed requests: {failed_requests}\")\n",
    "    print(f\"New video records saved: {total_records_saved}\")\n",
    "    if len(to_scrape_df) > 0:\n",
    "        print(f\"Success rate for new requests: {(successful_requests/len(to_scrape_df))*100:.1f}%\")\n",
    "\n",
    "def analyze_scrape_log(log_file=\"scrape_log.json\"):\n",
    "    \"\"\"\n",
    "    Analyze the scrape log and provide statistics\n",
    "    \"\"\"\n",
    "    log_data = load_scrape_log(log_file)\n",
    "    \n",
    "    if not log_data:\n",
    "        print(\"No scrape log data found\")\n",
    "        return\n",
    "    \n",
    "    successful = sum(1 for entry in log_data.values() if entry['success'])\n",
    "    failed = len(log_data) - successful\n",
    "    total_records = sum(entry.get('record_count', 0) for entry in log_data.values() if entry['success'])\n",
    "    \n",
    "    print(f\"\\n=== SCRAPE LOG ANALYSIS ===\")\n",
    "    print(f\"Total attempts logged: {len(log_data)}\")\n",
    "    print(f\"Successful attempts: {successful}\")\n",
    "    print(f\"Failed attempts: {failed}\")\n",
    "    print(f\"Success rate: {(successful/len(log_data))*100:.1f}%\")\n",
    "    print(f\"Total video records found: {total_records}\")\n",
    "    \n",
    "    # Analyze by year if available\n",
    "    year_stats = {}\n",
    "    for entry in log_data.values():\n",
    "        year = entry.get('year', 'unknown')\n",
    "        if year not in year_stats:\n",
    "            year_stats[year] = {'attempts': 0, 'success': 0, 'records': 0}\n",
    "        year_stats[year]['attempts'] += 1\n",
    "        if entry['success']:\n",
    "            year_stats[year]['success'] += 1\n",
    "            year_stats[year]['records'] += entry.get('record_count', 0)\n",
    "    \n",
    "    print(f\"\\nBreakdown by year:\")\n",
    "    \n",
    "    # Separate numeric years from non-numeric ones for proper sorting\n",
    "    numeric_years = []\n",
    "    non_numeric_years = []\n",
    "    \n",
    "    for year in year_stats.keys():\n",
    "        try:\n",
    "            # Try to convert to int - if successful, it's a numeric year\n",
    "            numeric_years.append(int(year))\n",
    "        except (ValueError, TypeError):\n",
    "            # If conversion fails, it's non-numeric (like 'unknown')\n",
    "            non_numeric_years.append(str(year))\n",
    "    \n",
    "    # Sort numeric years numerically and non-numeric years alphabetically\n",
    "    sorted_years = sorted(numeric_years) + sorted(non_numeric_years)\n",
    "    \n",
    "    for year in sorted_years:\n",
    "        stats = year_stats[year]\n",
    "        success_rate = (stats['success'] / stats['attempts']) * 100 if stats['attempts'] > 0 else 0\n",
    "        print(f\"  {year}: {stats['attempts']} attempts, {stats['success']} successful ({success_rate:.1f}%), {stats['records']} records\")\n",
    "\n",
    "def combine_batches_by_year(output_dir=\"scraped_data\"):\n",
    "    \"\"\"\n",
    "    Combine all batch files within each year into a single CSV per year\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Output directory {output_dir} does not exist\")\n",
    "        return\n",
    "    \n",
    "    # Find all year directories\n",
    "    year_dirs = [d for d in os.listdir(output_dir) if d.startswith('year_') and os.path.isdir(os.path.join(output_dir, d))]\n",
    "    \n",
    "    if not year_dirs:\n",
    "        print(\"No year directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Found {len(year_dirs)} year directories: {sorted(year_dirs)}\")\n",
    "    \n",
    "    for year_dir in sorted(year_dirs):\n",
    "        year_path = os.path.join(output_dir, year_dir)\n",
    "        year = year_dir.replace('year_', '')\n",
    "        \n",
    "        # Find all batch files for this year\n",
    "        batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "        \n",
    "        if not batch_files:\n",
    "            print(f\"No batch files found for {year}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\nProcessing {year}: Found {len(batch_files)} batch files\")\n",
    "        \n",
    "        # Load and combine all batches for this year\n",
    "        year_batches = []\n",
    "        for file in batch_files:\n",
    "            file_path = os.path.join(year_path, file)\n",
    "            df = pd.read_csv(file_path)\n",
    "            year_batches.append(df)\n",
    "            print(f\"  Loaded {file}: {len(df)} records\")\n",
    "        \n",
    "        # Combine all batches for this year\n",
    "        if year_batches:\n",
    "            final_df = pd.concat(year_batches, ignore_index=True)\n",
    "            \n",
    "            # Remove duplicates if any\n",
    "            initial_count = len(final_df)\n",
    "            final_df = final_df.drop_duplicates()\n",
    "            final_count = len(final_df)\n",
    "            \n",
    "            if initial_count != final_count:\n",
    "                print(f\"  Removed {initial_count - final_count} duplicate records for {year}\")\n",
    "            \n",
    "            # Save final combined file for this year\n",
    "            final_filename = f\"combined_video_details_{year}.csv\"\n",
    "            final_path = os.path.join(year_path, final_filename)\n",
    "            final_df.to_csv(final_path, index=False)\n",
    "            print(f\"  ✓ Combined file saved: {final_path} with {final_count} total records\")\n",
    "\n",
    "def get_year_summary(output_dir=\"scraped_data\"):\n",
    "    \"\"\"\n",
    "    Print a summary of data collected by year\n",
    "    \"\"\"\n",
    "    if not os.path.exists(output_dir):\n",
    "        print(f\"Output directory {output_dir} does not exist\")\n",
    "        return\n",
    "    \n",
    "    year_dirs = [d for d in os.listdir(output_dir) if d.startswith('year_') and os.path.isdir(os.path.join(output_dir, d))]\n",
    "    \n",
    "    if not year_dirs:\n",
    "        print(\"No year directories found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n=== DATA SUMMARY BY YEAR ===\")\n",
    "    total_records = 0\n",
    "    \n",
    "    for year_dir in sorted(year_dirs):\n",
    "        year = year_dir.replace('year_', '')\n",
    "        year_path = os.path.join(output_dir, year_dir)\n",
    "        \n",
    "        # Look for combined file first, otherwise count batch files\n",
    "        combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "        \n",
    "        if os.path.exists(combined_file):\n",
    "            df = pd.read_csv(combined_file)\n",
    "            record_count = len(df)\n",
    "            print(f\"{year}: {record_count:,} records (combined)\")\n",
    "        else:\n",
    "            # Count records in batch files\n",
    "            batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "            record_count = 0\n",
    "            for file in batch_files:\n",
    "                file_path = os.path.join(year_path, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                record_count += len(df)\n",
    "            print(f\"{year}: {record_count:,} records ({len(batch_files)} batch files)\")\n",
    "        \n",
    "        total_records += record_count\n",
    "    \n",
    "    print(f\"\\nTotal records across all years: {total_records:,}\")\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Analyze existing log\n",
    "    analyze_scrape_log()\n",
    "    \n",
    "    # Run the scraper (set force_retry_failed=True to retry previously failed attempts)\n",
    "    scrape_nba_video_details(\n",
    "        master_record_path='master_record.csv',\n",
    "        context_measure=\"DEF_FGA\",\n",
    "        delay_between_requests=.001,  # 2 seconds between requests\n",
    "        batch_size=50,  # Save every 50 successful requests\n",
    "        force_retry_failed=True  # Set to True to retry failed attempts\n",
    "    )\n",
    "    \n",
    "    # Combine all batch files by year\n",
    "    combine_batches_by_year()\n",
    "    \n",
    "    # Print summary\n",
    "    get_year_summary()\n",
    "    \n",
    "    # Final log analysis\n",
    "    analyze_scrape_log()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac4e4e-8a31-4512-9beb-07da679b7b8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
