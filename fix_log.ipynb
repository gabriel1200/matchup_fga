{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d41ca6a5-3c8d-43bd-9000-82752790f5f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FIXING CORRUPTED SCRAPE LOG ===\n",
      "Backed up corrupted log to: scrape_log.json.corrupted_backup_20250619_091454\n",
      "Attempting to recover data from scrape_log.json...\n",
      "File size: 2 characters\n",
      "Total lines: 1\n",
      "Found 0 potentially complete entries using regex\n",
      "\n",
      "Trying line-by-line recovery...\n",
      "\n",
      "Trying manual entry extraction...\n",
      "Error during recovery attempt: unknown extension ?2 at position 31\n",
      "Rebuilding log from existing CSV files...\n",
      "No scraped data directory found\n",
      "Started with 0 recovered entries\n",
      "Added 0 new entries from CSV files\n",
      "Final log contains 0 total entries\n",
      "‚úì Successfully saved fixed log to scrape_log.json\n",
      "‚úì Verification successful: 0 entries loaded\n",
      "\n",
      "üéâ Log recovery completed successfully!\n",
      "You can now restart your scraping process.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def backup_corrupted_log(log_file=\"scrape_log.json\"):\n",
    "    \"\"\"\n",
    "    Create a backup of the corrupted log file\n",
    "    \"\"\"\n",
    "    if os.path.exists(log_file):\n",
    "        backup_name = f\"{log_file}.corrupted_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        shutil.copy2(log_file, backup_name)\n",
    "        print(f\"Backed up corrupted log to: {backup_name}\")\n",
    "        return backup_name\n",
    "    return None\n",
    "\n",
    "def attempt_json_recovery(log_file=\"scrape_log.json\"):\n",
    "    \"\"\"\n",
    "    Attempt to recover as much data as possible from the corrupted JSON\n",
    "    \"\"\"\n",
    "    print(f\"Attempting to recover data from {log_file}...\")\n",
    "    \n",
    "    try:\n",
    "        with open(log_file, 'r') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(f\"File size: {len(content)} characters\")\n",
    "        \n",
    "        # Try to find where the corruption starts\n",
    "        # Look for the last complete entry\n",
    "        lines = content.split('\\n')\n",
    "        print(f\"Total lines: {len(lines)}\")\n",
    "        \n",
    "        # Strategy 1: Try to find the last valid JSON structure\n",
    "        # Look for the pattern that indicates complete entries\n",
    "        recovered_data = {}\n",
    "        \n",
    "        # Find all complete key-value pairs\n",
    "        import re\n",
    "        \n",
    "        # Pattern to match complete log entries\n",
    "        pattern = r'\"(\\d+_\\d+)\"\\s*:\\s*\\{[^}]*\"error_msg\":\\s*(?:\"[^\"]*\"|null)\\s*\\}'\n",
    "        matches = re.findall(pattern, content, re.DOTALL)\n",
    "        \n",
    "        print(f\"Found {len(matches)} potentially complete entries using regex\")\n",
    "        \n",
    "        # Try a different approach: manually parse line by line\n",
    "        print(\"\\nTrying line-by-line recovery...\")\n",
    "        \n",
    "        # Look for the opening brace\n",
    "        brace_count = 0\n",
    "        in_string = False\n",
    "        escape_next = False\n",
    "        last_complete_pos = 0\n",
    "        \n",
    "        for i, char in enumerate(content):\n",
    "            if escape_next:\n",
    "                escape_next = False\n",
    "                continue\n",
    "                \n",
    "            if char == '\\\\' and in_string:\n",
    "                escape_next = True\n",
    "                continue\n",
    "                \n",
    "            if char == '\"' and not escape_next:\n",
    "                in_string = not in_string\n",
    "                continue\n",
    "                \n",
    "            if not in_string:\n",
    "                if char == '{':\n",
    "                    brace_count += 1\n",
    "                elif char == '}':\n",
    "                    brace_count -= 1\n",
    "                    if brace_count == 1:  # Back to main object level\n",
    "                        last_complete_pos = i\n",
    "        \n",
    "        if last_complete_pos > 0:\n",
    "            print(f\"Found last complete position at character {last_complete_pos}\")\n",
    "            \n",
    "            # Extract the valid portion and try to close it properly\n",
    "            valid_portion = content[:last_complete_pos + 1]\n",
    "            \n",
    "            # Make sure it ends properly\n",
    "            if not valid_portion.rstrip().endswith('}'):\n",
    "                valid_portion += '\\n}'\n",
    "            \n",
    "            # Try to parse this portion\n",
    "            try:\n",
    "                recovered_data = json.loads(valid_portion)\n",
    "                print(f\"‚úì Successfully recovered {len(recovered_data)} entries\")\n",
    "                return recovered_data\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"Still couldn't parse recovered portion: {e}\")\n",
    "        \n",
    "        # Strategy 2: Try to extract individual entries manually\n",
    "        print(\"\\nTrying manual entry extraction...\")\n",
    "        \n",
    "        # Look for individual complete entries\n",
    "        entry_pattern = r'\"(\\d+_\\d+)\"\\s*:\\s*(\\{(?:[^{}]|(?2))*\\})'\n",
    "        individual_matches = re.finditer(entry_pattern, content)\n",
    "        \n",
    "        recovered_entries = {}\n",
    "        valid_entries = 0\n",
    "        \n",
    "        for match in individual_matches:\n",
    "            key = match.group(1)\n",
    "            entry_json = match.group(2)\n",
    "            try:\n",
    "                entry_data = json.loads(entry_json)\n",
    "                recovered_entries[key] = entry_data\n",
    "                valid_entries += 1\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        if valid_entries > 0:\n",
    "            print(f\"‚úì Manually recovered {valid_entries} individual entries\")\n",
    "            return recovered_entries\n",
    "        \n",
    "        print(\"‚ùå Could not recover any data from the corrupted file\")\n",
    "        return {}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error during recovery attempt: {e}\")\n",
    "        return {}\n",
    "\n",
    "def rebuild_log_from_csv(output_dir=\"scraped_data\"):\n",
    "    \"\"\"\n",
    "    Rebuild the log from existing CSV files\n",
    "    \"\"\"\n",
    "    print(\"Rebuilding log from existing CSV files...\")\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        print(\"No scraped data directory found\")\n",
    "        return {}\n",
    "    \n",
    "    rebuilt_log = {}\n",
    "    \n",
    "    # Find all year directories\n",
    "    year_dirs = [d for d in os.listdir(output_dir) if d.startswith('year_') and os.path.isdir(os.path.join(output_dir, d))]\n",
    "    \n",
    "    for year_dir in year_dirs:\n",
    "        year_path = os.path.join(output_dir, year_dir)\n",
    "        year = year_dir.replace('year_', '')\n",
    "        \n",
    "        # Check for combined file first\n",
    "        combined_file = os.path.join(year_path, f\"combined_video_details_{year}.csv\")\n",
    "        files_to_process = []\n",
    "        \n",
    "        if os.path.exists(combined_file):\n",
    "            files_to_process.append(combined_file)\n",
    "        else:\n",
    "            # Get all batch files\n",
    "            batch_files = [f for f in os.listdir(year_path) if f.startswith('batch_') and f.endswith('.csv')]\n",
    "            files_to_process = [os.path.join(year_path, f) for f in batch_files]\n",
    "        \n",
    "        # Process each file\n",
    "        for file_path in files_to_process:\n",
    "            try:\n",
    "                import pandas as pd\n",
    "                df = pd.read_csv(file_path)\n",
    "                \n",
    "                if 'def_id' in df.columns and 'gi' in df.columns:\n",
    "                    # Group by player_id and game_id to count records\n",
    "                    grouped = df.groupby(['def_id', 'gi']).agg({\n",
    "                        'team_id': 'first',\n",
    "                        'player_name': 'first',\n",
    "                        'year': 'first'\n",
    "                    }).reset_index()\n",
    "                    \n",
    "                    for _, row in grouped.iterrows():\n",
    "                        player_id = str(row['def_id'])\n",
    "                        game_id = str(row['gi'])\n",
    "                        key = f\"{player_id}_{game_id}\"\n",
    "                        \n",
    "                        # Count records for this combination\n",
    "                        record_count = len(df[(df['def_id'] == row['def_id']) & (df['gi'] == row['gi'])])\n",
    "                        \n",
    "                        rebuilt_log[key] = {\n",
    "                            \"player_id\": player_id,\n",
    "                            \"game_id\": game_id,\n",
    "                            \"team_id\": str(row['team_id']),\n",
    "                            \"player_name\": row['player_name'],\n",
    "                            \"year\": int(row['year']),\n",
    "                            \"timestamp\": \"rebuilt_from_csv\",\n",
    "                            \"success\": True,\n",
    "                            \"record_count\": record_count,\n",
    "                            \"has_data\": True,\n",
    "                            \"error_msg\": None\n",
    "                        }\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file_path}: {e}\")\n",
    "    \n",
    "    print(f\"Rebuilt log with {len(rebuilt_log)} entries from CSV files\")\n",
    "    return rebuilt_log\n",
    "\n",
    "def fix_corrupted_log(log_file=\"scrape_log.json\"):\n",
    "    \"\"\"\n",
    "    Main function to fix the corrupted log file\n",
    "    \"\"\"\n",
    "    print(\"=== FIXING CORRUPTED SCRAPE LOG ===\")\n",
    "    \n",
    "    # Step 1: Backup the corrupted file\n",
    "    backup_path = backup_corrupted_log(log_file)\n",
    "    \n",
    "    # Step 2: Try to recover data from the corrupted file\n",
    "    recovered_data = attempt_json_recovery(log_file)\n",
    "    \n",
    "    # Step 3: Rebuild from CSV files\n",
    "    csv_data = rebuild_log_from_csv()\n",
    "    \n",
    "    # Step 4: Merge recovered data with CSV data (CSV data takes precedence for conflicts)\n",
    "    final_log = {}\n",
    "    \n",
    "    # Start with recovered data\n",
    "    final_log.update(recovered_data)\n",
    "    print(f\"Started with {len(recovered_data)} recovered entries\")\n",
    "    \n",
    "    # Add CSV data (will overwrite any conflicts)\n",
    "    added_from_csv = 0\n",
    "    for key, value in csv_data.items():\n",
    "        if key not in final_log:\n",
    "            added_from_csv += 1\n",
    "        final_log[key] = value\n",
    "    \n",
    "    print(f\"Added {added_from_csv} new entries from CSV files\")\n",
    "    print(f\"Final log contains {len(final_log)} total entries\")\n",
    "    \n",
    "    # Step 5: Save the fixed log\n",
    "    try:\n",
    "        with open(log_file, 'w') as f:\n",
    "            json.dump(final_log, f, indent=2)\n",
    "        print(f\"‚úì Successfully saved fixed log to {log_file}\")\n",
    "        \n",
    "        # Verify the fixed file\n",
    "        with open(log_file, 'r') as f:\n",
    "            verification = json.load(f)\n",
    "        print(f\"‚úì Verification successful: {len(verification)} entries loaded\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error saving fixed log: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    success = fix_corrupted_log()\n",
    "    if success:\n",
    "        print(\"\\nüéâ Log recovery completed successfully!\")\n",
    "        print(\"You can now restart your scraping process.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå Log recovery failed. You may need to start with a fresh log.\")\n",
    "        print(\"Consider renaming the corrupted file and starting fresh.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f9bd49-422f-4b26-a478-1959a8e532d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
